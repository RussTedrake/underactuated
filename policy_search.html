<!DOCTYPE html>

<html>

  <head>
    <title>Underactuated Robotics: Policy
  Search</title>
    <meta name="Underactuated Robotics: Policy
  Search" content="text/html; charset=utf-8;" />
    <link rel="canonical" href="http://underactuated.mit.edu/policy_search.html" />

    <script src="https://hypothes.is/embed.js" async></script>
    <script type="text/javascript" src="htmlbook/book.js"></script>

    <script src="htmlbook/mathjax-config.js" defer></script> 
    <script type="text/javascript" id="MathJax-script" defer
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
    </script>
    <script>window.MathJax || document.write('<script type="text/javascript" src="htmlbook/MathJax/es5/tex-chtml.js" defer><\/script>')</script>

    <link rel="stylesheet" href="htmlbook/highlight/styles/default.css">
    <script src="htmlbook/highlight/highlight.pack.js"></script> <!-- http://highlightjs.readthedocs.io/en/latest/css-classes-reference.html#language-names-and-aliases -->
    <script>hljs.initHighlightingOnLoad();</script>

    <link rel="stylesheet" type="text/css" href="htmlbook/book.css" />
  </head>

<body onload="loadChapter('underactuated');">

<div data-type="titlepage">
  <header>
    <h1><a href="index.html" style="text-decoration:none;">Underactuated Robotics</a></h1>
    <p data-type="subtitle">Algorithms for Walking, Running, Swimming, Flying, and Manipulation</p> 
    <p style="font-size: 18px;"><a href="http://people.csail.mit.edu/russt/">Russ Tedrake</a></p>
    <p style="font-size: 14px; text-align: right;"> 
      &copy; Russ Tedrake, 2021<br/>
      Last modified <span id="last_modified"></span>.</br>
      <script>
      var d = new Date(document.lastModified);
      document.getElementById("last_modified").innerHTML = d.getFullYear() + "-" + (d.getMonth()+1) + "-" + d.getDate();</script>
      <a href="misc.html">How to cite these notes, use annotations, and give feedback.</a><br/>
    </p>
  </header>
</div>

<p><b>Note:</b> These are working notes used for <a
href="http://underactuated.csail.mit.edu/Spring2021/">a course being taught
at MIT</a>. They will be updated throughout the Spring 2021 semester.  <a 
href="https://www.youtube.com/channel/UChfUOAhz7ynELF-s_1LPpWg">Lecture  videos are available on YouTube</a>.</p> 

<table style="width:100%;"><tr style="width:100%">
  <td style="width:33%;text-align:left;"><a class="previous_chapter" href=trajopt.html>Previous Chapter</a></td>
  <td style="width:33%;text-align:center;"><a href=index.html>Table of contents</a></td>
  <td style="width:33%;text-align:right;"><a class="next_chapter" href=planning.html>Next Chapter</a></td>
</tr></table>


<!-- EVERYTHING ABOVE THIS LINE IS OVERWRITTEN BY THE INSTALL SCRIPT -->
<chapter style="counter-reset: chapter 10"><h1>Policy
  Search</h1>

  <p>In our case study on <a href="trajopt.html#perching">perching
  aircraft</a>, we solved a challenging control problem, but our approach to
  control design based on only linear optimal control (around an optimized
  trajectory).  We've also discussed some approaches to nonlinear optimal
  control that could scale beyond small discretized state spaces.  These were
  based on estimating the cost-to-go function, including
  <a href="dp.html#function_approximation">value iteration using function
  approximation</a>, and lower bounds based on approximate dynamic programming
  as a <a href="dp.html#LP">linear program</a>
  or as a <a href="lyapunov.html#adp">sums-of-squares program</a>.  </p>

  <p>There are a lot of things to like about methods that estimate the
  cost-to-go function (aka value function).  The cost-to-go function reduces
  the long-term planning problem into a one-step planning problem; it encodes
  all relevant information about the future (and nothing more).  The HJB gives
  us optimality conditions for the cost-to-go that give us a strong algorithmic
  handle to work with.  <!--But there is, I think, one very reasonable concern
  about value-based methods: we have seen many examples where the optimal
  cost-to-go function can be a very complicated function (by most measures of
  complexity; e.g. it would require a very fine discretization or high-degree
  polynomial to approximate accurately), even when the dynamics are smooth (or
  low-degree).  The swing-up problem for the pendulum is a nice example: the
  dynamics are smooth and simple, and even with a quadratic cost, the optimal
  policy is discontinuous (there are neighboring states that will take one
  "pump" or two pumps to reach the goal).--></p>

  <p>In this chapter, we will explore another very natural idea: let us
  parameterize a controller with some decision variables, and then search over
  those decision variables directly in order to achieve a task and/or optimize
  a performance objective.  We'll refer to this broad class of methods as
  "policy search" or, when optimization methods are used, "policy
  optimization".  This idea has become very popular again lately due to the
  empirical success of "policy gradient" algorithms in reinforcement learning
  (RL).  This chapter includes a discussion of the "model-based" version of
  these RL policy-gradient algorithms; we'll describe their "model-free"
  versions in <a href="rl_policy_search.html">a future chapter</a>.
  </p>

  <section><h1>Problem formulation</h1>

    <p>Consider a static full-state feedback policy, $$\bu =
    \bpi_\balpha(\bx),$$ where $\bpi$ is potentially a nonlinear function, and
    $\balpha$ is the vector of parameters that describe the controller.  The
    control might take time as an input, or might even have it's own internal
    state, but let's start with this simple form.  </p>

    <p>Using our prescription for optimal control using additive costs, we can
    evaluate the performance of this controller from any initial condition
    using, e.g.: \begin{align*} J_\balpha(\bx) =& \int_0^\infty \ell(\bx(t),
    \bu(t)) dt, \\ \subjto \quad & \dot\bx = f(\bx, \bu), \quad \bu =
    \bpi_\balpha(\bx), \quad  \bx(0) = \bx.\end{align*}  In order to provide a
    scalar cost for each set of policy parameters, $\balpha$, we need one more
    piece: a relative importance for the different initial conditions.</p>

    <p>As we will further elaborate when we discuss <a
    href="robust.html">stochastic optimal control</a>, a very natural choice --
    one that preserves the recursive structure of the HJB -- is to optimize the
    expected value of the cost, given some distribution over initial
    conditions: $$\min_\balpha E_{\bx \sim {\mathcal X}_0} \left[J_\balpha(\bx)
    \right],$$ where ${\mathcal X}_0$ is a probability distribution over
    initial conditions, $\bx(0).$</p>

  </section>

  <section><h1>Linear Quadratic Regulator</h1>
  
    <p>To start thinking about the problem of searching directly in the policy
    parameters, it's very helpful to start with a problem we know and can
    understand well.  In LQR problems for linear, time-invariant systems, we
    know that the optimal policy is a linear function: $\bu = -{\bf K}(\bx).$
    So far, we have always obtained ${\bf K}$ indirectly -- by solving a
    Riccati equation to find the cost-to-go and then backing out the optimizing
    policy.  Here, let us study the case where we parameterize the elements of
    ${\bf K}$ as decision variables, and attempt to optimize the expected
    cost-to-go directly.</p>
  
    <subsection><h1>Policy Evaluation</h1>

      <p>First, let's evaluate our objective for a given ${\bf K}$.  This step
      is known as "<i>policy evaluation</i>".  If we use a Gaussian with mean
      zero and covariance ${\bf \Omega}$ as our distribution over initial
      conditions, then for LQR we have \begin{align*} & E\left[ \int_0^\infty
      [\bx^T{\bf Q}\bx + \bu^T\bR\bu] dt \right], \\ \subjto \quad & \dot\bx =
      {\bf A}\bx + {\bf B}\bu, \quad \bu = - {\bf K}\bx, \quad \bx(0) \sim
      \mathcal{N}(0, {\bf \Omega}).\end{align*}  First, let us re-arrange the
      cost function slightly, using the properties of the matrix trace:
      \begin{gather*} \bx^T\bQ\bx + \bx^T\bK^T\bR\bK\bx = \bx^T(\bQ +
      \bK^T\bR\bK)\bx^T = \trace\left((\bQ + \bK^T\bR\bK)\bx\bx^T\right),
      \end{gather*} and the linearity of the integral and expected value:
      $$E\left[ \int_0^\infty \trace((\bQ + \bK^T\bR\bK)\bx\bx^T) dt \right] =
      \trace\left((\bQ + \bK^T\bR\bK) E \left[\int_0^\infty \bx\bx^T dt
      \right]\right),$$ For any given initial condition, the solution of the
      closed-loop dynamics is given by the matrix exponential: $$\bx(t) =
      e^{(\bA - \bB\bK)t}\bx(0).$$  For the distribution of initial conditions,
      we have \begin{gather*}  \\ E\left[\bx(t)\bx(t)^T\right] = e^{(\bA -
      \bB\bK)t} E\left[\bx(0)\bx(0)^T\right] e^{(\bA - \bB\bK)^Tt} = e^{(\bA -
      \bB\bK)t} {\bf \Omega} e^{(\bA - \bB\bK)^Tt}, \end{gather*} which is just
      a (symmetric) matrix function of $t$.  The integral of this function,
      call it ${\bf X}$, represents the expected 'energy' of the closed-loop
      response: $${\bf X} = E\left[ \int_0^\infty \bx \bx^T dt \right].$$
      Assuming $\bK$ is stabilizing, ${\bf X}$ can be computed as the (unique)
      solution to the Lyapunov equation: $$(\bA - \bB\bK){\bf X} + {\bf X}(\bA
      - \bB\bK)^T + {\bf \Omega} = 0.$$  Finally, the total policy evaluation
      is given by \begin{equation}E_{\bx \sim \mathcal{N}(0,{\bf \Omega})} [J_\bK(\bx)] =
      \trace\left((\bQ + \bK^T\bR\bK){\bf
      X}\right)\label{eq:lqr_evaluation}.\end{equation}</p>

    </subsection>

    <subsection><h1>A nonconvex objective in ${\bf K}$</h1>

      <p>Unfortunately, the Lyapunov equation represents a nonlinear constraint
      (on the pair $\bK$, ${\bf X}$).  Indeed, it is well known that even the
      set of controllers that stabilizing a linear systems can be non-convex in
      the parameters $\bK$ when there are 3 or more state
      variables<elib>Fazel18</elib>.</p>

      <example><h1>The set of stabilizing $\bK$ can be non-convex</h1>

        <p>The following example was given in <elib>Fazel18</elib>.  Consider a
        discrete-time linear system with $\bA = {\bf I}_{3 \times 3}, \bB =
        {\bf I}_{3 \times 3}$. The controllers given by $${\bf K}_1 =
        \begin{bmatrix} 1 & 0 & -10 \\ -1 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix}
        \quad \text{and} \quad \bK_2 = \begin{bmatrix} 1 & -10 & 0 \\ 0 & 1 & 0
        \\ -1 & 0 & 1 \end{bmatrix},$$ are both stabilizing controllers for
        this system (all eigenvalues of $\bA - \bB\bK$ are inside the unit
        circle of the complex plane).  However the controller $\hat{\bK} =
        (\bK_1 + \bK_2)/2$ has two eigenvalues outside the unit circle.
           </p>
      
      </example>

      <p>Since the set of controllers that achieve finite total cost is
      non-convex, clearly the cost function we consider here is also
      non-convex.  For this problem we do know a <a href="lqr.html#lmi">change
      of variables that make the problem convex</a>, but what implications does
      this non-convexity result have for searching directly in the policy
      parameters $\bK$?</p>
  
    </subsection>

    <subsection><h1>No local minima</h1>
    
      <p>Although convexity is sufficient to guarantee that an optimization
      landscape does not have any local minima, it is not actually necessary.
      <elib>Fazel18</elib> showed that for this LQR objective, all local optima
      are in fact global optima.  This analysis was extended in
      <elib>Mohammadi19</elib> to give a simpler analysis and include
      convergence rates.</p>

      <p>How does one show that an optimization landscape has no local minima
      (even though it may be non-convex)?  One of the most popular tools is to
      demonstrate <i>gradient dominance</i> with the famous
      <i>Polyak-≈Åojasiewicz (PL) inequality</i> <elib>Karimi16</elib>.  For an
      optimization problem $$\min_{\bx \in \Re^d} f(\bx)$$ we first assume the
      function $f$ is $L$-smooth (<a
      href="https://en.wikipedia.org/wiki/Lipschitz_continuity">Lipschitz</a>
      gradients): $$\forall \bx,\bx', \quad \| \nabla f(\bx') - \nabla f(\bx)
      \|_2 \le L \| \bx' - \bx \|_2.$$ Then we say that the function satisfies
      the PL-inequality if the following holds for some $\mu > 0$: $$\forall
      \bx, \quad \frac{1}{2} \| \nabla f(\bx) \|_F^2 \ge \mu (f(\bx) - f^*),$$
      where $f^*$ is a value obtained at the optima.  In words, the gradient of
      the objective must grow faster than some quadratic function.  Note,
      however, that the distance here is measured in $f(\bx)$, not $\bx$; we do
      not require (nor imply) that the optimal solution is unique.  It clearly
      implies that for any minima, $\bx'$ with $\nabla f(x') = 0$, since the
      left-hand side is zero, we must have the right-hand side also be zero, so
      $x'$ is also a global optima: $f(x') = f^*$. </p>

      <!-- another way is to establish quasi-convexity ... etc -->
      
      <example><h1>A nonconvex function with no local minima</h1>
        
        <p>Consider the function $$f(x) = x^2 + 3 \sin^2(x).$$</p>

        <figure>
          <iframe id="igraph" scrolling="no" style="border:none;"
          seamless="seamless" src="data/pl_inequality.html" height="250"
          width="100%"></iframe>
        </figure>

        <p>We can establish that this function is not convex by observing that
        for $a=\frac{\pi}{4}$, $b=\frac{3\pi}{4}$, we have
        $$f\left(\frac{a+b}{2}\right) = \frac{\pi^2}{4} + 3 \approx 5.47 >
        \frac{f(a)+f(b)}{2} = \frac{5\pi^2}{16} + \frac{3}{2} \approx 4.58.$$
        </p>

        <p>We can establish the PL conditions using the gradient $$\nabla f(x)
        = 2x + 6 \sin(x) \cos(x).$$  We can establish that this function is
        $L$-smooth with $L=8$ by $$\| \nabla f(b) - \nabla f(a) \|_2 = |2b - 2a
        + 6\sin(b-a)| \le 8|b - a|,$$ because $\sin(x) \le x.$  Finally, we
        have gradient-dominance from the PL-inequality: $$\frac{1}{2}(2x+6sc)^2
        \ge \mu(x^2 + 3s^2),$$ with $\mu=0.175$. (I confirmed this with a small
        <a href="https://dreal.github.io/">dReal</a> <a
        href="examples/pl_inequality.py">program</a>).
          
        <figure>
          <iframe id="igraph" scrolling="no" style="border:none;"
          seamless="seamless" src="data/pl_inequality_grad.html" height="250"
          width="100%"></iframe>
        </figure>
</p>

      </example>

      <p><elib>Karimi16</elib> gives a convergence rate for convergence to an
      optima for gradient descent given the PL conditions.
      <elib>Mohammadi19</elib> showed that the gradients of the LQR cost we
      examine here with respect to $\bK$ satisfy the PL conditions.</p>

    </subsection>

    <subsection><h1>True gradient descent</h1>
    
      <p>The results described above suggest that one can use gradient descent
      to obtain the optimal controller, $\bK^*$ for LQR.  For the variations
      we've seen so far (were we know the model), I would absolutely recommend
      that solving the Riccati equations is a much better algorithm; it is
      faster and more robust, with no parameters like step-size to tune.  But
      gradient descent becomes more interesting / viable when we think of it as
      a model for a less perfect algorithm, e.g. where the plant model is not
      given and the gradients are estimated from noisy samples.</p>

      <p>It is a rare luxury, due here to our ability to integrate the linear
      plants/controllers, quadratic costs, and Gaussian initial conditions,
      that we could compute the value function exactly in
      (\ref{eq:lqr_evaluation}).  We can also compute the true gradient -- this
      is a pinnacle of exactness we should strive for in our methods but will
      rarely achieve again.  The gradient is given by $$\pd{E[J_\bK(\bx)]}{\bK}
      = 2(\bR\bK - \bB^T{\bf P}){\bf X},$$ where ${\bf P}$ satisfies another
      Lyapunov equation: $$(\bA - \bB\bK)^T{\bf P} + {\bf P}(\bA - \bB\bK) +
      \bQ + \bK^T\bR\bK = 0.$$
      </p>
      <todo>cite Jack if/when we publish our draft</todo>

      <p>Note that the term <i>policy gradient</i> used in reinforcement
      learning typically refers to the slightly different class of algorithms I
      hinted at above. In those algorithms, we use the true gradients of the
      policy, but we do not assume that we have access to the gradients of the
      plant.  They typically require many more roll-outs to estimate the
      gradients we compute here, and should only be weaker (less efficient)
      than the algorithms in this chapter.  The papers investigating the
      convergence of gradient descent for LQR have also started exploring these
      cases.  We will study these so-called <a
      href="rl_policy_search.html">model-free" policy search</a>
      algorithms soon.</p>
      
    </subsection>

  </section>

  <section><h1>More convergence results and counter-examples</h1>

    <todo>Some related results: LQG, mixed-LQR/H-inf, ...</todo>

    <p><i>more coming soon...</i></p>

    <p>Youla parameters also convexify the LQR objective and may extend more readily to nonlinear systems.  Their utility in RL was studied initially in <elib>Roberts11</elib>.</p>

    <p>The case of output feedback is not as nice.  Searching directly over static output feedback controllers, $\bu = -{\bf K}\by$, is known to be hard -- even the set of stabilizing ${\bf K}$ matrices can be disconnected.  We can see that with a simple example (given to me once during a conversation with Alex Megretski). 
    </p>

    <example><h1>Parameterizations of Static Output Feedback</h1>

    <p>Consider the single-input, single-output LTI system $$\dot{\bx} = {\bf A}\bx + {\bf B} u, \quad y = {\bf C}\bx,$$ with $${\bf A} = \begin{bmatrix} 0 & 0 & 2 \\ 1 & 0 & 0 \\ 0 & 1 & 0\end{bmatrix}, \quad {\bf B} = \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}, \quad {\bf C} = \begin{bmatrix} 1 & 1 & 3 \end{bmatrix}.$$  Here the linear static-output-feedback policy can be written as $u = -ky$, with a single scalar parameter $k$. </p>

    <p>Go ahead and make a plot of the maximum eigenvalue of the closed-loop system, as a function of $k$.  The system is only stable when this maximum eigenvalue is less than zero.  You'll find the set of stabilizing $k$'s is a disconnected set.</p>

    </example>

    <todo>Counter-example from Russo</todo>

  </section>

  <section><h1>Trajectory-based policy search</h1>


  </section>

  <section><h1>Lyapunov-based approaches to policy
    search.</h1>


  </section>

  <section><h1>Approximate Dynamic Programming</h1>

  </section>

  <!-- TODO:
  Guided Policy Search, etc (see Robert V's review);
  PILCO and PIPPS (arxiv from Doya).
  -->

</chapter>
<!-- EVERYTHING BELOW THIS LINE IS OVERWRITTEN BY THE INSTALL SCRIPT -->

<div id="references"><section><h1>References</h1>
<ol>

<li id=Fazel18>
<span class="author">Maryam Fazel and Rong Ge and Sham M. Kakade and Mehran Mesbahi</span>,
<span class="title">"Global Convergence of Policy Gradient Methods for the Linear Quadratic Regulator"</span>,
<span class="publisher">International Conference on Machine Learning</span>, <span class="year">2018</span>.

</li><br>
<li id=Mohammadi19>
<span class="author">Mohammadi, Hesameddin and Zare, Armin and Soltanolkotabi, Mahdi and Jovanovi{\'c}, Mihailo R</span>,
<span class="title">"Global exponential convergence of gradient methods over the nonconvex landscape of the linear quadratic regulator"</span>,
<span class="publisher">2019 IEEE 58th Conference on Decision and Control (CDC)</span>, pp. 7474--7479, <span class="year">2019</span>.

</li><br>
<li id=Karimi16>
<span class="author">Karimi, Hamed and Nutini, Julie and Schmidt, Mark</span>,
<span class="title">"Linear convergence of gradient and proximal-gradient methods under the polyak-{\l}ojasiewicz condition"</span>,
<span class="publisher">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</span>, pp. 795--811, <span class="year">2016</span>.

</li><br>
<li id=Roberts11>
<span class="author">John Roberts and Ian Manchester and Russ Tedrake</span>,
<span class="title">"Feedback Controller Parameterizations for Reinforcement Learning"</span>,
<span class="publisher">Proceedings of the 2011 IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning (ADPRL)</span>, <span class="year">2011</span>.
[&nbsp;<a href="http://groups.csail.mit.edu/robotics-center/public_papers/Roberts11.pdf">link</a>&nbsp;]

</li><br>
</ol>
</section><p/>
</div>

<table style="width:100%;"><tr style="width:100%">
  <td style="width:33%;text-align:left;"><a class="previous_chapter" href=trajopt.html>Previous Chapter</a></td>
  <td style="width:33%;text-align:center;"><a href=index.html>Table of contents</a></td>
  <td style="width:33%;text-align:right;"><a class="next_chapter" href=planning.html>Next Chapter</a></td>
</tr></table>

<div id="footer">
  <hr>
  <table style="width:100%;">
    <tr><td><a href="https://accessibility.mit.edu/">Accessibility</a></td><td style="text-align:right">&copy; Russ
      Tedrake, 2021</td></tr>
  </table>
</div>


</body>
</html>
