{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "2dd04a7e788445abad3de20cc5c7b7b9",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "# ROA Estimation for the Time-Reversed Van der Pol Oscillator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "09ef0848bb8345d897d220a6a1354df6",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 3,
    "execution_start": 1678230395235,
    "source_hash": "aa09f44"
   },
   "outputs": [],
   "source": [
    "# python libraries\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "\n",
    "# pydrake imports\n",
    "from pydrake.all import (\n",
    "    LinearQuadraticRegulator,\n",
    "    MathematicalProgram,\n",
    "    RealContinuousLyapunovEquation,\n",
    "    Solve,\n",
    "    Variables,\n",
    ")\n",
    "from pydrake.examples import VanDerPolOscillator\n",
    "\n",
    "# underactuated imports\n",
    "from underactuated import plot_2d_phase_portrait\n",
    "\n",
    "# increase default size matplotlib figures\n",
    "rcParams[\"figure.figsize\"] = (6, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "73f75ad88eb94e3eb922abebaf18371a",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## Problem Description\n",
    "In this notebook we will study the time-reversed Van der Pol oscillator.\n",
    "The equations of motion for this system are polynomial, and read as follows:\n",
    "$$\\begin{aligned}\\dot x_1 &= - x_2, \\\\ \\dot x_2 &= x_1 + (x_1^2 - 1) x_2.\\end{aligned}$$\n",
    "We compactly represent the latter as $\\dot{\\mathbf{x}} = f(\\mathbf{x})$, with $\\mathbf{x} = [x_1, x_2]^T$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "311bf19310c84879a72e4a8a0384d078",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 6,
    "execution_start": 1678228164691,
    "source_hash": "b3959b58"
   },
   "outputs": [],
   "source": [
    "# function that implements the time-reversed Van der Pol dynamics\n",
    "f = lambda x: [-x[1], x[0] + (x[0] ** 2 - 1) * x[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "5ec50db5d14846fc86c0b80fd64ddd45",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "Here is the phase portrait of the time-reversed Van der Pol oscillator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "a678c0ececec4af89e455889890c555b",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 949,
    "execution_start": 1678228167554,
    "source_hash": "4a379c87"
   },
   "outputs": [],
   "source": [
    "# compute and plot the unstable periodic orbit\n",
    "limit_cycle = VanDerPolOscillator.CalcLimitCycle()\n",
    "plt.plot(\n",
    "    limit_cycle[0],\n",
    "    limit_cycle[1],\n",
    "    color=\"b\",\n",
    "    linewidth=3,\n",
    "    label=\"ROA boundary\",\n",
    ")\n",
    "plt.legend(loc=1)\n",
    "\n",
    "# plot the phase portrait\n",
    "xlim = (-3, 3)\n",
    "plot_2d_phase_portrait(f, x1lim=xlim, x2lim=xlim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "a519d312f77e4c50a551c10ae33e2f90",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "\n",
    "As you can see, the origin of this system is locally asymptotically stable whereas, outside the Region Of Attraction (ROA) of the origin, the trajectories escape to infinity. The boundary of the ROA is an *unstable periodic orbit*: if the system state at time $t=0$ is exactly on this curve, the oscillator will orbit around the origin forever. However, any disturbance will make the system either converge to the origin or escape to infinity. Notice that the shape of this ROA is nontrivial (it is not even a convex set) and no analytic description of it is available.\n",
    "\n",
    "**Note:** Reversing the sign of $f$, we obtain the [classical Van der Pol oscillator](https://en.wikipedia.org/wiki/Van_der_Pol_oscillator); for which the above periodic orbit is a (globally) asymptotically stable [limit cycle (https://underactuated.csail.mit.edu/simple_legs.html#limit_cycle) and the origin is an unstable equilibrium. Here we reverse time (i.e. change the sign of $f$) to make the origin a stable equilibrium.\n",
    "\n",
    "**In this notebook we will use Sums-Of-Squares (SOS) optimization to find an inner approximation of the ROA of the equilibrium point in the origin. We will write three different SOS optimizations, and analyze their pros and cons.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "c023cc0fb34d41ae9f64c17fa4245ff9",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## Lyapunov Function via Linearization\n",
    "The approach we will follow to estimate the ROA of the oscillator is the following:\n",
    "\n",
    "- We linearize the dynamics $\\dot{\\mathbf{x}} = f(\\mathbf{x})$ around the origin, to get $\\dot{\\mathbf{x}} = A \\mathbf{x}$.\n",
    "\n",
    "- We solve the Lyapunov equation $$A^T P + P A = - Q$$ to get the positive definite matrix $P$, and the Lyapunov function $V(\\mathbf{x}) = \\mathbf{x}^T P \\mathbf{x}$ for the linearized system. **Note**: A standard choice for $Q$ is an identity matrix.\n",
    "\n",
    "- Lyapunov theory tells us that if $A$ is strictly stable (all its eigenvalues have strictly negative real part) then the origin is a locally asymptotically stable equilibrium point for the nonlinear system $\\dot{\\mathbf{x}} = f(\\mathbf{x})$.\n",
    "Moreover, a conservative approximation of the ROA can be obtained using the Lyapunov function $V(\\mathbf{x})$ derived in the linear analysis.\n",
    "\n",
    "- We consider the level sets $$L(\\rho) = \\{ \\mathbf{x} : V(\\mathbf{x}) \\leq \\rho \\},$$ and we look for the maximum value of $\\rho$ such that $$\\dot{V}(\\mathbf{x}) = \\frac{\\partial V}{\\partial \\mathbf{x}} f(\\mathbf{x}) = 2 \\mathbf{x}^T P f(\\mathbf{x}) < 0, \\quad \\forall \\mathbf{x} \\in L(\\rho)\\backslash \\{0\\}.$$\n",
    "In words, we try to find the largest level set $L(\\rho)$ entirely contained the region of space where $\\dot{V}(\\mathbf{x})$ is negative.\n",
    "Lyapunov theory tells us that any trajectory that starts inside such a set will eventually converge to the origin.\n",
    "\n",
    "We start by deriving $V(\\mathbf{x})$. In the next cell, write down the matrix $A$ (obtained by linearizing $f(x)$ near the origin), the positive definite matrix $Q$, and compute $P$ using the drake function `RealContinuousLyapunovEquation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "39afd6cf40a446e48420d63a11e0862c",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 3,
    "execution_start": 1678228174996,
    "source_hash": "d33dcca9"
   },
   "outputs": [],
   "source": [
    "A = np.eye(2)  # MODIFY HERE\n",
    "Q = np.eye(2)\n",
    "P = np.eye(2)  # MODIFY HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "cc1d3912bb464a0dad9cfa13d853a4af",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "### Manual Check\n",
    "The advantage of working in 2D is that we can plot things!\n",
    "Before starting with complicated optimizations, let us plot $V(\\mathbf{x})$ and $\\dot{V}(\\mathbf{x})$ to get a sense of what we are actually looking for in this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "09e5a0aa5c3f428f9bc638fa172a9e4d",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 18,
    "execution_start": 1678230488956,
    "source_hash": "cd1f01fc"
   },
   "outputs": [],
   "source": [
    "# function that given rho plots the boundary\n",
    "# of the the set L(rho) defined above\n",
    "def plot_V(rho):\n",
    "    # grid of the state space\n",
    "    x1 = np.linspace(*xlim)\n",
    "    x2 = np.linspace(*xlim)\n",
    "    X1, X2 = np.meshgrid(x1, x2)\n",
    "\n",
    "    # function that evaluates V(x) at a given x\n",
    "    # (looks bad, but it must accept meshgrids)\n",
    "    eval_V = lambda x: sum(\n",
    "        sum(x[i] * x[j] * Pij for j, Pij in enumerate(Pi)) for i, Pi in enumerate(P)\n",
    "    )\n",
    "\n",
    "    # contour plot with only the rho level set\n",
    "    cs = plt.contour(\n",
    "        X1,\n",
    "        X2,\n",
    "        eval_V([X1, X2]),\n",
    "        levels=[rho],\n",
    "        colors=\"r\",\n",
    "        linewidths=3,\n",
    "        zorder=3,\n",
    "    )\n",
    "\n",
    "    # misc plot settings\n",
    "    plt.xlabel(r\"$x_1$\")\n",
    "    plt.ylabel(r\"$x_2$\")\n",
    "    plt.gca().set_aspect(\"equal\")\n",
    "\n",
    "    # fake plot for legend\n",
    "    plt.plot(\n",
    "        0,\n",
    "        0,\n",
    "        color=\"r\",\n",
    "        linewidth=3,\n",
    "        label=r\"$\\{ \\mathbf{x} : V(\\mathbf{x}) = \\rho \\}$\",\n",
    "    )\n",
    "    plt.legend()\n",
    "\n",
    "    return cs\n",
    "\n",
    "\n",
    "# function that plots the levels sets of Vdot(x)\n",
    "def plot_Vdot():\n",
    "    # grid of the state space\n",
    "    x1 = np.linspace(*xlim)\n",
    "    x2 = np.linspace(*xlim)\n",
    "    X1, X2 = np.meshgrid(x1, x2)\n",
    "\n",
    "    # function that evaluates Vdot(x) at a given x\n",
    "    eval_Vdot = lambda x: 2 * sum(\n",
    "        sum(x[i] * f(x)[j] * Pij for j, Pij in enumerate(Pi)) for i, Pi in enumerate(P)\n",
    "    )\n",
    "\n",
    "    # contour plot with only the rho level set\n",
    "    cs = plt.contour(\n",
    "        X1,\n",
    "        X2,\n",
    "        eval_Vdot([X1, X2]),\n",
    "        colors=\"b\",\n",
    "        levels=np.linspace(-10, 40, 11),\n",
    "    )\n",
    "    plt.gca().clabel(cs, inline=1, fontsize=10)\n",
    "\n",
    "    # misc plot settings\n",
    "    plt.xlabel(r\"$x_1$\")\n",
    "    plt.ylabel(r\"$x_2$\")\n",
    "    plt.gca().set_aspect(\"equal\")\n",
    "\n",
    "    # fake plot for legend\n",
    "    plt.plot(0, 0, color=\"b\", label=r\"$\\dot{V}(\\mathbf{x})$\")\n",
    "    plt.legend()\n",
    "\n",
    "    return cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "a945460b900941589577ed7d2b7156ac",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 610,
    "execution_start": 1678230489945,
    "source_hash": "8704eb86"
   },
   "outputs": [],
   "source": [
    "# tune rho by hand to make it as big as possible\n",
    "# while staying in the region where Vdot(x) is negative\n",
    "rho_max = 2.3\n",
    "\n",
    "# plot Vdot(x) and V(x) = rho\n",
    "Vdot_cs = plot_Vdot()\n",
    "V_cs = plot_V(rho_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "653cef441e894b5fb4aa7934dce91e85",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "With correct values of $A$ and $P$, you will see that the largest $\\rho$ for which $\\dot{V}(\\mathbf{x}) < 0$ is $\\approx 2.3$. The red elliptical curve (that represents the 2.3 level set for $V(\\mathbf{x})$) touches the zero-valued contours for $\\dot{V}(\\mathbf{x})$. \n",
    "Superimposing the level set to the phase portrait, we see that this is a pretty good approximation of the ROA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "179c3b8010a947ebb2d34ae2f28cab21",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 439,
    "execution_start": 1678230710005,
    "source_hash": "8db6c40c"
   },
   "outputs": [],
   "source": [
    "plot_2d_phase_portrait(f, x1lim=xlim, x2lim=xlim)\n",
    "plot_V(rho_max)\n",
    "plt.plot(\n",
    "    limit_cycle[0],\n",
    "    limit_cycle[1],\n",
    "    color=\"b\",\n",
    "    linewidth=3,\n",
    "    label=\"ROA boundary\",\n",
    ")\n",
    "plt.legend(loc=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "5a8658e82e2449acbaa694c4ac33f84d",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "Of course, when $\\text{dim}(\\mathbf{x}) > 2$, a \"manual\" maximization of $\\rho$ has no hope to work.\n",
    "That's where SOS programming really makes the difference!\n",
    "The goal of this notebook is to experiment these tools on a case where things can actually be visualized, so that we get a better sense of the power of this technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "62de3e11973e442b844c583297a990ee",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## Method 1: Line-Search on $\\rho$\n",
    "The first method we use to estimate the ROA is the one from the textbook example \"[Region of attraction for the one-dimensional cubic system](https://underactuated.csail.mit.edu/lyapunov.html#roa_cubic_system)\".\n",
    "\n",
    "We look for the largest $\\rho$ for which there exists a SOS polynomial $\\lambda(\\mathbf{x})$ such that $$- \\dot{V}(\\mathbf{x}) - \\lambda(\\mathbf{x}) (\\rho - V(\\mathbf{x})) - \\epsilon \\mathbf{x}^T \\mathbf{x} \\ \\text{is SOS},$$\n",
    "with $\\epsilon$ very small.\n",
    "This problem cannot be written as a single SOS program, since both the polynomial $\\lambda$ and scalar $\\rho$ are decision variables, which are here multiplied together. Hence we naively solve a sequence of SOS programs with increasing value of $\\rho$.\n",
    "\n",
    "The intuition behind this formulation is the following.\n",
    "Think of the condition \"is SOS\" as \"$\\geq 0$\" (actually, the first is sufficient for the second).\n",
    "Then what we are asking is $- \\dot{V}(\\mathbf{x}) \\geq \\lambda(\\mathbf{x}) (\\rho - V(\\mathbf{x})) + \\epsilon \\mathbf{x}^T \\mathbf{x}$. \n",
    "Inside the level set $L(\\rho)$, we have $\\rho - V(\\mathbf{x}) \\geq 0$ and, since $\\lambda(\\mathbf{x})$ is SOS, $\\lambda(\\mathbf{x}) (\\rho - V(\\mathbf{x})) \\geq 0$.\n",
    "Thus the condition above is just sayng that, for all $\\mathbf{x}$ in $L(\\rho)$, we must have $- \\dot{V}(\\mathbf{x})\\geq \\epsilon \\mathbf{x}^T \\mathbf{x}$, i.e., $\\dot{V}(\\mathbf{x})$ negative definite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "391f396319db458cb6a20feda8325610",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "Carefully read the following code, as later you will be asked to write similar mathmatical programs. The following drake [tutorial on SOS](https://deepnote.com/workspace/Drake-0b3b2c53-a7ad-441b-80f8-bf8350752305/project/Tutorials-2b4fc509-aef2-417d-a40d-6071dfed9199/notebook/sum_of_squares_optimization-4137868b87ee423dbc50d0076f72bbb9) can be helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "allow_embed": false,
    "cell_id": "29610a47696146c597244af8c8f7c407",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 3,
    "execution_start": 1678230719391,
    "source_hash": "81bfd7c6"
   },
   "outputs": [],
   "source": [
    "# function that verifies the condition described above\n",
    "# for the level set L(rho) for a given rho\n",
    "def is_verified(rho):\n",
    "    # initialize optimization problem\n",
    "    # (with Drake there is no need to specify that\n",
    "    # this is going to be a SOS program!)\n",
    "    prog = MathematicalProgram()\n",
    "\n",
    "    # SOS indeterminates\n",
    "    x = prog.NewIndeterminates(2, \"x\")\n",
    "\n",
    "    # Lyapunov function\n",
    "    V = x.dot(P).dot(x)\n",
    "    V_dot = 2 * x.dot(P).dot(f(x))\n",
    "\n",
    "    # degree of the polynomial lambda(x)\n",
    "    # no need to change it, but if you really want to,\n",
    "    # keep l_deg even (why?) and do not set l_deg greater than 10\n",
    "    # (otherwise optimizations will take forever)\n",
    "    l_deg = 4\n",
    "    assert l_deg % 2 == 0\n",
    "\n",
    "    # SOS Lagrange multipliers\n",
    "    l = prog.NewSosPolynomial(Variables(x), l_deg)[0].ToExpression()\n",
    "\n",
    "    # main condition above\n",
    "    eps = 1e-3  # do not change\n",
    "    prog.AddSosConstraint(-V_dot - l * (rho - V) - eps * x.dot(x))\n",
    "\n",
    "    # solve SOS program\n",
    "    # no objective function in this formulation\n",
    "    result = Solve(prog)\n",
    "\n",
    "    # return True if feasible, False if infeasible\n",
    "    return result.is_success()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "011e63360be64b869fb1411cfd189e50",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "Now that we have the building block of our algorithm, it's your time to write the line search to find the maximum $\\rho$ for which the condition described above holds.\n",
    "\n",
    "Implement the line search in next cell.\n",
    "Start with `rho = 0`, check if the level set $L(\\rho)$ is verified (i.e. the function `is_verified(rho)` returns `True`); if yes, increase `rho` by `rho_step = .01`, if no, assign to the variable `rho_method_1` the maximum verified `rho` you've found with this procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "51daeaa64dd24e9a96fe2926cc8ccc27",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 2,
    "execution_start": 1678230727013,
    "source_hash": "e7b94275"
   },
   "outputs": [],
   "source": [
    "# implement your line-search here\n",
    "\n",
    "# set the maximum value of rho you've found with line search\n",
    "rho_method_1 = 0  # MODIFY HERE\n",
    "\n",
    "print(f\"Method 1 verified rho = {rho_method_1}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "16ce3bb868744a77b7398568e4c92a0f",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "Did this method do a good job in approximating (from below) the maximum $\\rho$ we have found by hand?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "a6dbbcb083ef41dc95949bf41086f4e0",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## Method 2: Single-Shot SOS Program\n",
    "With the previous formulation we had to solve a sequence of SOS programs, now we consider an equivalent formulation of the SOS problem in which we can directly maximize $\\rho$.\n",
    "In the previous case we wrote a SOS program to check the impication\n",
    "$$\\mathbf{x} \\in L(\\rho) \\Rightarrow \\dot{V}(\\mathbf{x}) < 0.$$\n",
    "This, however, can be equivalently stated as\n",
    "$$\\dot{V}(\\mathbf{x}) \\geq 0 \\Rightarrow \\mathbf{x} \\not\\in L(\\rho).$$\n",
    "Expressing $\\mathbf{x} \\not\\in L(\\rho)$ as $V(\\mathbf{x}) - \\rho \\geq 0$, it turns out that the latter condition can be verified with a single SOS program.\n",
    "(Actually, we should say $V(\\mathbf{x}) - \\rho > 0$, but working with a computer there is no difference.)\n",
    "\n",
    "The new problem reads as follows.\n",
    "Find an SOS polynomial $\\lambda(\\mathbf{x})$ such that\n",
    "$$V(\\mathbf{x}) - \\rho - \\lambda(\\mathbf{x}) \\dot{V}(\\mathbf{x}) \\ \\text{is SOS}.$$\n",
    "In fact, this implies $V(\\mathbf{x}) - \\rho \\geq \\lambda(\\mathbf{x}) \\dot{V}(\\mathbf{x})$ and, for all $\\mathbf{x}$ where $\\dot{V}(\\mathbf{x}) \\geq 0$, we get $V(\\mathbf{x}) - \\rho \\geq 0$.\n",
    "\n",
    "Notice that now $\\lambda$ does not multiply $\\rho$, and we can search over both of them at the same time.\n",
    "Hence we can ask the optimizer to maximize $\\rho$.\n",
    "There is however an issue with the current problem formulation...\n",
    "\n",
    "### Not quite there yet...\n",
    "\n",
    "Do you see anything wrong with the problem formulation we put together so far? What do you think the maximum $\\rho$ will be?\n",
    "\n",
    "As stated so far, the problem will always return $\\rho = 0$!\n",
    "To see why, first notice that for $\\rho = \\lambda = 0$ the SOS condition above would become $V(\\mathbf{x})$ is SOS, which holds since $V(\\mathbf{x}) = \\mathbf{x}^T P \\mathbf{x}$.\n",
    "Now consider a positive $\\rho$.\n",
    "Since $V(0) = \\dot{V} (0) = 0$, evaluating the SOS condition in the origin, we would get $-\\rho \\geq 0$ which can never hold!\n",
    "\n",
    "Not everything is lost, we have a neat fix for you.\n",
    "Certainly, if $V(\\mathbf{x}) - \\rho$ is nonnegative, so is $\\mathbf{x}^T\\mathbf{x}(V(\\mathbf{x}) - \\rho)$.\n",
    "Hence we consider the SOS condition\n",
    "$$\\mathbf{x}^T\\mathbf{x}(V(\\mathbf{x}) - \\rho) - \\lambda(\\mathbf{x}) \\dot{V}(\\mathbf{x}) \\ \\text{is SOS},$$\n",
    "with $\\lambda(\\mathbf{x})$ SOS.\n",
    "Now the issue in the origin is fixed, since for $\\mathbf{x} = 0$, we get \"$0$ is SOS\", which is always true.\n",
    "Moreover, where $\\mathbf{x}$ is such that $\\dot{V}(\\mathbf{x}) \\geq 0$, the new SOS condition requires $\\mathbf{x}^T\\mathbf{x}(V(\\mathbf{x}) - \\rho) \\geq 0$ and hence $V(\\mathbf{x}) - \\rho \\geq 0$ as desired.\n",
    "\n",
    "Now we are good to go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "f06b90d969294a21a68c64e423bba4dc",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "In the next cell you need to code the SOS program we just described. Refer to the previous code block and the [drake tutorial](https://deepnote.com/workspace/Drake-0b3b2c53-a7ad-441b-80f8-bf8350752305/project/Tutorials-2b4fc509-aef2-417d-a40d-6071dfed9199/notebook/sum_of_squares_optimization-4137868b87ee423dbc50d0076f72bbb9) for hint on writing SOS programs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "83557c28bdca4004a970d5bd7aeaf33c",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 2,
    "execution_start": 1678230730517,
    "source_hash": "ea4a7205"
   },
   "outputs": [],
   "source": [
    "# initialize optimization problem\n",
    "prog2 = MathematicalProgram()  # Do not modify\n",
    "\n",
    "# 1. define SOS indeterminates\n",
    "\n",
    "# 2. define Lyapunov function and its derivative\n",
    "\n",
    "# 3. define SOS Lagrange multipliers, with any even degree between 4 and 10\n",
    "\n",
    "# 4. define 'rho' (level set) as optimization variable\n",
    "\n",
    "# 5. write the SOS constraints as discussed above\n",
    "\n",
    "# 6. write the objective function (use AddLinearCost)\n",
    "\n",
    "# 7. call the solver on the mathematical program, and set the value of rho_method_2 below.\n",
    "\n",
    "rho_method_2 = 0\n",
    "\n",
    "print(f\"Method 2 verified rho = {rho_method_2}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "bdc594219a244ab28a6f89ecc6b5be75",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## Method 3: Smarter Single-Shot SOS Program\n",
    "The SOS program we just wrote was already a satisfying solution, but it turns out we can do even better!\n",
    "From the textbook chapter \"[Lyapunov analysis with convex optimization](https://underactuated.csail.mit.edu/lyapunov.html#optimization)\", you know that every SOS constraint brings with it a lot of optimization variables and an SDP constraint.\n",
    "So, whenever we can, removing redundant SOS requirements is always a good thing to do.\n",
    "\n",
    "We claim that in the previous formulation we don't need $\\lambda(\\mathbf{x})$ to be SOS. How is this possible?\n",
    "\n",
    "We start by noticing that, in a neighborhood of the origin, excluded the origin itself, $\\dot{V}(\\mathbf{x})$ is strictly negative.\n",
    "(This because $V(\\mathbf{x})$ is a Lyapunov function for the linearized system hence, locally, it works also for the nonlinear system.)\n",
    "\n",
    "In light of the latter observation, instead of asking that $\\dot{V}(\\mathbf{x})$ is negative for all $\\mathbf{x} \\neq 0$ in $L(\\rho)$, we can equivalently ask that all the points $\\mathbf{x} \\neq 0$ where $\\dot{V}(\\mathbf{x}) = 0$ must be outside the level set $L(\\rho)$.\n",
    "This might take a second to parse!\n",
    "\n",
    "The latter condition can be enforced exactly as the one above:\n",
    "$$\\mathbf{x}^T\\mathbf{x}(V(\\mathbf{x}) - \\rho) - \\lambda(\\mathbf{x}) \\dot{V}(\\mathbf{x}) \\ \\text{is SOS},$$\n",
    "but this time we do not require $\\lambda(\\mathbf{x})$ to be SOS.\n",
    "\n",
    "Here is the reasoning.\n",
    "First, notice that this condition implies $\\mathbf{x}^T\\mathbf{x}(V(\\mathbf{x}) - \\rho) \\geq \\lambda(\\mathbf{x}) \\dot{V}(\\mathbf{x})$.\n",
    "Then, observe that for all $\\mathbf{x}$ such that $\\dot{V}(\\mathbf{x}) = 0$, we get $\\mathbf{x}^T\\mathbf{x}(V(\\mathbf{x}) - \\rho) \\geq 0$.\n",
    "This implies $V(\\mathbf{x}) - \\rho \\geq 0$, i.e., $\\mathbf{x} \\not\\in L(\\rho)$ as desired.\n",
    "(As before, no need to care about what happens at the boundary of the level set.)\n",
    "\n",
    "This trick can make a huge difference when you need to verify high-dimensional systems!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "330d739ccc574e07bd17364a6daac680",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "To try this new idea, in the next cell define the third mathematical program that has only one SOS constraint. **Note:** Do not use `NewSosPolynomial` for defining $\\lambda(x)$, instead use `NewFreePolynomial`. If you have done thing correctly, `rho_method_3` should \"closely match\" `rho_method_2`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "fc903f5dd0454d3b899f0b5a307ac51b",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 2,
    "execution_start": 1678230733277,
    "source_hash": "759c025e"
   },
   "outputs": [],
   "source": [
    "# initialize optimization problem\n",
    "prog3 = MathematicalProgram()  # Do not modify\n",
    "\n",
    "# Write the modified Single-Shot SOS program here. Make sure there is only one SOS constraint.\n",
    "\n",
    "rho_method_3 = 0  # MODIFY HERE\n",
    "\n",
    "print(f\"Method 3 verified rho = {rho_method_3}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "581c8d5946c94ed6a62dc53ae3f9902f",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## Final Note\n",
    "More advanced techniques to approximate ROAs are available.\n",
    "Generally they require some sort of alternation between optimizing over the multiplier $\\lambda$ (as we did here) and modifying the shape of the Lyapunov function $V(\\mathbf{x})$, e.g., by considering higher-order polynomials (here we stuck to the quadratic one coming from\n",
    "linear analysis).\n",
    "The level set $\\rho$ is generally kept fixed (e.g. equal to unity) since, when reshaping the Lyapunov function, the optimizer is allowed to scale the range of this function arbitrarily.\n",
    "\n",
    "Here is an image of SOS in its full glory approximating the ROA of the Van der Pol oscillator.\n",
    "Impressive, isn't it?!\n",
    "\n",
    "![figure](https://raw.githubusercontent.com/RussTedrake/underactuated/master/book/figures/exercises/van_der_pol_roa.png)\n",
    "(Courtesy of Shen Shen.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "de7705722adc449ca907e298a722e61c",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## Autograding\n",
    "You can check your work by running the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "6186447f664648818b01d86f894604b2",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 4,
    "execution_start": 1678230738080,
    "source_hash": "667bdfaf"
   },
   "outputs": [],
   "source": [
    "from underactuated.exercises.grader import Grader\n",
    "from underactuated.exercises.lyapunov.test_van_der_pol import TestVanDerPol\n",
    "\n",
    "Grader.grade_output([TestVanDerPol], [locals()], \"results.json\")\n",
    "Grader.print_test_results(\"results.json\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "deepnote": {},
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "960f3b5f0ba24cd5997f6f05f5919be9",
  "kernelspec": {
   "display_name": "drake_env",
   "language": "python",
   "name": "drake_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}