<!DOCTYPE html>

<html>

  <head>
    <title>Ch. 12 - Motion Planning as
Search</title>
    <meta name="Ch. 12 - Motion Planning as
Search" content="text/html; charset=utf-8;" />
    <link rel="canonical" href="http://underactuated.mit.edu/planning.html" />

    <script src="https://hypothes.is/embed.js" async></script>
    <script type="text/javascript" src="chapters.js"></script>
    <script type="text/javascript" src="htmlbook/book.js"></script>

    <script src="htmlbook/mathjax-config.js" defer></script>
    <script type="text/javascript" id="MathJax-script" defer
      src="htmlbook/MathJax/es5/tex-chtml.js">
    </script>
    <script>window.MathJax || document.write('<script type="text/javascript" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" defer><\/script>')</script>

    <link rel="stylesheet" href="htmlbook/highlight/styles/default.css">
    <script src="htmlbook/highlight/highlight.pack.js"></script> <!-- http://highlightjs.readthedocs.io/en/latest/css-classes-reference.html#language-names-and-aliases -->
    <script>hljs.initHighlightingOnLoad();</script>

    <link rel="stylesheet" type="text/css" href="htmlbook/book.css" />
  </head>

<body onload="loadChapter('underactuated');">

<div data-type="titlepage">
  <header>
    <h1><a href="index.html" style="text-decoration:none;">Underactuated Robotics</a></h1>
    <p data-type="subtitle">Algorithms for Walking, Running, Swimming, Flying, and Manipulation</p>
    <p style="font-size: 18px;"><a href="http://people.csail.mit.edu/russt/">Russ Tedrake</a></p>
    <p style="font-size: 14px; text-align: right;">
      &copy; Russ Tedrake, 2024<br/>
      Last modified <span id="last_modified"></span>.</br>
      <script>
      var d = new Date(document.lastModified);
      document.getElementById("last_modified").innerHTML = d.getFullYear() + "-" + (d.getMonth()+1) + "-" + d.getDate();</script>
      <a href="misc.html">How to cite these notes, use annotations, and give feedback.</a><br/>
    </p>
  </header>
</div>

<p><b>Note:</b> These are working notes used for <a
href="https://underactuated.csail.mit.edu/Spring2024/">a course being taught
at MIT</a>. They will be updated throughout the Spring 2024 semester.  <a
href="https://www.youtube.com/playlist?list=PLkx8KyIQkMfU5szP43GlE_S1QGSPQfL9s">Lecture videos are available on YouTube</a>.</p>

<table style="width:100%;"><tr style="width:100%">
  <td style="width:33%;text-align:left;"><a class="previous_chapter" href=policy_search.html>Previous Chapter</a></td>
  <td style="width:33%;text-align:center;"><a href=index.html>Table of contents</a></td>
  <td style="width:33%;text-align:right;"><a class="next_chapter" href=feedback_motion_planning.html>Next Chapter</a></td>
</tr></table>

<script type="text/javascript">document.write(notebook_header('planning'))
</script>
<!-- EVERYTHING ABOVE THIS LINE IS OVERWRITTEN BY THE INSTALL SCRIPT -->
<chapter style="counter-reset: chapter 11"><h1>Motion Planning as
Search</h1>

  <p>The term "motion planning" is a quite general term which almost certainly
  encompasses the dynamic programming, feedback design, and trajectory optimization
  algorithms that we have already discussed.  However, there are a number of algorithms
  and ideas that we have not yet discussed which have grown from the idea of formulating
  motion planning as a search problem -- for instance searching for a path from a start
  to a goal in a graph which is too large solve completely with dynamic programming.
  <!--Some, but certainly not all, of these algorithms sacrifice optimality in order to
  find any path if it exists, and the notion of a planner being "complete" -- guaranteed
  to find a path if one exists -- is highly valued.-->  </p>
  
  <p>My goal for this chapter is to introduce these additional tools based on search
  into our toolkit.  For robotics, they will play a particularly valuable role when the
  planning problem is geometrically complex (e.g. a robot moving around obstacles in 3D)
  or the optimization landscape is very nonconvex, because these are the problems where
  the nonlinear trajectory optimization formulations that we've studied before will
  potentially suffer badly from local minima. Many of these algorithms were developed
  initially for discrete or purely kinematic planning problems; a major theme of this
  chapter will be the adaptations that allow them to work for underactuated systems.
  There are also many deep mathematical connections between discrete search and
  <i>combinatorial</i> optimization; I hope to make a few of those connections
  for you here.</p>

  <p><elib>LaValle06</elib> is a very nice book on planning algorithms in general and on
  motion planning algorithms in particular.  Compared to other planning problems,
  <em>motion planning</em> typically refers to problems where the planning domain is
  continuous (e.g. continuous state space, continuous action space), but many motion
  planning algorithms trace their origins back to ideas in discrete domains.  The term
  <em>kinodynamic motion planning</em> is often used when the plant is subject to
  dynamic constraints like underactuation, in addition to kinematic constraints like
  obstacle avoidance.</p>

  <!--
  <p>For this chapter, we will consider the following problem formulation:
  given a system defined by the nonlinear dynamics (in continuous- or
  discrete-time) $$\dot{\bx} = f(\bx,\bu) \quad \text{or} \quad \bx[n+1] =
  f(\bx[n],\bu[n]),$$ and given a start state $\bx(0) = \bx_0$ and a goal
  region ${\cal G}$, find any finite-time trajectory from $\bx_0$ to to $\bx
  \in {\cal G}$ if such a trajectory exists.</p>-->

  <section><h1>Artificial Intelligence as Search</h1>

    <p>Before the recent rise of <a href="https://openai.com/blog/chatgpt">large
    language models (LLMs)</a>, the . For decades, many AI researchers felt that the
    route to creating intelligent machines was to collect large ontologies of knowledge,
    and then perform very efficient search. Some notable examples from this storied
    history include Samuel's checker players, theorem proving, Cyc, Deep Blue playing
    chess, and IBM Watson. Now it seems that LLMs are providing evidence that humans
    have put enough "knowledge" on the web such that relatively simple learning
    paradigms like next-word prediction can lead to a surprising level of
    "intelligence". As of early 2023, the word on the street is that these models are
    not yet good at long-term reasoning -- making multi-step plans with more than a few
    steps
    <elib>Bubeck23</elib>. But they appear to be able to use (software) tools, and
    <i>maybe</i> the tools that they need are basic planning/search algorithms.</p>

    <p>Indeed, thanks to decades of research, planning algorithms in AI have
    also scaled to impressive heights, making efficient use of heuristics and
    factorizations to solve impressively large planning instances. Since 1998,
    the International Conference on Automated Planning and Scheduling (ICAPS)
    has been hosting <a
    href="https://www.icaps-conference.org/competitions/">regular planning
    competitions</a> which have helped to solidify <a
    href="https://en.wikipedia.org/wiki/Planning_Domain_Definition_Language">problem
    specification formats</a> and to benchmark the state of the art.</p>

    <p>These algorithms have focused on primarily on logical/discrete planning
    (although they do support "objects", and so can have a sort of open
    vocabulary).  In the language of underactuated, this connects back to the
    discrete-state, discrete-action, discrete-time planning problems that we
    discussed to introduce <a href="dp.html#graph_search">dynamic programming
    as graph search</a>. Dynamic programming is a very efficient algorithm to
    solve for the cost-to-go from <i>every</i> state, but if we only need to
    find an (optimal) path from a single start state to the goal, we can
    potentially do better.  In particular, in order to scale to very large
    planning instances, one of the essential ideas is the idea of "incremental"
    search, which can avoid every putting the entire (often exponentially
    large) graph into memory.</p>
    
    <p>Is it possible to find an optimal path from the start to a goal without
    visiting every node? Yes! Indeed, one of the key insights that powers these
    incremental algorithms is the use of <a
    href="https://en.wikipedia.org/wiki/Admissible_heuristic">admissible
    heuristics</a> to guide the search -- an approximate cost-to-go which is
    guaranteed to never over-estimate the cost-to-go. A great example of this
    would be searching for directions on a map -- the straight-line distance
    from the start to the goal ("as the crow flies") is guaranteed to never
    over-estimate the true cost to go (which has to stay on the roads). The
    most famous search algorithm to leverage these heuristics is <a
    href="https://en.wikipedia.org/wiki/A*_search_algorithm">A*</a>. In
    robotics, we often use online planning extensions, such as <a
    href="https://en.wikipedia.org/wiki/D*">D* and D*-Lite</a>.
    </p>

    <p>There are numerous other heuristics that power state-of-the-art
    large-scale logical search algorithms. Another important one is
    factorization. For a robotics example, consider a robot manipulating many
    possible objects -- it's reasonable to plan the manipulation of one object
    assuming it's independent of the other objects and then to revise that plan
    only when the optimal plan ends up putting two objects on intersecting
    paths. Many of these heuristics are summarized nicely in the Fast Downward
    paper<elib>Helmert06</elib>; Fast Downward has been at the forefront of the
    ICAPS planning competitions for many years.</p>

    <p>Some of the most visible success stories in deep learning today still
    make use of planning. For example: DeepMind's
      <a href="https://en.wikipedia.org/wiki/AlphaGo">AlphaGo</a> and <a
        href="https://en.wikipedia.org/wiki/AlphaZero">AlphaZero</a> combine the planning algorithms developed over the
      years in discrete games , notably <a href="https://en.wikipedia.org/wiki/Monte_Carlo_tree_search">Monte-Carlo Tree
        Search (MCTS)</a>, with learned heuristics in the form of policies and value functions.</p>

  </section>

  <section><h1>Sampling-based motion planning</h1>

    <p>If you remember how we introduced dynamic programming initially as a
    graph search, you'll remember that there were some challenges in
    discretizing the state space.  Let's assume that we have discretized the
    continuous space into some finite set of discrete nodes in our graph.  Even
    if we are willing to discretize the action space for the robot (this might
    even be acceptable in practice), we had a problem where discrete actions
    from one node in the graph, integrated over some finite interval $h$, are
    extremely unlikely to land exactly on top of another node in the graph.  To
    combat this, we had to start working on methods for interpolating the value
    function estimate between nodes.</p>

    <todo>add figure illustrating the interpolation here</todo>

    <p>Interpolation can work well if you are trying to solve for the
    cost-to-go function over the entire state space, but it's less compatible
    with search methods which are trying to find just a single path through the
    space.  If I start in node 1, and land between node 2 and node 3, then
    which node do I continue to expand from?</p>

    <p>Fortunately, the incremental search algorithms from logical search
    already give us a way to think about this -- we can simply build a
    <em>search tree</em> as the search executes, instead of relying on a
    predefined mesh discretization.  This tree will contain nodes rooted in
    the continuous space at exactly the points where system can reach.</p>

    <p>Another potential problem with any fixed-mesh discretization of a
    continuous space, or even a fixed discretization of the action space, is
    that unless we have specific geometric / dynamic insights into our
    continuous system, it can be very difficult to provide a <i>complete</i>
    planning algorithm. A planning algorithm is complete if it always finds a
    path (from the start to the goal) when a path exists. Even if we can show
    that no path to the goal exists on the tree/graph, how can we be certain
    that there is no path for the continuous system?  Perhaps a solution would
    have emerged if we had discretized the system differently, or more
    finely?</p>

    <p>One approach to addressing this second challenge is to toss out the
    notion of a fixed discretizations, and replace them with random sampling
    (another approach would be to adaptively add resolution to the
    discretization as the algorithm runs).  Random sampling, e.g. of the action
    space, can yield algorithms that are <i>probabilistically complete</i>
    for the continuous space -- if a solution to the problem exists, then a
    probabilistically complete algorithm will find that solution with
    probability 1 as the number of samples goes to infinity.</p>

    <p>With these motivations in mind, we can build what is perhaps the
    simplest probabilistically complete algorithm for finding a path from a 
    starting state to some goal region within a continuous state and action
    space:</p>

    <example><h1>Planning with a Random Tree</h1>

      <p>Let us denote the data structure which contains the tree as ${\cal
      T}$.  The algorithm is very simple:
      <ul>
        <li>Initialize the tree with the start state: ${\cal T} \leftarrow \bx_0$.</li>
        <li>On each iteration:
          <ul>
            <li>Select a random node, $\bx_{rand}$, from the tree, ${\cal T}$</li>
            <li>Select a random action, $\bu_{rand}$, from a distribution over feasible actions.</li>
            <li>Compute the dynamics: $\bx_{new} = f(\bx_{rand},\bu_{rand})$</li>
            <li>If $\bx_{new} \in {\cal G}$, then terminate. Solution found!</li>
            <li>Otherwise add the new node to the tree, ${\cal T} \leftarrow \bx_{new}$.</li>
          </ul>
        </li>
      </ul>
      It can be shown that this algorithm is, in fact, probabilistically
      complete.  However, without strong heuristics to guide the selection of
      the nodes scheduled for expansion, it can be extremely inefficient.  For
      a simple example, consider the system $\bx[n] = \bu[n]$ with $\bx \in
      \Re^2$ and $\bu_i \in [-1,1]$. We'll start at the origin and put the goal
      region as $\forall i, 15 \le x_i \le 20$.</p>

      <!--
      Try it yourself:
     
      <pre><code class="matlab" testfile="testRandomTree">
T = struct('parent',zeros(1,1000),'node',zeros(2,1000));  % pre-allocate memory for the "tree"
for i=2:size(T.parent,2)
T.parent(i) = randi(i-1);
x_rand = T.node(:,T.parent(i));
u_rand = 2*rand(2,1)-1;
x_new = x_rand+u_rand;
if (15<=x_new(1) && x_new(1)<=20 && 15<=x_new(2) && x_new(2)<=20)
  disp('Success!'); break;
end
T.node(:,i) = x_new;
end
clf;
line([T.node(1,T.parent(2:end));T.node(1,2:end)],[T.node(2,T.parent(2:end));T.node(2,2:end)],'Color','k');
patch([15,15,20,20],[15,20,20,15],'g')
axis([-10,25,-10,25]);
</code></pre>
-->

      <p>Although this "straw-man" algorithm is probabilistically complete, it
      is certainly not efficient. After expanding 1000 nodes, the tree is
      basically a mess of node points all right on top of each other:
      <figure>
        <img width="70%" src="figures/random_tree.svg"/>
      </figure>
      <p>We're nowhere close to the goal yet, and this is a particularly easy
      problem instance.
      </p>

    </example>

    <p>The idea of generating a tree of feasible points has clear advantages,
    but it seems that we have lost the ability to mark a region of space as
    having been sufficiently explored. It seems that, to make randomized
    algorithms effective, we are going to at the very least need some form of
    heuristic for encouraging the nodes to spread out and explore the space.
    </p>

    <subsection><h1>Rapidly-Exploring Random Trees (RRTs)</h1>

      <figure>
        <img width="70%" src="figures/rrt.svg"/>
      </figure>

      <figure>
        <img width="90%" src="figures/rrt_basic.svg"/>
        <figcaption>(<a href="https://manipulation.csail.mit.edu/data/rrt_basic.html">Click here to watch the animation</a>)</figcaption>
      </figure>

      <figure>
        <img width="90%" src="figures/rrt_voronoi.svg"/>
        <figcaption>(<a href="figures/rrt_voronoi.swf">Click here to watch the animation</a>)</figcaption>
      </figure>


    </subsection>

    <subsection><h1>RRTs for robots with dynamics</h1>

    </subsection>

    <subsection><h1>Variations and extensions</h1>

      <p>Multi-query planning with PRMs, ...
      </p>

      <p>RRT*, RRT-sharp, RRTx, ...  </p>

      <p>Kinodynamic-RRT*, LQR-RRT(*)</p>

      <p>Complexity bounds and dispersion limits</p>

    </subsection>

    <subsection><h1>Discussion</h1>

      <p>Not sure yet whether randomness is fundamental here, or whether is a temporary "crutch"
        until we understand geometric and dynamic planning better.</p>

    </subsection>
  </section>

  <section><h1>Decomposition methods</h1>

    <p>Cell decomposition...</p>

    <p>Approximate decompositions for complex environments (e.g. IRIS)</p>

  </section>

  <section><h1>Planning as Combinatorial + Continuous Optimization</h1>

    <p>Shortest path on a graph as a linear program.</p>

    <p>Mixed-integer planning.</p>

    <subsection>
      <h1>Motion Planning on Graphs of Convex Sets (GCS)</h1>
    </subsection>
  </section>

  <section><h1>Exercises</h1>

    <exercise><h1>RRT Planning</h1>

      <p>In this  <script>document.write(notebook_link('planning', 'rrt_planning', link_text = 'notebook'))</script> 
         we will write code for the Rapidly-Exploring Random 
         Tree (RRT). Building on this implementation we will also implement RRT*, 
         a variant of RRT that converges towards an optimal solution.</p>

      <ol type="a">

        <li>Implement RRT</li>

        <li>Implement RRT*</li>

      </ol>

    </exercise>


  </section>


</chapter>
<!-- EVERYTHING BELOW THIS LINE IS OVERWRITTEN BY THE INSTALL SCRIPT -->

<div id="references"><section><h1>References</h1>
<ol>

<li id=LaValle06>
<span class="author">Steven M. LaValle</span>, 
<span class="title">"Planning Algorithms"</span>, Cambridge University Press
, <span class="year">2006</span>.

</li><br>
<li id=Bubeck23>
<span class="author">S{\'e}bastien Bubeck and Varun Chandrasekaran and Ronen Eldan and Johannes Gehrke and Eric Horvitz and Ece Kamar and Peter Lee and Yin Tat Lee and Yuanzhi Li and Scott Lundberg and others</span>, 
<span class="title">"Sparks of artificial general intelligence: Early experiments with gpt-4"</span>, 
<span class="publisher">arXiv preprint arXiv:2303.12712</span>, <span class="year">2023</span>.

</li><br>
<li id=Helmert06>
<span class="author">Malte Helmert</span>, 
<span class="title">"The fast downward planning system"</span>, 
<span class="publisher">Journal of Artificial Intelligence Research</span>, vol. 26, pp. 191--246, <span class="year">2006</span>.

</li><br>
</ol>
</section><p/>
</div>

<table style="width:100%;"><tr style="width:100%">
  <td style="width:33%;text-align:left;"><a class="previous_chapter" href=policy_search.html>Previous Chapter</a></td>
  <td style="width:33%;text-align:center;"><a href=index.html>Table of contents</a></td>
  <td style="width:33%;text-align:right;"><a class="next_chapter" href=feedback_motion_planning.html>Next Chapter</a></td>
</tr></table>

<div id="footer">
  <hr>
  <table style="width:100%;">
    <tr><td><a href="https://accessibility.mit.edu/">Accessibility</a></td><td style="text-align:right">&copy; Russ
      Tedrake, 2024</td></tr>
  </table>
</div>


</body>
</html>
