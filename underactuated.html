<!DOCTYPE html>

<html xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
 xsi:schemaLocation="http://www.w3.org/1999/xhtml HTMLBook/schema/htmlbook.xsd"
 xmlns="http://www.w3.org/1999/xhtml">

  <head>
    <title>Underactuated Robotics</title>
    <meta name="Underactuated Robotics" content="text/html; charset=utf-8;" />

    <script type="text/javascript">
      function getParameterByName(name) {
        name = name.replace(/[\[]/, "\\[").replace(/[\]]/, "\\]");
        var regex = new RegExp("[\\?&]" + name + "=([^&#]*)"),
          results = regex.exec(location.search);
        return results == null ? "" : decodeURIComponent(results[1].replace(/\+/g, " "));
      }

      function revealChapters() {
        // generate table of contents
        {

          var i;

          var toc = "<h1>Table of Contents</h1>\n<ul>\n<li><a href=#preface>Preface</a></li>\n";
          var chapters = document.getElementsByClassName("chapter");
          var app = document.getElementsByTagName("appendix");
          var start_appendix = chapters.length - app[0].getElementsByClassName("chapter").length;
          for (i = 0; i < chapters.length; i++) {
            var titles=chapters[i].getElementsByTagName("h1");
            if (!chapters[i].id) {
              chapters[i].id = "chap"+(i+1);
            }
//            toc = toc + "<li><a href=#"+chapters[i].id+">";
            toc = toc + "<li><a href=underactuated.html?chapter="+(i+1)+">";
            if (i>= start_appendix) {
              toc = toc + "Appendix " + (i+1-start_appendix);
            } else {
              toc = toc + "Chapter " + (i+1);
            }
            toc = toc + ": " + titles[0].innerHTML + "</a></li>\n";
          }
          toc = toc + "</ul>\n";
          document.getElementById("table_of_contents").innerHTML = toc;
        }

        MathJax.Hub.Queue(["Typeset",MathJax.Hub,"mathjax_setup"]);

        var chapter = getParameterByName("chapter");
        if (chapter) {
          chapter = Number(chapter);
          document.getElementById("preface").style.display = "none";
          document.getElementById("table_of_contents").style.display = "none";
          var chapters = document.getElementsByClassName("chapter");
          var i;
          //document.getElementById("debug_output").innerHTML="displaying only chapter " + chapter + " of " + chapters.length;
          for (i = 0; i < chapters.length; i++) {
            if ((i+1) != chapter) {
              chapters[i].style.display = "none";
            } else {
              chapters[i].style.display = "inline";
//              document.getElementById("debug_output").innerHTML="got here";
              chapters[i].style.counterReset = "chapter " + i;
              MathJax.Hub.Queue(["Typeset",MathJax.Hub,chapters[i]]);
            }
          }
        }

        var appendix = getParameterByName("appendix");
        if (appendix) {
          var app = document.getElementsByTagName("appendix");
          document.getElementById("preface").style.display = "none";
          document.getElementById("table_of_contents").style.display = "none";
          var chapters = document.getElementsByClassName("chapter");
          var i;
          for (i = 0; i < chapters.length; i++) {
            chapters[i].style.display = "none";
          }
          var chapters = app[0].getElementsByClassName("chapter");
          i = Number(appendix)-1;
          chapters[i].style.display = "inline";
          chapters[i].style.counterReset = "chapter " + i;
          MathJax.Hub.Queue(["Typeset",MathJax.Hub,chapters[i]]);
        }

      }
      </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        skipStartupTypeset:true,
        extensions: ["tex2jax.js"],
        jax: ["input/TeX", "output/HTML-CSS"],
        TeX: { equationNumbers: {autoNumber: "AMS"}, noErrors: { disabled: true } },
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"] ],
          displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
          processEscapes: true
        },
        "HTML-CSS": { availableFonts: ["TeX"] }
      });
    </script>
    <script type="text/javascript"
      src="MathJax/MathJax.js?config=TeX-AMS_HTML">
    </script>

    <link rel="stylesheet" href="highlight/styles/default.css">
    <script src="highlight/highlight.pack.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>

    <script type="text/javascript">
      function myMacros(){
        var elibTags=document.getElementsByTagName('elib');
        for(i=0;i<elibTags.length;i++)
        {
          var c = elibTags[i].innerHTML;
          elibTags[i].innerHTML='[<a href="http://groups.csail.mit.edu/locomotion/elib.cgi?b='+c+'">'+(i+1)+'</a>]';
        }

        var drakeTags=document.getElementsByTagName('drake');
        for(i=0;i<drakeTags.length;i++)
        {
          drakeTags[i].innerHTML='<a style="font-variant:small-caps; text-decoration:none;" href="http://drake.mit.edu">Drake</a>';
        }
      }
    </script>
    <link rel="stylesheet" type="text/css" href="underactuated.css">
  </head>


<body data-type="book" class="book" id="htmlbook" onload="myMacros(); revealChapters();">
<section data-type="titlepage" class="titlepage">
  <header>
    <h1><a href="underactuated.html" style="text-decoration:none;">Underactuated Robotics</a></h1>
    <p data-type="subtitle">Algorithms for Walking, Running, Swimming, Flying, and Manipulation</p>
  	<p data-type="author"><a href="http://people.csail.mit.edu/russt/">Russ Tedrake</a></p>
    <p data-type="copyright">&copy; Russ Tedrake, 2014<br><a href="tocite.html">How to cite these notes</a></p>
  </header>
</section>
<div style="display:none" id="mathjax_setup"> <!-- definitions for mathjax -->
  \[
  \newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}}
  \newcommand{\bq}{{\bf q}}
  \newcommand{\bv}{{\bf v}}
  \newcommand{\bx}{{\bf x}}
  \newcommand{\by}{{\bf y}}
  \newcommand{\bu}{{\bf u}}
  \newcommand{\bw}{{\bf w}}
  \newcommand{\bz}{{\bf z}}
  \newcommand{\bK}{{\bf K}}
  \newcommand{\balpha}{{\bf \alpha}}
  \newcommand{\bbeta}{{\bf \beta}}
  \newcommand{\avg}[1]{E\left[ #1 \right]}
  \newcommand{\subjto}{\textrm{subject to}}
  \newcommand{\minimize}{\operatorname{\textrm{minimize}}}
  \newcommand{\maximize}{\operatorname{\textrm{maximize}}}
  \newcommand{\find}{\operatorname{\textrm{find}}}
  \newcommand{\argmax}{\operatorname{\textrm{argmax}}}
  \newcommand{\argmin}{\operatorname{\textrm{argmin}}}
  \newcommand{\sgn}{\operatorname{\textrm{sgn}}}
  \newcommand{\trace}{\operatorname{\textrm{tr}}}
  \newcommand{\sos}{\text{is SOS}}
  \]
</div>

<div id="debug_output"></div>

<p><b>Note:</b> These are working notes that will be updated throughout the Fall 2014 semester.</p>

<section id="table_of_contents"></section>


<section id="preface">
<h1>Preface</h1>

<p>This book is about building robots that move with speed, efficiency,
and grace.  I believe that this can only be achieve through
a tight coupling between mechanical design, passive dynamics, and
nonlinear control synthesis.  Therefore, these notes contain selected
material from dynamical systems theory, as well as linear and
nonlinear control.<p>

<p>These notes also reflect a deep belief in computational algorithms
playing an essential role in finding and optimizing solutions to
complex dynamics and control problems.  Algorithms play an
increasingly central role in modern control theory; these days even rigorous
mathematicians consider finding convexity in a problem (therefore
making it amenable to an efficient computational solution) almost
tantamount to an analytical result.  Therefore, the notes necessarily also cover
selected material from optimization theory, motion planning, and
machine learning.</p>

<p>
Although the material in the book comes from many sources, the
presentation is targeted very specifically at a handful of robotics
problems.  Concepts are introduced only when and if they can help
progress the capabilities we are trying to develop.  I
hope that the result is a broad but reasonably self-contained and
readable manuscript that will be of use to any enthusiastic roboticist.</p>

<section><h1>Organization</h1>

<p>
The material in these notes is organized into a few main parts.
"Model Systems" introduces a series of increasingly complex
dynamical systems and overviews some of the relevant results from the
literature for each system. "Nonlinear Planning and Control"
introduces quite general computational algorithms for reasoning about those
dynamical systems, with optimization theory playing a central role.
Many of these algorithms treat the dynamical system as known and
deterministic until the last chapters in this part which introduce
stochasticity and robustness.  "Estimation and Learning" follows
this up with techniques from statistics and machine learning which
capitalize on this viewpoint to introduce additional algorithms which
can operate with less assumptions on knowing the model or having
perfect sensors.  The book closes with chapters on "Applications and
Extensions" and an "Appendix".</p>

<p>The order of the chapters was chosen to make the book valuable as a
reference.  When teaching the course, however, I take a spiral trajectory
through the material, introducing robot dynamics and control problems
one at a time, and introducing only the techniques that are required
to solve that particular problem.</p>

<todo>insert figure showing progression of problems here.  pendulum ->
cp/acro -> walking ...  with chapter numbers associated.</todo>

</section>

<section><h1>Software</h1>

<p>
All of the examples and algorithms in this book, plus many more, are
now available as a part of our open-source software project:
<drake></drake>.  I will give pointers to these examples
throughout the text.  </p>


</section>

</section> <!-- end preface -->

<section data-type="chapter" class="chapter" id="ch:intro">
<h1>Fully-actuated vs Underactuated Systems</h1>

<p>Robots today move far too conservatively, and accomplish only a fraction of
the tasks and achieve a fraction of the performance that they are mechanically
capable of.  In many cases, we are still fundamentally limited by control
technology which matured on rigid robotic arms in structured factory
environments.  The study of underactuated robotics focuses on building control
systems which use the natural dynamics of the machines in an attempt to achieve
extraordinary performance in terms of speed, efficiency, or robustness.</p>

<section data-type="sect1">
<h1>Motivation</h1>
<p>Let's start with some examples, and some videos.</p>

<section data-type="sect2">
<h1>Honda's ASIMO vs. Passive Dynamic Walkers</h1>

<p> The world of robotics changed when, in late 1996, Honda Motor Co. announced
that they had been working for nearly 15 years (behind closed doors) on walking
robot technology.  Their designs have continued to evolve, resulting in a
humanoid robot they call ASIMO (Advanced Step in Innovative MObility).  Honda's
ASIMO is widely considered to be state of the art in walking robots, although
there are now many robots with designs and performance very similar to ASIMO's.
We will dedicate effort to understanding a few of the details of ASIMO when we
discuss algorithms for walking... for now I just want you to become familiar
with the look and feel of ASIMO's movements [watch the asimo video below now].
</p>

<figure>
  <video width="80%" controls>
    <source src="http://world.honda.com/ASIMO/technology/2011/intelligence/video02/movie640w.mp4" type="video/mp4"/>
    <source src="figures/walking_while_avoiding_people.ogg" type="video/ogg"/>
  </video><br/>
<!--
<object name="kaltura_player_1409562873" id="kaltura_player_1409562873" type="application/x-shockwave-flash" allowScriptAccess="always" allowNetworking="all" allowFullScreen="true" height="300" width="100%" data="http://www.kaltura.com/index.php/kwidget/wid/1_bybwc53n/uiconf_id/8700151?autoPlay=false"><param name="allowScriptAccess" value="always" /><param name="allowNetworking" value="all" /><param name="allowFullScreen" value="true" /><param name="bgcolor" value="#000000" /><param name="movie" value="http://www.kaltura.com/index.php/kwidget/wid/1_bybwc53n/uiconf_id/8700151"/><param name="flashVars" value=""/><a href="http://corp.kaltura.com">video platform</a><a href="http://corp.kaltura.com/video_platform/video_management">video management</a><a href="http://corp.kaltura.com/solutions/video_solution">video solutions</a><a href="http://corp.kaltura.com/video_platform/video_publishing">video player</a></object>
-->
<figcaption>Honda's ASIMO (from <a href="http://world.honda.com/ASIMO/video/">http://world.honda.com/ASIMO/video/</a>)</figcaption>

</figure>

<p> I hope that your first reaction is to be incredibly impressed with the
quality and versatility of ASIMO's movements.  Now take a second look. Although
the motions are very smooth, there is something a little unnatural about ASIMO's
gait.  It feels a little like an astronaut encumbered by a heavy space suit.  In
fact this is a reasonable analogy... ASIMO is walking a bit like somebody that
is unfamiliar with his/her dynamics.  It's control system is using high-gain
feedback, and therefore considerable joint torque, to cancel out the natural
dynamics of the machine and strictly follow a desired trajectory. This control
approach comes with a stiff penalty.  ASIMO uses roughly 20 times the energy
(scaled) that a human uses to walk on the flat (measured by cost of
transport)<elib>Collins05</elib>.  Also, control stabilization in this approach
only works in a relatively small portion of the state space (when the stance
foot is flat on the ground), so ASIMO can't move nearly as quickly as a human,
and cannot walk on unmodelled or uneven terrain. </p>

<figure>
<p>
  <video width="80%" controls title="If your browser cannot play this video, then download it using the link below.">
    <source src="http://ruina.tam.cornell.edu/research/topics/locomotion_and_robotics/3d_passive_dynamic/from_angle.mpg" type="video/mp4"/>
    <source src="figures/passive_angle.ogg" type="video/ogg"/>
  </video><br/>
  <a href="http://ruina.tam.cornell.edu/research/topics/locomotion_and_robotics/3d_passive_dynamic/from_angle.mpg">Download the video</a>
</p>
<p>
  <video width="80%" controls title="If your browser cannot play this video, then download it using the link below.">
    <source src="http://ruina.tam.cornell.edu/research/topics/locomotion_and_robotics/3d_passive_dynamic/from_behind.mpg" type="video/mp4">
    <source src="figures/passive_behind.ogg" type="video/ogg"/>
  </video><br/>
  <a href="http://ruina.tam.cornell.edu/research/topics/locomotion_and_robotics/3d_passive_dynamic/from_behind.mpg">Download the video</a>
</p>
<figcaption>A 3D passive dynamic walker by Steve Collins and Andy Ruina<elib>Collins01</elib></figcaption>
</figure>

<p> For contrast, let's now consider a very different type of walking robot,
called a passive dynamic walker (PDW).  This "robot" has no motors, no
controllers, no computer, but is still capable of walking stably down a small
ramp, powered only by gravity [watch videos above now].  Most people will agree
that the passive gait of this machine is more natural than ASIMO's; it is
certainly more efficient.  Passive walking machines have a long history - there
are patents for passively walking toys dating back to the mid 1800's.  We will
discuss, in detail, what people know about the dynamics of these machines and
what has been accomplished experimentally.  This most impressive passive dynamic
walker to date was built by Steve Collins in Andy Ruina's lab at
Cornell<elib>Collins01</elib>. </p>

<p> Passive walkers demonstrate that the high-gain, dynamics-cancelling feedback
approach taken on ASIMO is not a necessary one.  In fact, the dynamics of
walking is beautiful, and should be exploited - not cancelled out. </p>
</section>

<section data-type="sect2">
<h1>Birds vs. modern aircraft</h1>

<p> The story is surprisingly similar in a very different type of machine.
Modern airplanes are extremely effective for steady-level flight in still air.
Propellers produce thrust very efficiently, and today's cambered airfoils are
highly optimized for speed and/or efficiency. It would be easy to convince
yourself that we have nothing left to learn from birds.  But, like ASIMO, these
machines are mostly confined to a very conservative, low angle-of-attack flight
regime where the aerodynamics on the wing are well understood.  Birds routinely
execute maneuvers outside of this flight envelope (for instance, when they are
landing on a perch), and are considerably more effective than our best aircraft
at exploiting energy (eg, wind) in the air.  </p>

<p> As a consequence, birds are extremely efficient flying machines; some are
capable of migrating thousands of kilometers with incredibly small fuel
supplies.  The wandering albatross can fly for hours, or even days, without
flapping its wings - these birds exploit the shear layer formed by the wind over
the ocean surface in a technique called dynamic soaring.  Remarkably, the
metabolic cost of flying for these birds is indistinguishable from the baseline
metabolic cost<elib>Arnould96</elib>, suggesting that they can travel incredible
distances (upwind or downwind) powered almost completely by gradients in the
wind.  Other birds achieve efficiency through similarly rich interactions with
the air - including formation flying, thermal soaring, and ridge soaring.  Small
birds and large insects, such as butterflies and locusts, use `gust soaring' to
migrate hundreds or even thousands of kilometers carried primarily by the wind.
</p>

<p> Birds are also incredibly maneuverable.  The roll rate of a highly acrobatic
aircraft (e.g, the A-4 Skyhawk) is approximately 720 deg/sec<elib>Shyy08</elib>;
a barn swallow has a roll rate in excess of 5000   deg/sec<elib>Shyy08</elib>.
Bats can be flying at full-speed in one direction, and completely reverse
direction while maintaining forward speed, all in just over 2 wing-beats and in
a distance less than half the wingspan<elib>Tian06</elib>.  Although
quantitative flow visualization data from maneuvering flight is scarce, a
dominant theory is that the ability of these animals to produce sudden, large
forces for maneuverability can be attributed to unsteady aerodynamics, e.g., the
animal creates a large suction vortex to rapidly change
direction<elib>Triantafyllou95</elib>.  These astonishing capabilities are
called upon routinely in maneuvers like flared perching, prey-catching, and high
speed flying through forests and caves.  Even at high speeds and high turn
rates, these animals are capable of incredible agility - bats sometimes capture
prey on their wings, Peregrine falcons can pull 25 G's out of a 240 mph dive to
catch a sparrow in mid-flight<elib>Tucker98</elib>, and even the small birds
outside our building can be seen diving through a chain-link fence to grab a
bite of food. </p>

<p> Although many impressive statistics about avian flight have been recorded,
our understanding is partially limited by experimental accessibility - it's is
quite difficult to carefully measure birds (and the surrounding airflow) during
their most impressive maneuvers without disturbing them.  The dynamics of a
swimming fish are closely related, and can be more convenient to study.
Dolphins have been known to swim gracefully through the waves alongside ships
moving at 20 knots<elib>Triantafyllou95</elib>.  Smaller fish, such as the
bluegill sunfish, are known to possess an escape response in which they propel
themselves to full speed from rest in less than a body length; flow
visualizations indeed confirm that this is accomplished by creating a large
suction vortex along the side of the body<elib>Tytell08</elib> - similar to how
bats change direction in less than a body length. There are even observations of
a dead fish swimming upstream by pulling energy out of the wake of a cylinder;
this passive propulsion is presumably part of the technique used by rainbow
trout to swim upstream at mating season<elib>Beal06</elib>. </p>

</section>

<section data-type="sect2">
<h1>The common theme</h1>

<p> Classical control techniques for robotics are based on the idea that
feedback can be used to override the dynamics of our machines.  These examples
suggest that to achieve outstanding dynamic performance (efficiency, agility,
and robustness) from our robots, we need to understand how to design control
system which take advantage of the dynamics, not cancel them out.  That is the
topic of this course. </p>

<p> Surprisingly, there are relatively few formal control ideas that consider
"exploiting" the dynamics.  In order to convince a control theorist to consider
the dynamics (efficiency arguments are not enough), you have to do something
drastic, like taking away his control authority - remove a motor, or enforce a
torque-limit.  These issues have created a formal class of systems, the
underactuated systems, for which people have begun to more carefully consider
the dynamics of their machines in the context of control. </p>

</section>
</section>


<section data-type="sect1">
<h1>Definitions</h1>

<p> According to Newton, the dynamics of mechanical systems are second order ($F =
ma$).  Their state is given by a vector of positions, $\bq$, and a vector of
velocities, $\dot{\bq}$, and (possibly) time. The general form for a
second-order controllable dynamical system is: $$\ddot{\bq} = {\bf
f}(\bq,\dot{\bq},\bu,t),$$ where $\bu$ is the control vector.  As we will see,
the dynamics for many of the robots that we care about turn out to be affine in
commanded torque, so let's consider a slightly constrained form:
\begin{equation}\ddot{\bq} = {\bf f}_1(\bq,\dot{\bq},t) + {\bf
f}_2(\bq,\dot{\bq},t)\bu \label{eq:f1_plus_f2}.\end{equation} </p>

<div data-type="definition"><h1>Fully-Actuated</h1> A control system described
by equation \ref{eq:f1_plus_f2} is fully-actuated in configuration
$(\bq,\dot{\bq},t)$ if it is able to command an instantaneous acceleration in an
arbitrary direction in $\bq$: \begin{equation} \textrm{rank}\left[{\bf f}_2
(\bq,\dot{\bq},t)\right] = \dim\left[\bq\right].\end{equation} </div>

<div data-type="definition"><h1>Underactuated</h1> A control system described by
equation \ref{eq:f1_plus_f2} is underactuated in configuration
$(\bq,\dot{\bq},t)$ if it is not able to command an instantaneous acceleration
in an arbitrary direction in $\bq$: \begin{equation} \textrm{rank}\left[{\bf
f}_2(\bq,\dot{\bq},t)\right] < \dim\left[\bq\right].
\label{eq:underactuated_def}\end{equation} </div>

<p>Notice that whether or not a control system is underactuated may depend on
the state of the system, although for most systems (including all of the systems
in this book) underactuation is a global property of the system.</p>

<p> In words, underactuated control systems are those in which the control input
cannot accelerate the state of the robot in arbitrary directions.  As a
consequence, unlike fully-actuated systems, underactuated system cannot be
commanded to follow arbitrary trajectories.  </p>

<div data-type="example"><h1>Robot Manipulators</h1>

<figure>
<img style="width:250px;" src="figures/simple_double_pend.svg"/>
<todo>make this image spring to life with a matlab movie<</todo>
<figcaption>Simple double pendulum</figcaption>
</figure>

<p> Consider the simple robot manipulator illustrated above.  As described in
Appendix A, the equations of motion for this system are quite simple to derive,
and take the form of the standard "manipulator equations": $${\bf
H}(\bq)\ddot\bq + {\bf C}(\bq,\dot\bq)\dot\bq + {\bf G}(\bq) = {\bf
B}(\bq)\bu.$$ It is well known that the inertial matrix, ${\bf H}(\bq)$ is
(always) uniformly symmetric and positive definite, and is therefore invertible.
Putting the system into the form of equation \ref{eq:f1_plus_f2} yields:
\begin{align*}\ddot{\bq} =& -{\bf H}^{-1}(\bq)\left[ {\bf C}(\bq,\dot\bq)
\dot\bq + {\bf G}(\bq) \right]\\ &+ {\bf H}^{-1}(\bq) {\bf B}(\bq)
\bu.\end{align*} Because ${\bf H}^{-1}(\bq)$ is always full rank, we find that a
system described by the manipulator equations is fully-actuated if and only if
${\bf B}(\bq)$ is full row rank.   For this particular example, $\bq =
[\theta_1,\theta_2]^T$ and $\bu = [\tau_1,\tau_2]^T$, and ${\bf B}(\bq) = {\bf
I}_{2 \times 2}$.  The system is fully actuated.</p>

<todo>make drake elements expandable/collapseable.  nice example here: http://quhno.internetstrahlen.de/myopera/csstests/collapsible-paragraph.html</todo>
<div data-type="drake"><h1>MATLAB Example</h1>

<p> I personally learn best when I can experiment and get some physical
intuition.  The <a href="http://drake.mit.edu">companion software for the
course</a> should make it easy for you to see this   system in action.  To try
it, make sure you've installed <drake></drake>, then open MATLAB and run
<code>addpath_drake</code> (possibly from your <code>startup.m</code>) in the
<code>drake</code> directory, then try the following lines (one at a time) from
the MATLAB command line:

<pre><code class="matlab">
% a platform independent way to cd into the example directory
cd(fullfile(getDrakePath,'examples','SimpleDoublePendulum'));

% first construct the "robot" or "plant" object
plant = DoublePendPlant;  % check out the code in DoublePendPlant.m

% a visualizer will draw the robot
visualizer = DoublePendVisualizer(plant);
visualizer.inspector();  % (optional) simple gui to examine the joints

% simulate the robot from time=0 to time=5 seconds from random
% initial conditions taken from a Gaussian distribution (using randn)
trajectory = simulate(plant, [0 5], randn(4,1));

% play back the simulation using the cpu clock to get the timing right
visualizer.playback(trajectory);

% if you want to access the manipulator equations programmatically,
% you can use, e.g.:
[H,C_times_v,G,B] = plant.manipulatorEquations()
% which outputs using the abbreviations v for qdot, s1 for sin(q1), etc
</code></pre>
</p>
</div>

<div data-type="drake"><h1>MATLAB Example (an even simpler version)</h1>

<p>Note that you don't have to write the dynamics yourself.  The software
supports a high-level description format for describing robots.  We could have
alternatively done: </p>

<pre><code class="matlab">
cd(fullfile(getDrakePath,'examples','SimpleDoublePendulum'));

% construct the robot from a URDF high-level description file
plant = RigidBodyManipulator('SimpleDoublePendulum.urdf');
visualizer = plant.constructVisualizer();

% all of the other commands will work, e.g.:
trajectory = simulate(plant, [0 5], randn(4,1));
visualizer.playback(trajectory);
[H,C_times_v,G,B] = plant.manipulatorEquations()
</code></pre>

<p>Make sure you take a peek at the file <code>SimpleDoublePendulum.urdf</code>.
Documentation for the input format is available on the <a
href="http://drake.mit.edu">drake wiki</a>; it supports even quite complex
robots.</p>
</div>

<p> While the basic double pendulum is fully actuated, imagine the somewhat
bizarre case that we have a motor to provide torque at the elbow, but no motor
at the shoulder.  In this case, we have $\bu = \tau_2$, and ${\bf B}(\bq) =
[0,1]^T$.  This system is clearly underactuated.  While it may sound like a
contrived example, it turns out that it is almost exactly the dynamics we will
use to study as our simplest model of walking later in the class. </p>

</div>

<p>The matrix ${\bf f}_2$ is equation \ref{eq:f1_plus_f2} always has dim$[\bq]$
rows, and dim$[\bu]$ columns. Therefore, as in the example, one of the most
common cases for underactuation, which trivially implies that ${\bf f}_2$ is not
full row rank, is dim$[\bu] < $ dim$[\bq]$.  This is the case when a robot has
joints with no motors. But this is not the only case.  The human body, for
instance, has an incredible number of actuators (muscles), and in many cases has
multiple muscles per joint; despite having more actuators that position
variables, when I jump through the air, there is no combination of muscle inputs
that can change the ballistic trajectory of my center of mass (barring
aerodynamic effects).  My control system is underactuated.</p>

<p>For completeness, let's generalize the definition of underactuation to
systems beyond the second-order control affine systems.

<div data-type="definition"> <h1>Underactuated Control Differential
Equations</h1> An $n$th-order control differential equation control described by
the equations \begin{equation} \frac{d^n{\bf q}}{dt^n} = f({\bf q}, ...,
\frac{d^{n-1} {\bf q}}{dt^{n-1}}, t, {\bf u}) \end{equation} is fully actuated
in state ${\bf x} = ({\bf q}, ..., \frac{d^{n-1} {\bf q}}{dt^{n-1}})$ and time
$t$ if the resulting map $f$ is surjective: for every $\frac{d^n{\bf q}}{dt^n} $
there exists a ${\bf u}$ which produces the desired response.  Otherwise it is
underactuated. </div>

It is easy to see that equation \ref{eq:underactuated_def} is a sufficient
condition for underactuation.  This definition can also be extended to
discrete-time systems and/or differential inclusions. </p>

<p>A quick note about notation.  When describing the dynamics of rigid-body
systems in this class, I will use $\bq$ for positions, $\dot{\bq}$ for
velocities, and use $\bx$ for the full state ($\bx = [\bq,\dot{\bq}]^T$).  There
is an important limitation to this convention, described in the Appendix<!--
ch:robot_dynamics -->, but it will keep the notes more clean. Unless otherwise
noted, vectors are always treated as column vectors. Vectors and matrices are
bold (scalars are not). </p>

</section> <section data-type="sect1"> <h1>Feedback Linearization</h1>

<p> Fully-actuated systems are dramatically easier to control than underactuated
systems.  The key observation is that, for fully-actuated systems with known
dynamics (e.g., ${\bf f}_1$ and ${\bf f}_2$ are known), it is possible to use
feedback to effectively change a nonlinear control problem into a linear control
problem.  The field of linear control is incredibly advanced, and there are many
well-known solutions for controlling linear systems. </p>

<p>The trick is called feedback linearization.  When ${\bf f}_2$ is full row
rank, it is invertible.  Consider the nonlinear feedback law: $$\bu = {\bf
\pi}(\bq,\dot\bq,t) = {\bf f}_2^{-1}(\bq,\dot\bq,t) \left[ \bu' - {\bf
f}_1(\bq,\dot\bq,t) \right],$$ where $\bu'$ is some additional control input.
Applying this feedback controller to equation~\ref{eq:f1_plus_f2} results in the
linear, decoupled, second-order system: $$\ddot{\bq} = \bu'.$$ In other words,
if ${\bf f}_1$ and ${\bf f}_2$ are known and ${\bf f}_2$ is invertible, then we
say that the system is "feedback equivalent" to $\ddot{\bq} = \bu'$.  There are
a number of strong results which generalize this idea to the case where ${\bf
f}_1$ and ${\bf f}_2$ are estimated, rather than known (e.g,
<elib>Slotine90</elib>).</p>

<div data-type="example"><h1>Feedback-Cancellation Double Pendulum</h1>

<p> Let's say that we would like our simple double pendulum to act like a simple
single pendulum (with damping), whose dynamics are given by: \begin{align*}
\ddot \theta_1 &= -\frac{g}{l}\cos\theta_1 -b\dot\theta_1 \\ \ddot\theta_2 &= 0.
\end{align*} This is easily achieved using <sidenote>Note that our chosen
dynamics do not actually stabilize $\theta_2$ - this detail was left out for
clarity, but would be necessary for any real implementation.</sidenote> $$\bu =
{\bf B}^{-1}\left[ {\bf C}\dot{\bq} + {\bf G} + {\bf H}\begin{bmatrix}
-\frac{g}{l}c_1 - b\dot{q}_1 \\ 0 \end{bmatrix} \right].$$ </p>

<p> Since we are embedding a nonlinear dynamics (not a linear one), we refer to
this as "feedback cancellation", or "dynamic inversion".  This idea can, and
does, make control look easy - for the special case of a fully-actuated
deterministic system with known dynamics.  For example, it would have been just
as easy for me to invert gravity. Observe that the control derivations here
would not have been any more difficult if the robot had 100 joints. </p>

<div data-type="drake"><h1>MATLAB Example</h1> You can find scripts which
implement both of these examples in <drake></drake>:

<pre><code class="matlab">
cd(fullfile(getDrakePath,'examples','SimpleDoublePendulum'));
runSimplePend;    % feedback linearization to simulate a pendulum
runSimplePendInv; % and a gravity-inverted pendulum
</code></pre>
</div>
</div>

<p> The underactuated systems are not feedback linearizable.  Therefore, unlike
fully-actuated systems, the control designer has no choice but to reason about
the nonlinear dynamics of the plant in the control design.  This dramatically
complicates feedback controller design. </p>

</section>

<section data-type="sect1"><h1>Input and State Constraints</h1>

<p> Although the dynamic constraints due to missing actuators certainly embody
the spirit of this course, many of the systems we care about could be subject to
other dynamic constraints, as well.  For example, the actuators on our machines
may only be mechanically capable of producing some limited amount of torque, or
their may be a physical obstacle in the free space which we cannot permit our
robot to come into contact with. </p>

<div data-type="definition"><h1>Input and State Constraints</h1> A dynamical
system described by $\dot{\bx} = {\bf f}(\bx,\bu,t)$ may be subject to one or
more constraints described by ${\bf \phi}(\bx,\bu,t)\le0$. </div>

<p>In practice it can useful to separate out constraints which depend only on
the input, e.g. $\phi(\bu)\le0$, such as actuator limits, as they can often be
easier to handle than state constraints.  An obstacle in the environment might
manifest itself as one or more constraints that depend only on position, e.g.
$\phi(\bq)\le0$.</p>

<p>By our generalized definition of underactuation, we can see that input and
state constraints can also cause a system to be underactuated. <div
data-type="example"><h1>Input limits</h1> Consider the constrained second-order
linear system \[ \ddot{x} = u, \quad |u| \le 1. \] By our definition, this
system is underactuated.  For example, there is no $u$ which can produce the
acceleration $\ddot{x} = 2$. </div> Input and state constraints can complicate
control design in similar ways to having an insufficient number of actuators,
(i.e., the robot cannot follow arbitrary trajectories), and often require
similar tools to find a control solution.</p>

<section data-type="sect2"><h1>Nonholonomic constraints</h1>

<p> You might have heard of the term "nonholonomic system" (e.g.
<elib>Bloch03</elib>), and be thinking about how nonholonomy relates to
underactuation.  Briefly, a nonholonomic constraint is a constraint on the
derivatives of the system which cannot be integrated into a constraint on the
positions of the system.  An automobile or traditional wheeled robot provides a
canonical example:

<div data-type="example"><h1>Wheeled robot</h1> Consider a simple model of a
wheeled robot who's configuration is described by its Cartesian position $x,y$
and its orientation, $\theta$, so ${\bf q} = \begin{bmatrix} x, y, \theta
\end{bmatrix}^T$.  The system is subject to differential constraints that
prevent side-slip, e.g., \begin{gather*} \dot{x} = v \cos\theta \\ \dot{y} = v
\sin\theta \\ v = \sqrt{\dot{x}^2 + \dot{y}^2} \end{gather*} These constraints
cannot be integrated into constraint on position--the car can get to any
configuration $(x,y,\theta)$, it just can't move directly sideways--so they are
nonholonomic constraints. </div>

Contrast the wheel robot example with a robot on train tracks.  The train tracks
would represent a holonomic constraint: if the system was written down in the
$(x,y,\theta)$ coordinates then the differential track constraints could be
integrated resulting in a description of the system in a reduced coordinate
system--train tracks are holonomic constraints. </p>

<p> A nonholomic constraint like the no-side-slip constraint on the wheeled
vehicle certainly results in an underactuated system.  The converse is not
necessarily true--the double pendulum system which is missing an actuator is
underactauted but would not typically be called a nonholonmic system.  However,
while the definition of a nonholonomic constraint is clear, in my view the
definition used in the literature for a "nonholonomic system" is more
ambiguous&dagger;<sidenote>&dagger; Why, for instance, are the Lagrangian
equations of motion for the double pendulum--clearly non-integrable-- not
considered nonholonomic constraints?</sidenote>, and we'll generally avoid using
the term. </p>

</section>
</section>

<section data-type="sect1"><h1>Underactuated robotics</h1>

<p> The control of underactuated systems is an open and interesting problem in
controls - although there are a number of special cases where underactuated
systems have been controlled, there are relatively few general principles.  Now
here's the rub... most of the interesting problems in robotics are
underactuated: </p>

<ul>

<li> Legged robots are underactuated.  Consider a legged machine with $N$
internal joints and $N$ actuators.  If the robot is not bolted to the ground,
then the degrees of freedom of the system include both the internal joints and
the six degrees of freedom which define the position and orientation of the
robot in space.  Since $\bu \in \Re^N$ and $\bq \in \Re^{N+6}$, equation
\ref{eq:underactuated_def} is satisfied.</li>

<li> (Most) Swimming and flying robots are underactuated.  The story is the same
here as for legged machines.  Each control surface adds one actuator and one
DOF.  And this is already a simplification, as the true state of the system
should really include the (infinite-dimensional) state of the flow.</li>

<li> Robot manipulation is (often) underactuated.  Consider a fully-actuated
robotic arm.  When this arm is manipulating an object w/ degrees of freedom
(even a brick has six), it can become underactuated.  If force closure is
achieved, and maintained, then we can think of the system as fully-actuated,
because the degrees of freedom of the object are constrained to match the
degrees of freedom of the hand.  That is, of course, unless the manipulated
object has extra DOFs.</li>

</ul>

<p> Even fully-actuated and otherwise unconstrained control systems can be
improved using the lessons from underactuated systems, particularly if there is
a need to increase the efficiency of their motions or reduce the complexity of
their designs.</p>

</section>

<section data-type="sect1"><h1>Goals for the course</h1>

<p> This course is based on the observation that there are new computational
tools from optimization theory, control theory, motion planning, and even
machine learning which be used to design feedback control for underactuated
systems.  The goal of this class is to develop these tools in order to design
robots that are more dynamic and more agile than the current
state-of-the-art.</p>

<p> The target audience for the class includes both computer science and
mechanical/aero students pursuing research in robotics.  Although I assume a
comfort with linear algebra, ODEs, and MATLAB, the course notes will provide
most of the material and references required for the course. </p>

</section>

</section> <!-- end chapter -->

<section data-type="chapter" class="chapter" id="ch:pend">
<h1>The Simple Pendulum</h1>

<section data-type="sect1"><h1>Introduction</h1>

<p> Our goals for this chapter are modest: we'd like to understand the dynamics
of a pendulum.</p>

<p>Why a pendulum?  In part, because the dynamics of a majority of our
multi-link robotics manipulators are simply the dynamics of a large number of
coupled pendula.  Also, the dynamics of a single pendulum are rich enough to
introduce most of the concepts from nonlinear dynamics that we will use in this
text, but tractable enough for us to (mostly) understand in the next few pages.
</p>

<figure> <img width="30%" src="figures/simple_pend.svg"/> <figcaption>The simple
pendulum</figcaption> </figure>

<p> The Lagrangian derivation of the equations of motion (as described in the
appendix) of the simple pendulum yields: \begin{equation*} m l^2 \ddot\theta(t) +
mgl\sin{\theta(t)} = Q. \end{equation*} We'll consider the case where the
generalized force, $Q$, models a damping torque (from friction) plus a control
torque input, $u(t)$: $$Q = -b\dot\theta(t) + u(t).$$ </p>

</section>

<section data-type="sect1"><h1>Nonlinear Dynamics w/ a Constant Torque</h1>

<p> Let us first consider the dynamics of the pendulum if it is driven in a
particular simple way: a torque which does not vary with time: \begin{equation}
ml^2 \ddot\theta + b\dot\theta + mgl \sin\theta = u_0. \end{equation}
You can experiment with this system in <drake></drake> using

<pre><code class="matlab">
cd(fullfile(getDrakePath,'examples','Pendulum'));
open_system('constantTorqueDemo');
% hit play to start the simulation, and click on the boxes
% labeled as 'slider' to control the torque and damping
</code></pre>

These are relatively simple differential equations, so if I give you $\theta(0)$ and
$\dot\theta(0)$, then you should be able to integrate them to obtain
$\theta(t)$... right?  Although it is possible, integrating even the simplest
case ($b = u = 0$) involves elliptic integrals of the first kind; there is
relatively little intuition to be gained here. </p>

<p> This is in stark contrast to the case of linear systems, where much of
our understanding comes from being able to explictly integrate the equations.
For instance, for a simple linear system we have $$\dot{q} = a q \quad
\rightarrow \quad q(t) = q(0) e^{at},$$ and we can immediately understand that
the long-term behavior of the system is a (stable) decaying exponential if
$a<0$, an (unstable) growing expontential if $a>0$, and that the system does
nothing if $a=0$. Here we are with certainly one of the simplest nonlinear
systems we can imagine, and we can't even solve this system? </p>

<p> All is not lost.  If what we care about is the long-term behavior of the
system, then there are a number of techniques we can apply.  In this chapter, we
will start by investigating graphical solution methods. These methods are
described beautifully in a book by Steve Strogatz<elib>Strogatz94</elib>.

<section data-type="sect2"><h1>The Overdamped Pendulum</h1>

<p> Let's start by studying a special case -- intuitively when $b \gg ml^2$, but
to get the units right we actually need to include a fundamental time constant,
so $\frac{b}{ml^2} \sqrt\frac{g}{l} \gg 1$. This is the case of heavy damping,
for instance if the pendulum was moving in molasses.  In this case, the damping
term dominates the acceleration term, and we have: $$ml^2 \ddot\theta +
b\dot\theta \approx b\dot\theta = u_0 - mgl\sin\theta.$$ In other words, in the
case of heavy damping, the system looks approximately first-order. This is a
general property of heavily damped systems, such as fluids at very low Reynolds
number. </p>

<p> I'd like to ignore one detail for a moment: the fact that $\theta$ wraps
around on itself every $2\pi$.  To be clear, let's write the system without the
wrap-around as: \begin{equation}b\dot{x} = u_0 -
mgl\sin{x}.\label{eq:overdamped_pend_ct}\end{equation} Our goal is to understand
the long-term behavior of this system: to find $x(\infty)$ given $x(0)$.  Let's
start by plotting $\dot{x}$ vs $x$ for the case when $u_0=0$:

<figure> <img width="70%" src="figures/pend_sinx.svg"/> </figure> </p>

<p> The first thing to notice is that the system has a number of <em>fixed
points</em> or <em>steady states</em>, which occur whenever $\dot{x} = 0$.  In
this simple example, the zero-crossings are $x^* = \{..., -\pi, 0, \pi, 2\pi, ...\}$.
When the system is in one of these states, it will never leave that state.  If
the initial conditions are at a fixed point, we know that $x(\infty)$ will be at
the same fixed point.</p>

<p>Next let's investigate the behavior of the system in the local vicinity of
the fixed points.  Examing the fixed point at $x^* = \pi$, if the system starts
just to the right of the fixed point, then $\dot{x}$ is positive, so the system
will move away from the fixed point.  If it starts to the left, then $\dot{x}$
is negative, and the system will move away in the opposite direction.  We'll
call fixed-points which have this property <em>unstable</em>.  If we look at the
fixed point at $x^* = 0$, then the story is different: trajectories starting to
the right or to the left will move back towards the fixed point.  We will call
this fixed point <em>locally stable</em>.  More specifically, we'll distinguish
between three types of local stability:

<ul>

<li>Locally stable <em>in the sense of Lyapunov</em> (i.s.L.).  A fixed point, $x^*$ is
locally stable i.s.L. if for every small $\epsilon$, I can produce a $\delta$
such that if $\| x(0) - x^* \| < \delta$ then $\forall t$ $\| x(t) - x^*\| <
\epsilon$.  In words, this means that for any ball of size $\epsilon$ around the
fixed point, I can create a ball of size $\delta$ which guarantees that if the
system is started inside the $\delta$ ball then it will remain inside the
$\epsilon$ ball for all of time.</li>

<li>Locally <em>asymptotically stable</em>.  A fixed point is locally asymptotically
stable if it is stable i.s.L. and $x(0) = x^* + \epsilon$ implies that $\lim_{t\rightarrow \infty} x(t) = x^*$.</li>

<li>Locally <em>exponentially stable</em>.  A fixed point is locally exponentially stable
if $x(0) = x^* + \epsilon$ implies that $\| x(t) - x^* \| < Ce^{-\alpha t}$, for
some positive constants $C$ and $\alpha$.</li>

</ul>

An initial condition near a fixed point that is stable in the sense of Lyapunov
may never reach the fixed point (but it won't diverge), near an asymptotically
stable fixed point will reach the fixed point as $t \rightarrow \infty$, and
near an exponentially stable fixed point will reach the fixed point with a
bounded rate.  An exponentially stable fixed point is also an asymptotically
stable fixed point, and an asymptotically stable fixed point is also stable (by
definition<!-- see
http://books.google.com/books?id=ByCCAwAAQBAJ&pg=PA6&lpg=PA6&dq=locally+attractive+but+not++lyapunov+stable&source=bl&ots=BLlwzfFryl&sig=lDXX4oFzIhXEH7do-4llBXU3NKo&hl=en&sa=X&ei=tqEVVKDEGoqYyATh84G4Cw&ved=0CDkQ6AEwAw#v=onepage&q=locally%20attractive%20but%20not%20%20lyapunov%20stable&f=false
for a counter-example -->) but the converse of these is not necessarily
true. Interestingly, it is also possible to have nonlinear systems that converge
(or diverge) in finite-time; a so-called <em>finite-time stability</em>.  We
will see examples of this later in the book, but it is a difficult topic to
penetrate with graphical analysis.</p>

<p> Our graph of $\dot{x}$ vs. $x$ can be used to convince ourselves of i.s.L.
and asymptotic stability by visually inspecting $\dot{x}$ in the vicinity of a
fixed point.  Even exponential stability can be inferred if the function can be
bounded away from the origin by a negatively-sloped line through the fixed
point, since it implies that the nonlinear system will converge at least as fast
as the linear system represented by the straight line.  I will graphically
illustrate unstable fixed points with open circles and stable fixed points
(i.s.L.) with filled circles. </p>

<p>Next, we need to consider what happens to initial conditions which begin
farther from the fixed points.  If we think of the dynamics of the system as a
flow on the $x$-axis, then we know that anytime $\dot{x} > 0$, the flow is
moving to the right, and $\dot{x} < 0$, the flow is moving to the left.  If we
further annotate our graph with arrows indicating the direction of the flow,
then the entire (long-term) system behavior becomes clear:

<figure> <img width="70%" src="figures/pend_sinx_annotated.svg"/> </figure>

For instance, we can see that any initial condition $x(0) \in (\pi,\pi)$ will
result in $\lim_{t\rightarrow \infty} x(t) = 0$.  This region is called the <em>basin of
attraction</em> of the fixed point at $x^* = 0$. Basins of attraction of two
fixed points cannot overlap, and the manifold separating two basins of
attraction is called the <em>separatrix</em>.  Here the unstable fixed points,
at $x^* = \{.., -\pi, \pi, 3\pi, ...\}$ form the separatrix between the basins
of attraction of the stable fixed points. </p>

<p> As these plots demonstrate, the behavior of a first-order one dimensional
system on a line is relatively constrained.  The system will either
monotonically approach a fixed-point or monotonically move toward $\pm \infty$.
There are no other possibilities.  Oscillations, for example, are impossible.
Graphical analysis is a fantastic analysis tool for many first-order nonlinear
systems (not just pendula); as illustrated by the following example: </p>

<div data-type="example"><h1>Nonlinear autapse</h1> Consider the following
system: \begin{equation} \dot{x} + x = \tanh(w x), \end{equation} which is
plotted below for a few values of $w$.  It's convenient to note that $\tanh(z)
\approx z$ for small $z$.  For $w \le 1$ the system has only a single fixed
point.  For $w > 1$ the system has three fixed points : two stable and one
unstable.

<figure> <img width="80%" src="figures/pend_autapse.svg"/> </figure>

These equations are not arbitrary - they are actually a model for one of the
simplest neural networks, and one of the simplest model of persistent
memory<elib>Seung00</elib>.  In the equation $x$ models the firing rate of a
single neuron, which has a feedback connection to itself.  $\tanh$ is the
activation (sigmoidal) function of the neuron, and $w$ is the weight of the
synaptic feedback. </div>

<p> One last piece of terminology.  In the neuron example, and in many dynamical
systems, the dynamics were parameterized; in this case by a single parameter,
$w$.  As we varied $w$, the fixed points of the system moved around.  In fact,
if we increase $w$ through $w=1$, something dramatic happens - the system goes
from having one fixed point to having three fixed points.  This is called a
<em>bifurcation</em>.  This particular bifurcation is called a pitchfork
bifurcation.  We often draw bifurcation diagrams which plot the fixed points of
the system as a function of the parameters, with solid lines indicating stable
fixed points and dashed lines indicating unstable fixed points, as seen in the
figure:</p>

<figure> <todo>bifurcation diagram asymptotes to $x^* = 1$</todo> <img
width="80%" src="figures/pend_autapse_bifurcation.svg"/> <figcaption>Bifurcation
diagram of the nonlinear autapse.</figcaption> </figure>

<p> Our pendulum equations also have a (saddle-node) bifurcation when we change
the constant torque input, $u_0$.  <!-- This is the subject of
exercise~\ref{p:pend_bifurcation}.-->  Finally, let's return to the original
equations in $\theta$, instead of in $x$.  Only one point to make: because of
the wrap-around, this system will <em>appear</em> have oscillations.  In fact,
the graphical analysis reveals that the pendulum will turn forever whenever
$|u_0| > mgl$, but now you understand that this is not an oscillation, but an
instability with $\theta \rightarrow \pm \infty$. </p>

</section> <!-- end of overdamped pend -->

<section data-type="sect2"><h1>The Undamped Pendulum w/ Zero Torque</h1>

<p> Consider again the system $$ml^2 \ddot\theta = u_0 - mgl \sin\theta -
b\dot\theta,$$ this time with $b = 0$.  This time the system dynamics are truly
second-order.  We can always think of any second-order system as (coupled)
first-order system with twice as many variables. Consider a general, autonomous
(not dependent on time), second-order system, $$\ddot{q} = f(q,\dot q,u).$$ This
system is equivalent to the two-dimensional first-order system \begin{align*}
\dot x_1 =& x_2 \\ \dot x_2 =& f(x_1,x_2,u), \end{align*} where $x_1 = q$ and
$x_2 = \dot q$.  Therefore, the graphical depiction of this system is not a
line, but a vector field where the vectors $[\dot x_1, \dot x_2]^T$ are plotted
over the domain $(x_1,x_2)$.  This vector field is known as the <em>phase
portrait</em> of the system.</p>

<p> In this section we restrict ourselves to the simplest case when $u_0 = 0$.
Let's sketch the phase portrait.  First sketch along the $\theta$-axis. The
$x$-component of the vector field here is zero, the $y$-component is
$-mgl\sin\theta.$ As expected, we have fixed points at $\pm \pi, ...$ Now sketch
the rest of the vector field.  Can you tell me which fixed points are stable?
Some of them are stable i.s.L., none are asymptotically stable.

<figure> <img width="80%" src="figures/pend_undamped_phase.svg"/> </figure> </p>

<section data-type="sect3"><h1>Orbit Calculations</h1>

<p> You might wonder how we drew the black countour lines in the figure above.  We could
have obtained them by simulating the system numerically, but those lines can be
easily obtained in closed-form.  Directly integrating the equations of motion is
difficult, but at least for the case when $u_0 = 0$, we have some additional
physical insight for this problem that we can take advantage of.  The kinetic
energy, $T$, and potential energy, $U$, of the pendulum are given by $$T =
\frac{1}{2}I\dot\theta^2, \quad U = -mgl\cos(\theta),$$ where $I=ml^2$ and the
total energy is $E(\theta,\dot\theta) = T(\dot\theta)+U(\theta)$.  The undamped
pendulum is a conservative system: total energy is a constant over system
trajectories.  Using conservation of energy, we have:

\begin{gather*}  E(\theta(t),\dot\theta(t)) = E(\theta(0),\dot\theta(0)) = E_0 \\
\frac{1}{2} I \dot\theta^2(t) - mgl\cos(\theta(t)) = E_0  \\ \dot\theta(t) = \pm
\sqrt{\frac{2}{I}\left[E_0 + mgl\cos\left(\theta(t)\right)\right]} \end{gather*}

Using this, if you tell me $\theta$ I can determine one of two possible values
for $\dot\theta$, and the solution has all of the richness of the black countour
lines from the plot.  This equation has a real solution when $\cos(\theta) >
\cos(\theta_{max})$, where $$\theta_{max} = \begin{cases} \cos^{-1}\left(
\frac{E}{mgl} \right), & E < mgl \\ \pi, & \text{otherwise}. \end{cases}$$ Of
course this is just the intuitive notion that the pendulum will not swing above
the height where the total energy equals the potential energy.  As an exercise,
you can verify that differentiating this equation with respect to time indeed
results in the equations of motion.</p>

</section>
<section data-type="sect3"><h1>Trajectory Calculations</h1>

<p> For completeness, I'll include what it would take to solve for $\theta(t)$,
even thought it cannot be accomplished using elementary functions.  Feel free to
skip this subsection.  We begin the integration with

\begin{gather*} \frac{d\theta}{dt} = \sqrt{\frac{2}{I}\left[E +
mgl\cos\left(\theta(t)\right)\right]} \\ \int_{\theta(0)}^{\theta(t)}
\frac{d\theta}{\sqrt{\frac{2}{I}\left[E +
mgl\cos\left(\theta(t)\right)\right]}} = \int_0^t dt' = t \end{gather*}

The
integral on the left side of this equation is an (incomplete) elliptic integral
of the first kind.  Using the identity: $$\cos(\theta) = 1 - 2
\sin^2(\frac{1}{2}\theta),$$ and manipulating, we have $$t =
\sqrt{\frac{I}{2(E+mgl)}} \int_{\theta(0)}^{\theta(t)} \frac{d\theta}{\sqrt{1 -
k_1^2\sin^2(\frac{\theta}{2})}}, \quad \text{with
}k_1=\sqrt{\frac{2mgl}{E+mgl}}.$$ In terms of the incomplete elliptic integral
function, $$F(\phi,k) = \int_0^\phi \frac{d\theta}{\sqrt{1-k^2\sin^2\theta}},$$
accomplished by a change of variables.  If $E <= mgl$, which is the case of
closed-orbits, we use the following change of variables to ensure $ 0 < k < 1 $ :
\begin{gather*}\phi = \sin^{-1}\left[ k_1 \sin\left( \frac{\theta}{2} \right)
\right] \\ \cos(\phi) d\phi = \frac{1}{2} k_1 \cos\left(\frac{\theta}{2}\right)
d\theta = \frac{1}{2} k_1 \sqrt{1 - \frac{\sin^2 (\phi)}{k_1^2}} d\theta
\end{gather*} so we have
\begin{gather*}
t = \frac{1}{k_1}\sqrt{\frac{2I}{(E+mgl)}} \int_{\phi(0)}^{\phi(t)}
\frac{d\phi}{\sqrt{1 - \sin^2(\phi)}} \frac{\cos(\phi)}{\sqrt{1 -
\frac{\sin^2\phi}{k_1^2}}} \\ = \sqrt{\frac{I}{mgl}} \left[
F\left(\phi(t),k_2\right) - F\left(\phi(0),k_2\right) \right],\quad
k_2 = \frac{1}{k_1}.\end{gather*} The inverse of $F$ is given by the
Jacobi elliptic functions (sn,cn,...), yielding:
<!-- http://en.wikipedia.org/wiki/Jacobi%27s_elliptic_functions -->
\begin{gather*}\sin(\phi(t)) = \text{sn}\left(t
  \sqrt{\frac{mgl}{I}} + F\left(\phi(0),k_2\right),k_2 \right) \\
\theta(t) = 2\sin^{-1} \left[ k_2 \text{sn}\left(t
  \sqrt{\frac{mgl}{I}} + F\left(\phi(0),k_2\right),k_2 \right) \right]
\end{gather*}
The function $\text{sn}$ used here can be evaluated in MATLAB by
calling $$\text{sn}(u,k) = \text{ellipj}(u,k^2).$$ The function
$F$ is not implemented in MATLAB, but implementations can be
downloaded. (note that $F(0,k) = 0$).</p>

<p>
For the open-orbit case, $E>mgl$, we use $$\phi =
\frac{\theta}{2},\quad \frac{d\phi}{d\theta} = \frac{1}{2},$$ yielding
\begin{gather*}
t = \frac{2I}{E+mgl} \int_{\phi(0)}^{\phi(t)} \frac{d\phi}{\sqrt{1 -
    k_1^2 \sin^2(\phi)}} \\
\theta(t) = 2 \tan^{-1} \left[ \frac{ \text{sn} \left( t
    \sqrt{\frac{E+mgl}{2I}} + F\left( \frac{\theta(0)}{2}, k_1 \right)
    \right) } { \text{cn} \left( t
    \sqrt{\frac{E+mgl}{2I}} + F\left( \frac{\theta(0)}{2}, k_1 \right)
    \right) }
 \right]
\end{gather*}
Notes: Use MATLAB's <code>atan2</code> and <code>unwrap</code> to recover the
complete trajectory.</p>

<!-- primary refs:
  http://en.wikipedia.org/wiki/Pendulum_%28mathematics%29
  http://kr.cs.ait.ac.th/~radok/math/mat6/calc81.htm
  http://books.google.com/books?id=xWrJlTIYl_IC&pg=PA280&lpg=PA280&dq=pendulum+elliptic+integrals&source=web&ots=zas9k5qVc0&sig=Tz-OwdqS84VPM2I_pZbtaspVZNk#PPA280,M1 (Computational Physics: Problem Solving with Computers by Rubin H. Landau, Cristian C. Bordeianu, p.280)
  http://en.wikipedia.org/wiki/Binomial_theorem
  http://mathworld.wolfram.com/EllipticIntegraloftheFirstKind.html -->


</div>
</section>

</section> <!-- end of undamped pend -->

<section data-type="sect2"><h1>The Undamped Pendulum w/ a Constant Torque</h1>

<p> Now what happens if we add a constant torque?  If you visualize the
bifurcation diagram, you'll see that the fixed points come together, towards $q =
\frac{\pi}{2}, \frac{5\pi}{2}, ...$, until they disappear.  One fixed-point is
unstable, and one is stable.</p>

<!-- todo: find a way to add an animation or something in here.  do a matlab demo in class -->

</section> <!-- undamped constant torque -->

<section data-type="sect2"><h1>The Dampled Pendulum</h1>

<p>
Now let's add damping back.  You can still add torque to move the fixed points
(in the same way).</p>

<figure>
  <img width="90%" src="figures/pend_damped_phase.svg"/>
  <figcaption>Phase diagram for the damped pendulum</figcaption>
</figure>

<p> Here's a thought exercise.  If $u$ is no longer a constant, but a function
$\pi(q,\dot{q})$, then how would you choose $\pi$ to stabilize the vertical
position.  Feedback linearization is the trivial solution, for example: $$u =
\pi(q,\dot{q}) = 2 \frac{g}{l}\cos\theta.$$ But these plots we've been making
tell a different story.  How would you shape the natural dynamics - at each
point pick a $u$ from the stack of phase plots - to stabilize the vertical fixed
point <em>with minimal torque effort</em>?  This is exactly the way that I would
like you to think about control system design.  And we'll give you your first solution techniques -- using dynamic programming -- in the next lecture.</p>

</section> <!-- damped pend -->

</section> <!-- end of constant torque -->

<section data-type="sect1"><h1>The Torque-limited Simple Pendulum</h1>

<p>
The simple pendulum is fully actuated.  Given enough torque, we can
produce any number of control solutions to stabilize the originally
unstable fixed point at the top (such as designing a feedback law to
effectively invert gravity).</p>


<p>   The problem begins to get interesting (a.k.a. becomes underactuated) if we
impose a torque-limit constraint, $|u|\le u_{max}$.  Looking at the phase
portraits again, you can now visualize the control problem.  Via feedback, you
are allowed to change the direction of the vector field at each point, but only
by a fixed amount.  Clearly, if the maximum torque is small (smaller than
$mgl$), then there are some states which cannot be driven directly to the goal,
but must pump up energy to reach the goal.  Futhermore, if the torque-limit is
too severe and the system has damping, then it may be impossible to swing up to
the top.  The existence of a solution, and number of pumps required to reach the
top, is a non-trivial function of the initial conditions and the
torque-limits.</p>

<p> Although this system is very simple, its solution requires much of the same
reasoning necessary for controlling much more complex underactuated systems;
this problem will be a work-horse for us as we introduce new algorithms
throughout this book.</p>

</section>


</section>

 <!-- ***************  end of pendulum chapter **************   -->

<section data-type="chapter" class="chapter" id="ch:modelsys">
  <h1>Acrobots, Cart-Poles, and Quadrotors</h1>

<p> A great deal of work in the control of underactuated systems has been   done
in the context of low-dimensional model systems.  These model   systems capture
the essence of the problem without introducing all of   the complexity that is
often involved in more real-world examples. In   this chapter we will focus on
two of the most well-known and   well-studied model systems--the Acrobot and the
Cart-Pole.  After we have developed some tools, we will see that they can be
applied directly to other model systems; we will give a number of examples using
Quadrotors.  All of these systems are trivially underactuated, having less
actuators than degrees of freedom.</p>

<section data-type="sect1"><h1>The Acrobot</h1>

<p>
The Acrobot is a planar two-link robotic arm in the vertical plane
(working against gravity), with an actuator at the elbow, but no
actuator at the shoulder.  It was first
described in detail in <elib>Murray91</elib>.  The companion system,
with an actuator at the shoulder but not at the elbow, is known as the
Pendubot<elib>Spong97</elib>.  The Acrobot is so named because of its
resemblence to a gymnist (or acrobat) on a parallel bar, who controls
his/her motion predominantly by effort at the waist (and not effort at the
wrist).  The most common control task studied for the acrobot is the
swing-up task, in which the system must use the elbow (or waist)
torque to move the system into a vertical configuration then balance.
</p>

<figure>
<img width="40%" src="figures/acrobot.svg"/>
<figcaption>The Acrobot.  <a href="http://groups.csail.mit.edu/locomotion/drake/movies/acrobot_swingup.swf">Click here for an animation of the swing-up task</a> or <a href="http://youtu.be/FeCwtvrD76I">here to see a real robot</a>.</figcaption>
</figure>

<p>
The Acrobot is representative of the primary challenge in
underactuated robots.  In order to swing up and balance the entire
system, the controller must reason about and exploit the
state-dependent coupling between the actuated degree of freedom and
the un-actuated degree of freedom.  It is also an important system
because, as we will see, it closely resembles one of the simplest
models of a walking robot.</p>

<section data-type="sect2"><h1>Equations of Motion</h1>

<p>
Figure 3.1 illustrates the model parameters used in our
analysis.  $\theta_1$ is the shoulder joint angle, $\theta_2$ is the
elbow (relative) joint angle, and we will use $\bq =
[\theta_1,\theta_2]^T$, $\bx = [\bq,\dot\bq]^T$.  The zero state is
the with both links pointed directly down.  The moments of inertia,
$I_1,I_2$ are taken about the pivots<!--<elib>Spong95</elib> uses the
center of mass, which differs only by an extra term in each inertia
from the parallel axis theorem.-->.  The task is to stabilize the
unstable fixed point $\bx = [\pi,0,0,0]^T$.</p>

<p>
We will derive the equations of motion for the Acrobot using the
method of Lagrange.  The kinematics are given by:
\begin{equation}
\bx_1 = \begin{bmatrix} l_1 s_1 \\ -l_1 c_1
  \end{bmatrix}, \quad \bx_2 = \bx_1 + \begin{bmatrix} l_2 s_{1+2} \\ - l_2
    c_{1+2} \end{bmatrix} .
\end{equation}
The energy <!--\footnote{The complicated expression for $T_2$ comes from
  $T_2 = \frac{1}{2} m_2 v_{c2}^2 + \frac{1}{2}I_{c2} (\dot{q}_1 +
  \dot{q}_2)^2$, where $v_{c2}$ is the velocity of the center of mass
  of the second link and $I_{c2} = I_2 - m_2 l_{c2}^2$ is the moment
  of inertia about that point.  Use $c_1 c_{1+2} + s_1 s_{1+2} =
  c_2$ to simplify.}--> is given by:
\begin{gather}
T = T_1 + T_2, \quad T_1 = \frac{1}{2} I_1 \dot{q}_1^2 \\
T_2 = \frac{1}{2} ( m_2 l_1^2 + I_2 + 2 m_2 l_1 l_{c2} c_2 )
\dot{q}_1^2 + \frac{1}{2} I_2 \dot{q}_2^2 + (I_2 + m_2 l_1 l_{c2} c_2
) \dot{q}_1 \dot{q}_2 \\
% from expanding sum of point masses
U = -m_1 g l_{c1} c_1 - m_2 g (l_1 c_1 + l_{c2} c_{1+2})
\end{gather}
Entering these quantities into the Lagrangian yields the equations of
motion:
\begin{gather}
(I_1 + I_2 + m_2 l_1^2 + 2m_2 l_1 l_{c2} c_2) \ddot{q}_1 + (I_2 + m_2
  l_1 l_{c2} c_2)\ddot{q}_2 - 2m_2 l_1 l_{c2} s_2 \dot{q}_1 \dot{q}_2
  \\ \quad -m_2 l_1 l_{c2} s_2 \dot{q}_2^2 + m_1 g l_{c1}s_1 + m_2 g
  (l_1 s_1 + l_{c2} s_{1+2}) = 0 \\
(I_2 + m_2 l_1 l_{c2} c_2) \ddot{q}_1 + I_2 \ddot{q}_2 + m_2 l_1
  l_{c2} s_2 \dot{q}_1^2 + m_2 g l_{c2} s_{1+2} = \tau
\end{gather}
In standard, manipulator equation form, we have:
\begin{gather}
{\bf H}(\bq) = \begin{bmatrix} I_1 + I_2 + m_2 l_1^2 + 2m_2 l_1 l_{c2}
  c_2 & I_2 + m_2 l_1 l_{c2} c_2 \\ I_2 + m_2 l_1 l_{c2} c_2 & I_2
  \end{bmatrix},\label{eq:Hacrobot}\\{\bf C}(\bq,\dot{\bq}) = \begin{bmatrix} -2 m_2
  l_1 l_{c2} s_2 \dot{q}_2 & -m_2 l_1 l_{c2} s_2 \dot{q}_2 \\ m_2 l_1
  l_{c2} s_2 \dot{q}_1 & 0 \end{bmatrix}, \\
{\bf G}(\bq) = \begin{bmatrix} m_1 g l_{c1}s_1 + m_2 g (l_1 s_1 + l_{c2}s_{1+2})
\\ m_2 g l_{c2} s_{1+2} \end{bmatrix}, \quad
  {\bf B} = \begin{bmatrix} 0 \\ 1 \end{bmatrix}.
\end{gather}
</p>
<!--
Solving directly for the accelerations, we have
\begin{gather}
\det({\bf H}) = I_1 I_2 + m_2 l_1^2 (1 - m_2 l_{c2}^2 c_2^2)
\\
\ddot{q}_1 = \frac{1}{det({\bf H})} \left[ I_2 \left( m_2 l_1 l_{c2} s_2
  (2 \dot{q}_1 + \dot{q}_2) \dot{q}_2 - (m_1 l_{c1} + m_2 l_1) g s_1 -
  m_2 g l_{c2} s_{1+2} \right) + (I_{c2} + m_2 l_1 l_{c2} c_2) \left( -m_2
  l_1 l_{c2} s_2 \dot{q}_1^2 - m_2 g l_{c2} s_{1+2} \right)
  \right]
\end{gather} -->

<div data-type="example"><h1>The Acrobot in MATLAB</h1>
<p>You can experiment with the Acrobot dynamics in <drake></drake> using, e.g.
<pre><code class="matlab">
cd(fullfile(getDrakePath,'examples','Acrobot'));

plant=AcrobotPlant();
[H,C_times_v,G,B] = plant.manipulatorEquations()

runPassive();  % simulate with torque=0 (and display the results)
</code></pre>

</div>

</section> <!-- acrobot eqs of motion -->

</section> <!-- acrobot -->

<section data-type="sect1"><h1>The Cart-Pole System</h1>

<p> The other model system that we will investigate here is the cart-pole
system, in which the task is to balance a simple pendulum around its   unstable
unstable equilibrium, using only horizontal forces on the   cart.  Balancing the
cart-pole system is used in many introductory   courses in control, including
6.003 at MIT, because it can be   accomplished with simple linear control (e.g.
pole placement)   techniques.  In this chapter we will consider the full
swing-up and   balance control problem, which requires a full nonlinear control
treatment.</p>

<figure>
  <img width="50%" src="figures/cartpole.svg"/>
  <figcaption>The Cart-Pole System.  Click <a href="http://youtu.be/Bzq96V1yN5k">here to see a real robot</a>.</figcaption>
  <todo> add swing-up + balance swf</todo>
</figure>

<p> The figure shows our parameterization of the system. $x$   is the horizontal
position of the cart, $\theta$ is the   counter-clockwise angle of the pendulum
(zero is hanging straight   down).  We will use $\bq = [x,\theta]^T$, and $\bx =
[\bq,\dot\bq]^T$.   The task is to stabilize the unstable fixed point at $\bx =
[0,\pi,0,0]^T.$ </p>

<section data-type="sect2"><h1>Equations of Motion</h1>

<p>
The kinematics of the system are given by
  \begin{equation}\bx_1 = \begin{bmatrix} x \\ 0
  \end{bmatrix}, \quad \bx_2 = \begin{bmatrix} x + l\sin\theta \\
  -l\cos\theta \end{bmatrix}. \end{equation}
  The energy is given by
  \begin{align} T=& \frac{1}{2} (m_c + m_p)\dot{x}^2 + m_p
  \dot{x}\dot\theta l \cos{\theta} + \frac{1}{2}m_p l^2 \dot\theta^2 \\
  U =& -m_p g l \cos\theta. \end{align}
  The Lagrangian yields the equations of motion:
  \begin{gather}
  (m_c + m_p)\ddot{x} + m_p l \ddot\theta \cos\theta - m_p l \dot\theta^2 \sin\theta = f \\
  m_p l \ddot{x} \cos\theta + m_p l^2 \ddot\theta + m_p g l \sin\theta = 0
  \end{gather}
  In standard form, using $\bq = [x,\theta]^T$, $\bu = f$:
  $${\bf H}(\bq)\ddot\bq + {\bf C}(\bq,\dot\bq)\dot\bq + {\bf G}(\bq) =
  {\bf B}\bu,$$
  where \begin{gather*}
  {\bf H}(\bq) = \begin{bmatrix} m_c + m_p & m_p l \cos\theta \\ m_p l
    \cos\theta & m_p l^2 \end{bmatrix}, \quad {\bf C}(\bq,\dot{\bq}) =
  \begin{bmatrix} 0 & -m_p l \dot\theta \sin\theta \\ 0 & 0
  \end{bmatrix}, \\
  {\bf G}(\bq) = \begin{bmatrix} 0 \\ m_p gl \sin\theta \end{bmatrix},
  \quad {\bf B} = \begin{bmatrix} 1 \\ 0 \end{bmatrix}
  \end{gather*}
  In this case, it is particularly easy to solve directly for the accelerations:
  \begin{align}
  \ddot{x} =& \frac{1}{m_c + m_p \sin^2\theta}\left[ f+m_p \sin\theta (l \dot\theta^2 + g\cos\theta)\right] \label{eq:ddot_x}\\
  \ddot{\theta} =& \frac{1}{l(m_c + m_p \sin^2\theta)} \left[ -f
   \cos\theta - m_p l \dot\theta^2 \cos\theta \sin\theta - (m_c + m_p) g \sin\theta \right] \label{eq:ddot_theta}
  \end{align}
  In some of the follow analysis that follows, we will study the form of
  the equations of motion, ignoring the details, by arbitrarily setting
  all constants to 1:
  \begin{gather}
  2\ddot{x} + \ddot\theta \cos\theta - \dot\theta^2 \sin\theta = f \label{eq:simple}\\
  \ddot{x}\cos\theta + \ddot\theta + \sin\theta = 0. \label{eq:simple2}
  \end{gather}
</p>

<div data-type="example"><h1>The Cart-Pole System in MATLAB</h1>
<p>You can experiment with the Cart-Pole dynamics in <drake></drake> using, e.g.
<pre><code class="matlab">
cd(fullfile(getDrakePath,'examples','CartPole'));

plant=CartPolePlant();
[H,C_times_v,G,B] = plant.manipulatorEquations()

runPassive();  % simulate with force=0 (and display the results)
</code></pre>
</div>

</section> <!-- end eq of motion -->

</section> <!-- end cart-pole -->



<section data-type="sect1"><h1>Balancing</h1>

<p>
For both the Acrobot and the Cart-Pole systems, we will begin by
designing a linear controller which can balance the system when it
begins in the vicinity of the unstable fixed point.  To accomplish
this, we will linearize the nonlinear equations about the fixed point,
examine the controllability of this linear system, then using
linear quadratic regulator (LQR) theory to design our feedback
controller.</p>

<section data-type="sect2"><h1>Linearizing the Manipulator Equations</h1>

<p>
Although the equations of motion of both of these model systems are
relatively tractable, the forward dynamics still involve quite a few
nonlinear terms that must be considered in any linearization.  Let's
consider the general problem of linearizing a system described by the
manipulator equations.</p>

<p>
We can perform linearization around a fixed point, $(\bx^*,
\bu^*)$, using a Taylor expansion:
\begin{equation}
\dot\bx = {\bf f}(\bx,\bu) \approx {\bf f}(\bx^*,\bu^*) + \left[ \pd{\bf
  f}{\bx}\right]_{\bx=\bx^*,\bu=\bu^*} (\bx - \bx^*) + \left[ \pd{\bf
  f}{\bu}\right]_{\bx=\bx^*,\bu=\bu^*} (\bu - \bu^*)
\end{equation}
Let us consider the specific problem of linearizing the manipulator
equations around a (stable or unstable) fixed point.  In this case,
${\bf f}(\bx^*,\bu^*)$ is zero, and we are left with the standard
linear state-space form:
\begin{align}
\dot\bx =& \begin{bmatrix} \dot\bq \\ {\bf H}^{-1}(\bq) \left[ {\bf
      B}(\bq)\bu - {\bf C}(\bq,\dot\bq)\dot\bq - {\bf G}(\bq) \right]
      \end{bmatrix},\\
\approx& {\bf A}_{lin} (\bx-\bx^*) + {\bf B}_{lin} (\bu - \bu^*),
\end{align}
where ${\bf A}_{lin}$, and ${\bf B}_{lin}$ are constant matrices.  Let us define $\bar\bx = \bx - \bx^*, \bar\bu = \bu - \bu^*$, and
write $$\dot{\bar\bx} = {\bf A}_{lin}\bar\bx + {\bf B}_{lin}\bar\bu.$$ Evaluation of
the Taylor expansion around a fixed point yields the following, very
simple equations, given in block form by:
\begin{align}
{\bf A}_{lin} =& \begin{bmatrix} {\bf 0} & {\bf I} \\ -{\bf H}^{-1} \pd{\bf G}{\bq} + \sum_{j} {\bf H}^{-1}\pd{{\bf B}_j}{\bq} u_j  & -{\bf H}^{-1} {\bf
  C}  \end{bmatrix}_{\bx=\bx^*,\bu=\bu^*} \\
{\bf B}_{lin} =& \begin{bmatrix} {\bf 0} \\ {\bf H}^{-1} {\bf B}
  \end{bmatrix}_{\bx=\bx^*, \bu=\bu^*}
\end{align}
where ${\bf B}_j$ is the $j$th column of ${\bf B}$.  Note that the term involving $\pd{{\bf H}^{-1}}{q_i}$ disappears
because ${\bf B}\bu - {\bf C}\dot{\bq} - {\bf G}$ must be zero at the
fixed point.  Many of the ${\bf C}\dot\bq$ derivatives drop out, too,
  because $\dot{\bq}^* = 0$.  In many cases, including both
  the Acrobot and Cart-Pole systems (but not the Quadrotors), the matrix ${\bf B}(\bq)$ is a constant,
  so the $\pd{\bf B}{\bq}$ terms also drop out.
</p>

<div data-type="example"><h1>Linearization of the Acrobot</h1>

<p>
  Linearizing around the (unstable) upright point, we have:
\begin{gather}
{\bf C}(\bq,\dot\bq)_{\bx=\bx^*} = {\bf 0},\\
\left[\pd{\bf G}{\bq}\right]_{\bx=\bx^*} = \begin{bmatrix} -g (m_1 l_{c1}
    + m_2 l_1 + m_2 l_{c2}) & -m_2 g l_{c2} \\ -m_2 g l_{c2} & -m_2 g l_{c2}
\end{bmatrix}
\end{gather}
The linear dynamics follow directly from these equations and the
manipulator form of the Acrobot equations.</p>

</div>

<div data-type="example"><h1>Linearization of the Cart-Pole System</h1>

<p>
Linearizing around the unstable fixed point in this system, we have:
\begin{gather}
{\bf C}(\bq,\dot\bq)_{\bx=\bx^*} = {\bf 0},\quad
\left[\pd{\bf G}{\bq}\right]_{\bx=\bx^*} = \begin{bmatrix} 0 & 0 \\ 0
  & -m_p g l \end{bmatrix}
\end{gather}
Again, the linear dynamics follow simply.
</p>

</div>

</section> <!-- end linearizing -->

<section data-type="sect2"><h1>Controllability of Linear Systems</h1>

<p>   <div data-type="definition"><h1>Controllability</h1>     A control system
of the form $$\dot{\bx} = {\bf f}(\bx,\bu)$$ is called   <em>controllable</em>
if it is possible to construct an unconstrained   input signal, $\bu(t)$, $t \in
[0,t_f],$ which will move the system from any initial state to any final state
in a finite interval of time, $0 < t < t_f$ <elib>Ogata96</elib>. </div></p>

<p>
For the linear system $$\dot{\bx} = {\bf A}\bx + {\bf B}u,$$
we can integrate this linear system in closed
form, it is possible to derive the exact conditions of
controllability.  In particular, for linear systems it is sufficient to demonstrate
that there exists a control input which drives any initial condition to the origin. </p>

<section data-type"sect3"><h1>The special case of non-repeated eigenvalues</h1>

<p>
Let us first examine a special case, which falls short as a general
tool but may be more useful for understanding the intuition of
controllability.  Let's perform an eigenvalue analysis of the system
matrix ${\bf A}$, so that: $${\bf A}{\bf v}_i = \lambda_i {\bf v}_i,$$
where $\lambda_i$ is the $i$th eigenvalue, and ${\bf v}_i$ is the
corresponding (right) eigenvector.  There will be $n$ eigenvalues for
the $n \times n$ matrix ${\bf A}$.  Collecting the (column) eigenvectors into
the matrix ${\bf V}$ and the eigenvalues into a diagonal matrix ${\bf
 \Lambda}$, we have $${\bf A}{\bf V} = {\bf V}{\bf \Lambda}.$$
Here comes our primary assumption: let us assume that each of these
$n$ eigenvalues takes on a distinct value (no repeats).  With this
assumption, it can be shown that the eigenvectors ${\bf v}_i$ form a
linearly independent basis set, and therefore ${\bf V}^{-1}$ is
well-defined.</p>

<p>
We can continue our eigenmodal analysis of the linear system by
defining the modal coordinates, ${\bf r}$ with: $$\bx = {\bf V}{\bf
  r},\quad \text{or}\quad {\bf r} = {\bf V}^{-1}\bx.$$  In modal
coordinates, the dynamics of the linear system are given by $$\dot{\bf
  r} = {\bf V}^{-1} {\bf A} {\bf V} {\bf r} + {\bf V}^{-1} {\bf B} \bu
= {\bf \Lambda} {\bf r} + {\bf V}^{-1}{\bf B} \bu.$$  This illustrates
the power of modal analysis; in modal coordinates, the dynamics
diagonalize yeilding independent linear equations: $$\dot{r}_i =
\lambda_i r_i + \sum_j \beta_{ij} u_j,\quad {\bf \beta} = {\bf V}^{-1}
       {\bf B}.$$</p>

<p>
Now the concept of controllability becomes clear.  Input $j$ can
influence the dynamics in modal coordinate $i$ if and only if ${\bf
  \beta}_{ij} \neq 0$.  In the special case of non-repeated
eigenvalues, having control over each individual eigenmode is
sufficient to (in finite-time) regulate all of the
eigenmodes<elib>Ogata96</elib>.
Therefore, we say that the system is controllable if and only if
$$\forall i, \exists j \text{ such that }\beta_{ij} \neq 0.$$</p>

</section>

<section data-type="sect3"><h1>A general solution</h1>

<p>
A more general solution to the controllability issue, which removes
our assumption about the eigenvalues, can be obtained by examining the
time-domain solution of the linear equations.  The solution of this
system is $$\bx(t) = e^{{\bf A}t} \bx(0) + \int_0^{t} e^{{\bf A}(t -
\tau)} {\bf B} u(\tau) d\tau.$$ Without loss of generality, lets
consider the that the final state of the system is zero.  Then we
have: $$\bx(0) = - \int_0^{t_f} e^{-{\bf A}\tau}{\bf B}u(\tau)
d\tau.$$ You might be wondering what we mean by $e^{{\bf A}t}$; a
scalar raised to the power of a matrix..?  Recall that $e^{z}$ is
actually defined by a convergent infinite sum: $$e^{z} =1 + z +
\frac{1}{2} x^2 + \frac{1}{6} z^3 + ... .$$ The notation $e^{{\bf
A}t}$ uses the same definition: $$e^{{\bf A}t} = {\bf I} + {\bf A}t +
\frac{1}{2}({\bf A}t)^2 + \frac{1}{6}({\bf A}t)^3 + ... .$$ Not
surprisingly, this has many special forms.  For instance, $e^{{\bf
A}t} = {\bf V}e^{{\bf\Lambda}t}{\bf V}^{-1},$ where ${\bf A} = {\bf V
\Lambda V}^{-1}$ is the eigenvalue decomposition of ${\bf A}$
<elib>Strang98</elib>.  The particular form we will use here is
$$e^{-{\bf A}\tau} = \sum_{k=0}^{n-1} \alpha_k(\tau) {\bf A}^k.$$ This
is a particularly surprising form, because the infinite sum above is
represented by this finite sum; the derivation uses Sylvester's
Theorem <elib>Ogata96</elib>,<elib>Chen98a</elib>.  Then
we have, \begin{align*} \bx(0) =& - \sum_{k=0}^{n-1} {\bf A}^k {\bf B}
\int_0^{t_f} \alpha_k(\tau) u(\tau) d\tau \\ =& -\sum_{k=0}^{n-1} {\bf
A}^k {\bf B} w_k \text{, where } w_k = \int_0^{t_f} \alpha_k(\tau)
u(\tau) d\tau \\ =& - \begin{bmatrix} {\bf B} & {\bf AB} & {\bf
A}^2{\bf B} & \cdots & {\bf A}^{n-1}{\bf B} \end{bmatrix}_{n \times n}
\begin{bmatrix} w_0 \\ w_1 \\ w_2 \\ \vdots \\ w_{n-1} \end{bmatrix}
\end{align*}
The matrix containing the vectors ${\bf B}$, ${\bf AB}$, ... ${\bf
A}^{n-1}{\bf B}$ is called the controllability matrix.  In order for
the system to be controllable, for every initial
condition $\bx(0)$ we must be able to find the corresponding vector
${\bf w}$.  This is only possible when the columns of the
controllability matrix are linearly independent.  Therefore, the
condition of controllability is that this controllability matrix is
full rank.</p>

<p>
Although we only treated the case of a scalar $u$, it is possible to
extend the analysis to a vector $\bu$ of size $m$, yielding the
condition $$\text{rank} \begin{bmatrix} {\bf B} & {\bf AB} & {\bf
A}^2{\bf B} & \cdots & {\bf A}^{n-1}{\bf B} \end{bmatrix}_{n \times
(nm)} = n.$$ In Matlab (using the control systems toolbox),
you can obtain the controllability matrix using <code>Cm = ctrb(A,B)</code>,
and evaluate its rank with <code>rank(Cm)</code>.</p>

<p>Note that a
linear feedback to change the eigenvalues of the eigenmodes is not
sufficient to accomplish our goal of getting to the goal in finite
time.  In fact, the open-loop control to reach the goal is easily
obtained with a final-value LQR problem, and (for ${\bf
  R}={\bf I}$) is actually a simple function of the controllability
Grammian<elib>Chen98a</elib>.</p>

</section>

<section data-type="sect3"><h1>Controllability vs. Underactuated</h1>

<p>
Analysis of the controllability of both the Acrobot and Cart-Pole
systems reveals that the linearized dynamics about the upright are, in
fact, controllable.  This implies that the linearized system, if
started away from the zero state, can be returned to the zero state in
finite time.  This is potentially surprising - after all the systems
are underactuated.  For example, it is interesting and surprising that
the Acrobot can balance itself in the upright position without having
a shoulder motor.</p>

<p>
The controllability of these model systems demonstrates an extremely
important, point: <em>An underactuated system is not necessarily
an uncontrollable system.</em>  Underactuated systems cannot follow
arbitrary trajectories, but that does not imply that they cannot
arrive at arbitrary points in state space.  However, the trajectory
required to place the system into a particular state may be arbitrarly
complex.</p>

<p> The controllability analysis presented here is for linear time-invariant
(LTI) systems.  A comparable analysis exists for linear time-varying (LTV)
systems.  We will even see extensions to nonlinear systems; although it will
often be referred to by the synonym of "reachability" analysis.</p>

</section>

</section> <!-- end controllability -->

<section data-type="sect2"><h1>LQR Feedback</h1>

<p>
Controllability tells us that a trajectory to the fixed point exists,
but does not tell us which one we should take or what control inputs
cause it to occur?  Why not?  There are potentially infinitely many
solutions.  We have to pick one.</p>

<p> The tools for controller design in linear systems are very advanced. In
particular, as we describe in the linear optimal control chapter, one can easily
design an optimal feedback controller for a regulation task like balancing, so
long as we are willing to linearize the system around the operating point and
define optimality in terms of a quadratic cost function: $$J(\bx_0) =
\int_0^\infty \left[ \bx(t)^T {\bf Q} \bx(t) + \bu(t) {\bf R} \bu(t) \right]dt,
\quad \bx(0)=\bx_0, {\bf Q}={\bf Q}^T>0, {\bf R}={\bf R}^T>0.$$ The linear
feedback matrix ${\bf K}$ used as $$\bu(t) = - {\bf K}\bx(t),$$ is the so-called
optimal linear quadratic regulator (LQR).  Even without understanding the
detailed derivation, we can quickly become practioners of LQR.  Conveniently,
Matlab has a function, <code>K = lqr(A,B,Q,R)</code>. Therefore, to use LQR, one
simply needs to obtain the linearized system dynamics and to define the
symmetric positive-definite cost matrices, ${\bf Q}$ and ${\bf R}$.  In their
most common form, ${\bf Q}$ and ${\bf R}$ are positive diagonal matrices, where
the entries $Q_{ii}$ penalize the relative errors in state variable $x_i$
compared to the other state variables, and the entries $R_{ii}$ penalize actions
in $u_i$.</p>

<div data-type="example"><h1>LQR for the Acrobot and Cart-Pole</h1>

Take a minute to play around with the LQR controller for the Acrobot
<pre><code class="matlab">
cd(fullfile(getDrakePath,'examples','Acrobot'));
runLQR();
</code></pre>
and Cart-Pole
<pre><code class="matlab">
cd(fullfile(getDrakePath,'examples','CartPole'));
runLQR();
</code></pre>
Make sure that you take a minute to look at the code which runs during these
examples.  Can you set the ${\bf Q}$ and ${\bf R}$ matrices differently, to improve the performance?
</p>
</div>


<p>
Simulation of the closed-loop response with LQR feedback shows that the
task is indeed completed - and in an impressive manner.  Often times
the state of the system has to move violently away from the origin in
order to ultimately reach the origin.  Further inspection reveals the
(linearized) closed-loop dynamics are in fact non-minimum phase (acrobot has 3 right-half zeros, cart-pole
has 1).</p>


<div data-type="example"><h1>LQR for a Quadrotors</h1>

<p>LQR works essentially out of the box for Quadrotors, if linearized around
  a nominal fixed point (where the non-zero thrust from the propellors is balancing
  gravity).  We have a few Quadrotor models in <drake></drake>.  You should
  start with the Planar Quadrotor (technically a bi-rotor):
<pre><code class="matlab">
cd(fullfile(getDrakePath,'examples','Quadrotor2D'));
runLQR();
</code></pre>
but you can also do this with a full 3D quadrotor model:
<pre><code class="matlab">
cd(fullfile(getDrakePath,'examples','Quadrotor'));
runLQR();
</code></pre>
Try to change these ${\bf Q}$ and ${\bf R}$ costs here, too.
</p>
</div>

<p>
Note that LQR, although it is optimal for the linearized system, is
not necessarily the best linear control solution for maximizing basin
of attraction of the fixed-point.  The theory of <em>robust
control</em>(e.g., <elib>Zhou97</elib>), which explicitly takes into account the differences between the
linearized model and the nonlinear model, will produce controllers
which outperform our LQR solution in this regard.</p>

</section>

</section> <!-- end balancing -->

<section data-type="sect1"><h1>Partial Feedback Linearization</h1>

<p>
In the introductory chapters, we made the point that the underactuated
systems are not feedback linearizable.  At least not completely.
Although we cannot linearize the full dynamics of the system, it is
still possible to linearize a portion of the system dynamics.  The
technique is called <em>partial feedback linearization</em>.</p>

<p>
Consider the cart-pole example.  The dynamics of the cart are effected
by the motions of the pendulum.  If we know the model, then it seems
quite reasonable to think that we could create a feedback controller
which would push the cart in exactly the way necessary to counter-act
the dynamic contributions from the pendulum - thereby linearizing the
cart dynamics.  What we will see, which is potentially more
surprising, is that we can also use a feedback law for the cart to
feedback linearize the dynamics of the passive pendulum joint.</p>

<p>
We'll use the term <em>collocated</em> partial feedback linearization to
describe a controller which linearizes the dynamics of the actuated
joints.  What's more surprising is that it is often possible to
achieve <em>non-collocated</em> partial feedback linearization - a
controller which linearizes the dynamics of the unactuated joints.
The treatment presented here follows from <elib>Spong94a</elib>.</p>

<section data-type="sect2"><h1>PFL for the Cart-Pole System</h1>

<section data-type="sect3"><h1>Collocated</h1>

<p>
Starting from the equations \ref{eq:simple} and \ref{eq:simple2}, we have
\begin{gather*}
\ddot\theta = -\ddot{x}c - s \\
% \ddot\theta = -\frac{1}{l} (\ddot{x} c + g s)
\ddot{x}(2-c^2) - sc - \dot\theta^2 s = f
\end{gather*}
Therefore, applying the feedback control law
\begin{equation}
f = (2 - c^2) \ddot{x}^d - sc - \dot\theta^2 s
% f = (m_c + m_p) u + m_p (u c + g s) c - m_p l \dot\theta^2 s
\end{equation}
results in
\begin{align*}
\ddot{x} =& \ddot{x}^d \\ \ddot{\theta} =& -\ddot{x}^dc - s,
\end{align*}
which are valid globally.</p>

</section>


<section data-type="sect3"><h1>Non-collocated</h1>

<p>
Starting again from equations \ref{eq:simple} and \ref{eq:simple2}, we have
\begin{gather*}
\ddot{x} = -\frac{\ddot\theta + s}{c} \\
\ddot\theta(c - \frac{2}{c}) - 2 \tan\theta - \dot\theta^2 s = f
\end{gather*}
Applying the feedback control law \begin{equation} f = (c - \frac{2}{c})
\ddot\theta^d - 2 \tan\theta - \dot\theta^2 s \end{equation} results in
\begin{align*} \ddot\theta =& \ddot\theta^d \\ \ddot{x} =& -\frac{1}{c} \ddot\theta^d -
\tan\theta. \end{align*} Note that this expression is only valid when
$\cos\theta \neq 0$.  This is not surprising, as we know that the
force cannot create a torque when the beam is perfectly horizontal.</p>

</section>

</section> <!-- end PFL for the cart-pole -->

<section data-type="sect2"><h1>General Form</h1>

<p>
For systems that are trivially underactuated (torques on some joints,
no torques on other joints), we can, without loss of generality,
reorganize the joint coordinates in any underactuated system described
by the manipulator equations into the form:
\begin{align}
{\bf H}_{11} \ddot{\bq}_1 + {\bf H}_{12} \ddot{\bq}_2 + {\bf \phi}_1 = 0, \label{eq:passive_dyn}\\
{\bf H}_{21} \ddot{\bq}_1 + {\bf H}_{22} \ddot{\bq}_2 + {\bf \phi}_2 =
{\bf \tau}, \label{eq:active_dyn}
\end{align}
with $\bq \in \Re^n$, $\bq_1 \in \Re^m$, $\bq_2 \in \Re^l$, $l=n-m$.
$\bq_1$ represents all of the passive joints, and $\bq_2$ represents
all of the actuated joints, and the $\phi$ terms capture all of the
Coriolis and gravitational terms, and $${\bf H}(\bq) =
\begin{bmatrix} {\bf H}_{11} & {\bf H}_{12} \\ {\bf H}_{21} & {\bf
    H}_{22} \end{bmatrix}.$$
Fortunately, because ${\bf H}$ is uniformly (e.g. for all $\bq$) positive definite, ${\bf H}_{11}$
and ${\bf H}_{22}$ are also positive definite.</p>

<section data-type="sect3"><h1>Collocated linearization</h1>

<p>
Performing the same substitutions into the full manipulator equations,
we get:
\begin{gather}
\ddot\bq_1 = -{\bf H}_{11}^{-1} \left[ {\bf H}_{12} \ddot\bq_2 + {\bf \phi}_1
  \right] \\
({\bf H}_{22} - {\bf H}_{21} {\bf H}_{11}^{-1} {\bf H}_{12})
  \ddot\bq_2 + {\bf \phi_2} - {\bf H}_{21} {\bf H}_{11}^{-1} {\bf
  \phi}_1 = {\bf \tau}
\end{gather}
It can be easily shown that the matrix $({\bf H}_{22} - {\bf H}_{21}
{\bf H}_{11}^{-1} {\bf H}_{12})$ is invertible<elib>Spong94a</elib>; we can
see from inspection that it is symmetric.  PFL follows naturally, and
is valid globally.</p>

</section>


<section data-type="sect3"><h1>Non-collocated linearization</h1>

<p>
\begin{gather}
\ddot\bq_2 = -{\bf H}_{12}^+ \left[ {\bf H}_{11} \ddot\bq_1 + {\bf \phi}_1
  \right] \\
({\bf H}_{21} - {\bf H}_{22} {\bf H}_{12}^+ {\bf H}_{11})
  \ddot\bq_1 + {\bf \phi_2} - {\bf H}_{22} {\bf H}_{12}^+ {\bf
  \phi}_1 = {\bf \tau}
\end{gather}
where ${\bf H}_{12}^+$ is a Moore-Penrose pseudo-inverse.  This
inverse provides a unique solution when the rank of ${\bf H}_{12}$
equals $l$, the number of passive degrees of freedom in the system (it
cannot be more, since the matrix only has $l$ rows).  This rank
condition is sometimes called the property of ``Strong Inertial
Coupling''.  It is state dependent.  Global Strong Inertial Coupling
if every state is coupled.</p>

</section>

<section data-type="sect3"><h1>Task Space Partial Feedback Linearization</h1>

<p>
In general, we can define some combination of active and passive
joints that we would like to control.  This combination is sometimes
called a ``task space''.  % cite some task space refs here?
Consider an output function of the form, $${\bf y} = {\bf
f}(\bq),$$ with ${\bf y} \in \Re^p$, which defines the task space.  Define
${\bf J}_1 = \frac{\partial {\bf f}}{\partial \bq_1}$, ${\bf J}_2 =
\frac{\partial {\bf f}}{\partial \bq_2}$, ${\bf J} = [{\bf J}_1,{\bf J}_2]$.</p>

<div data-type="theorem"><h1>Task Space PFL</h1>

<p>
If the actuated joints are commanded so that
 \begin{equation} \ddot\bq_2 = \bar{\bf J}^+ \left [{\bf v} -
    \dot{\bf J}\dot\bq + {\bf J}_1 {\bf H}_{11}^{-1}{\bf \phi}_1 \right],  \label{eq:q2cmd} \end{equation}
where $\bar{{\bf J}} = {\bf J}_2 - {\bf J}_1 {\bf H}_{11}^{-1} {\bf H}_{12}.$
and $\bar{\bf J}^+$ is the right Moore-Penrose pseudo-inverse, $$\bar{\bf J}^+
= \bar{\bf J}^T (\bar{\bf J} \bar{\bf J}^T)^{-1},$$ then we have \begin{equation} \ddot{\bf y} = {\bf v}.\end{equation}
subject to
\begin{equation}\text{rank}\left(\bar{{\bf J}} \right) = p, \label{eq:rank_condition}\end{equation}
</p>

<p><b>Proof Sketch.</b>
  Differentiating the output function we have
  \begin{gather*}
  \dot{\bf y} = {\bf J} \dot\bq \\
  \ddot{\bf y} = \dot{\bf J} \dot\bq + {\bf J}_1 \ddot\bq_1 + {\bf J}_2 \ddot\bq_2.
  \end{gather*}
  Solving \ref{eq:passive_dyn} for the dynamics of the unactuated joints
  we have:
  \begin{equation}
  \ddot\bq_1 = - {\bf H}_{11}^{-1} ({\bf H}_{12} \ddot\bq_2 + {\bf \phi}_1)  \label{eq:q1cmd}
  \end{equation}
  Substituting, we have
  \begin{align}
  \ddot{\bf y} =& \dot{\bf J} \dot\bq - {\bf J}_1 {\bf H}_{11}^{-1}({\bf
    H}_{12}\ddot\bq_2 + {\bf \phi}_1) + {\bf J}_2 \ddot\bq_2 \\
  =& \dot{\bf J} \dot\bq + \bar{{\bf J}} \ddot\bq_2 - {\bf J}_1 {\bf H}_{11}^{-1}{\bf \phi}_1 \\
  =& {\bf v}
  \end{align}
  Note that the last line required the rank condition
  ($\ref{eq:rank_condition}$) on $\bar{\bf J}$ to ensure that the rows
  of $\bar{{\bf J}}$ are linearly independent, allowing $\bar{\bf J}
  \bar{\bf J}^+ = {\bf I}$.</p>

</div>

<p>
In order to execute a task space trajectory one could command $${\bf
v} = \ddot{\bf y}^d + {\bf K}_d (\dot{\bf y}^d - \dot{\bf y}) + {\bf K}_p ({\bf y}^d -{\bf y}).$$ Assuming the internal dynamics are stable, this yields
converging error dynamics, $({\bf y}_d - {\bf y})$, when ${\bf K}_p,{\bf K}_d
> 0$<elib>Slotine90</elib>.  For a position control robot, the
acceleration command of ($\ref{eq:q2cmd}$) suffices. Alternatively, a
torque command follows by substituting ($\ref{eq:q2cmd}$) and
($\ref{eq:q1cmd}$) into ($\ref{eq:active_dyn}$).</p>

<div data-type="example"><h1>End-point trajectory following with the Cart-Pole system</h1>

<p>
Consider the task of trying to track a desired kinematic trajectory
with the endpoint of pendulum in the cart-pole system.  With one
actuator and kinematic constraints, we might be hard-pressed to track
a trajectory in both the horizontal and vertical coordinates.  But we
can at least try to track a trajectory in the vertical position of the
end-effector.</p>


<p>Using the task-space PFL derivation, we have:
\begin{gather*}
y = f(\bq) = -l \cos\theta \\
\dot{y} = l \dot\theta \sin\theta
\end{gather*}
If we define a desired trajectory:
$$y^d(t) = \frac{l}{2} + \frac{l}{4} \sin(t),$$
then the task-space controller is easily implemented using the
derivation above.</p>

<todo>add results here</todo>
</div> <!-- end example-->

</section> <!-- task space pfl -->

<section data-type="sect3"><h1>Collocated and Non-Collocated PFL from Task Space derivation</h1>

<p>
The task space derivation above provides a convenient generalization
of the partial feedback linearization (PFL) <elib>Spong94a</elib>, which
emcompasses both the collocated and non-collocated results.  If we
choose ${\bf y} = \bq_2$ (collocated), then we have $${\bf J}_1 = 0, {\bf J}_2 =
{\bf I}, \dot{\bf J} = 0, \bar{\bf J} = {\bf I}, \bar{\bf J}^+ = {\bf I}.$$ From this, the command
in ($\ref{eq:q2cmd}$) reduces to $\ddot\bq_2 = v$. The torque command
is then
$$ \tau = - {\bf H}_{21} {\bf H}_{11}^{-1} ({\bf H}_{12} v + {\bf \phi}_1) +
{\bf H}_{22} v + {\bf \phi}_2,$$ and the rank condition
($\ref{eq:rank_condition}$) is always met.</p>

<p>
If we choose ${\bf y} = \bq_1$ (non-collocated), we have $${\bf J}_1 =
{\bf I}, {\bf J}_2
= 0, \dot{\bf J} = 0, \bar{{\bf J}}=-{\bf H}_{11}^{-1}{\bf H}_{12}.$$ The rank
condition ($\ref{eq:rank_condition}$) requires that
$\text{rank}({\bf H}_{12}) = l$, in which case we can write
$\bar{{\bf J}}^+=-{\bf H}_{12}^+{\bf H}_{11}$, reducing the rank condition to
precisely the ``Strong Inertial Coupling'' condition described in
<elib>Spong94a</elib>. Now the command in ($\ref{eq:q2cmd}$) reduces to
\begin{equation}
\ddot\bq_2 = -{\bf H}_{12}^+ \left [ {\bf H}_{11}{\bf v} + {\bf \phi}_1 \right] \label{eq:q2ddnonco}
\end{equation}
The torque command is found by substituting $\ddot\bq_1 = v$ and
($\ref{eq:q2ddnonco}$) into ($\ref{eq:active_dyn}$), yielding the same
results as in <elib>Spong94a</elib>.</p>

</section>
</section>
</section>


<section data-type="sect1"><h1>Swing-Up Control</h1>

<section data-type="sect2"><h1>Energy Shaping</h1>

<p>
Recall the phase portraits that we used to understand the dynamics of
the undamped, unactuated, simple pendulum ($u=b=0$).  The orbits of this phase plot were defined by
countours of constant energy.  One very special orbit, known as a
<em>homoclinic</em> orbit, is the orbit which passes through the
unstable fixed point.  In fact, visual inspection will reveal that any
state that lies on this homoclinic orbit must pass into the unstable
fixed point.  Therefore, if we seek to design a nonlinear feedback
control policy which drives the simple pendulum from any initial
condition to the unstable fixed point, a very reasonable strategy
would be to use actuation to regulate the energy of the pendulum to
place it on this homoclinic orbit, then allow the system dynamics to
carry us to the unstable fixed point.</p>

<p>
This idea turns out to be a bit more general than just for the simple
pendulum. As we will see, we can use similar concepts of `energy
shaping' to produce swing-up controllers for the acrobot and cart-pole
systems.  It's important to note that it only takes one actuator to
change the total energy of a system.</p>

<p>
Although a large variety of swing-up controllers have been proposed
for these model
systems (c.f. <elib>Fantoni02</elib>,<elib>Araki05</elib>,<elib>Xin04</elib>,<elib>Spong94</elib>,<elib>Mahindrakar05</elib>,<elib>Berkemeier99</elib>,<elib>Murray91</elib>,<elib>Lai06</elib>),
the energy shaping controllers tend to be the most natural to derive
and perhaps the most well-known.</p>

</section>

<section data-type="sect2" id="s:pend_energy_shaping"><h1>Simple Pendulum</h1>

<p>
Recall the equations of motion for the undamped simple pendulum were given
by $$ml^2 \ddot\theta + mgl\sin\theta = u.$$  The total energy of the
simple pendulum is given by $$E = \frac{1}{2} m l^2 \dot\theta^2 -
mgl\cos\theta.$$  To understand how to control the energy, observe
that \begin{align*} \dot{E} =& ml^2\dot\theta \ddot\theta + \dot\theta
  mgl\sin\theta \\ =& \dot\theta \left[ u - mgl\sin\theta \right] +
  \dot\theta mgl\sin\theta \\ =& u\dot\theta. \end{align*}
In words, adding energy to the system is simple - simply apply torque
in the same direction as $\dot\theta$.  To remove energy, simply apply
torque in the opposite direction (e.g., damping).</p>

<p>
  To drive the system to the homoclinic orbit, we must regulate the
energy of the system to a particular desired energy, $$E^d = mgl.$$ If
we define $\tilde{E} = E - E^d$, then we have $$\dot{\tilde{E}} =
\dot{E} = u\dot\theta.$$ If we apply a feedback controller of the
form $$u = -k \dot\theta \tilde{E},\quad k>0,$$ then the resulting
error dynamics are $$\dot{\tilde{E}} = - k \dot\theta^2 \tilde{E}.$$
These error dynamics imply an exponential convergence: $$\tilde{E}
\rightarrow 0,$$ except for states where $\dot\theta=0$.  The
essential property is that when $E>E^d$, we should remove energy from
the system (damping) and when $E<E^d$, we should add energy (negative
damping).  Even if the control actions are bounded, the convergence is
easily preserved.</p>

<p>
This is a nonlinear controller that will push all system trajectories
to the unstable equilibrium.  But does it make the unstable
equilibrium locally stable?  <em>No.</em>  Small perturbations may cause
the system to drive all of the way around the circle in order to once
again return to the unstable equilibrium.  For this reason, one
trajectories come into the vicinity of our swing-up controller, we
prefer to switch to our LQR balancing controller to performance to
complete the task.</p>

</section>

<section data-type="sect2"><h1>Cart-Pole</h1>

<p>
Having thought about the swing-up problem for the simple pendulum,
let's try to apply the same ideas to the cart-pole system.  The basic
idea, from <elib>Chung95</elib>, is to use collocated PFL to simplify the
dynamics, use energy shaping to regulate the pendulum to it's
homoclinic orbit, then to add a few terms to make sure that the cart
stays near the origin.  The collocated PFL (when all parameters
are set to 1) left us with:
\begin{gather}
\ddot{x} = u \\
\ddot\theta = - u c - s
%\ddot\theta = -\frac{1}{l}( u c + g s )
\end{gather}
The energy <em>of the pendulum</em> (a unit mass, unit length, simple
pendulum in unit gravity) is given by: $$E(\bx) = \frac{1}{2}
\dot\theta^2 - \cos\theta.$$
<!-- E = \frac{1}{2}ml^2\dot\theta^2 - mgl\cos\theta -->
The desired
energy, equivalent to the energy at the desired fixed-point, is
$$E^d = 1.$$
<!--  E^d = m_p g l -->
Again defining $\tilde{E}(\bx) = E(\bx) - E^d$, we now observe
that
\begin{align*}
\dot{\tilde{E}}(\bx) =& \dot{E}(\bx) = \dot\theta \ddot\theta +
\dot\theta s \\
% \dot\tilde{E} = ml^2 \dot\theta \ddot\theta + mgl s
=& \dot\theta [ -uc - s] + \dot\theta s \\
% - ml \dot\theta [ u c + g s ] + mgl s
=& - u\dot\theta \cos\theta.
% - ml u \dot\theta c
\end{align*}
Therefore, if we design a controller of the form
$$u = k \dot\theta\cos\theta \tilde{E},\quad k>0$$
the result is
$$\dot{\tilde{E}} = - k \dot\theta^2 \cos^2\theta \tilde{E}.$$  % times ml
This guarantees that $| \tilde{E} |$ is
non-increasing, but isn't quite enough to guarantee that it will go
to zero.  For example, if $\theta = \dot\theta = 0$, the system will
never move.  However, if we have that $$\int_0^t \dot\theta^2(t')
\cos^2 \theta(t') dt' \rightarrow \infty,\quad \text{as } t
\rightarrow \infty,$$ then we have $\tilde{E}(t) \rightarrow 0$. This
condition is satisfied for all but the trivial constant trajectories at fixed points.</p>

<p>
Now we return to the full system dynamics (which includes the
cart).  <elib>Chung95</elib> and <elib>Spong96</elib> use the simple pendulum
energy controller with an addition PD controller designed to regulate
the cart: $$\ddot{x}^d = k_E \dot\theta \cos\theta \tilde{E} - k_p x -
k_d \dot{x}.$$ <elib>Chung95</elib> provided a proof of convergence for this
controller with some nominal parameters.</p>

<figure>
<img width="70%" src="figures/cartpole_swingup_phase.svg"/>

<figcaption>Cart-Pole Swingup: Example phase plot of the pendulum subsystem using
  energy shaping control.  The controller drives the system to the
  homoclinic orbit, then switches to an LQR balancing controller near
  the top.</figcaption>
</figure>

</section>

<section data-type="sect2"><h1>Acrobot</h1>

<p>
Swing-up control for the acrobot can be accomplished in much the same
way.  <elib>Spong94</elib> - pump energy.  Clean and simple.  No proof.
Slightly modified version (uses arctan instead of sat) in <elib>Spong95</elib>.
Clearest presentation in <elib>Spong96</elib>.</p>

<p>
Use collocated PFL.  ($\ddot{q}_2 = \ddot{q}_2^d$).
$$E(\bx) = \frac{1}{2}\dot\bq^T{\bf H}\dot{\bq} + U(\bx).$$
$$ E_d = U(\bx^*).$$
$$\bar{u} = \dot{q}_1 \tilde{E}.$$
$$\ddot{x}^d  = - k_1 q_2 - k_2 \dot{q}_2 + k_3 \bar{u},$$</p>

<p>
Extra PD terms prevent proof of asymptotic convergence to homoclinic
orbit.  Proof of another energy-based controller in <elib>Xin04</elib>.</p>

</section>

<section data-type="sect2"><h1>Discussion</h1>

<p>
The energy shaping controller for swing-up presented here are pretty
faithful representatives from the field of nonlinear underactuated
control.  Typically these control derivations require some clever
tricks for simplifying or canceling out terms in the nonlinear
equations, then some clever Lyapunov function to prove stability.  In
these cases, PFL was used to simplify the equations, and therefore the
controller design.</p>

<p>
These controllers are important, representative, and relevant.  But
clever tricks with nonlinear equations seem to be fundamentally
limited.  Most of the rest of the material presented in this book will
emphasize more general computational approaches to formulating and
solving these and other control problems.
</p>

</section>

</section>

<section data-type="sect1"><h1>Other Model Systems</h1>

<p>
The acrobot and cart-pole systems are just two of the model systems
used heavily in underactuated control research.  Other examples include:
<ul>
  <li>Pendubot</li>
  <li>Inertia wheel pendulum</li>
  <li>Furata pendulum (horizontal rotation and vertical pendulum)</li>
  <li>Hovercraft</li>
</ul>
</p>

</section>

</section> <!-- ***************  end of acrobot/cartpole/quadrotor chapter **************   -->


<section data-type="chapter" class="chapter"><h1>Walking and Running</h1>

</section>

<!-- ***************  end of walking and running **************   -->

<section data-type="chapter" class="chapter"><h1>Manipulation</h1>

</section>

<!-- ***************  end of Manipulation **************   -->

<section data-type="chapter" class="chapter"><h1>Model Systems with Fluid Dynamics</h1>

</section>

<!-- ***************  end of fluid dynamics **************   -->

<section data-type="chapter" class="chapter"><h1>Model Systems with Stochasticity</h1>

</section>

<!-- ***************  end of stochasticity **************   -->

<!-- START PART II:  Nonlinear Planning and Control -->

<section data-type="chapter" class="chapter"><h1>Dynamic Programming</h1>

<p>In chapter 2, we spent some time thinking about the phase portrait of the
simple pendulum, and concluded with a challenge: can we design a nonlinear
controller to <em>reshape</em> the phase portrait, with a very modest amount of
actuation, so that the upright fixed point becomes globally stable?  With
unbounded torque, feedback linearization solutions (e.g., invert gravity) can
work well, but can also require an unecessarily large amount of control effort.
The energy-based swing-up control solutions presented for the acrobot and
cart-pole systems  are considerably more appealing, but required some cleverness
and might not scale to more complicated systems.  Here we investigate another
approach to the problem, using computational optimal control to synthesize a
feedback controller directly.</p>

<section data-type="sect1"><h1>Formulating control design as an optimization</h1>

<p>   In this chapter, we will introduce optimal control - a control design
process using optimization.  This approach is powerful for a number of reasons.
First and foremost, it is very general - allowing us to   specify the goal of
control equally well for fully- or under-actuated,   linear or nonlinear,
deterministic or stochastic, and continuous or   discrete systems. Second, it
permits concise descriptions of potentially very complex desired behaviours,
specifying the goal of control as an scalar objective (plus a list of
constraints).  Finally, and most importantly, optimal control is very amenable
to numerical solutions. <elib>Bertsekas00a</elib> is a fantastic reference on
this material.</p>

<p>   The fundamental idea in optimal control is to formulate the goal of
control as the <em>long-term</em> optimization of a scalar cost function.
Let's introduce the basic concepts by considering a system that is even
simpler than the simple pendulum.</p>

<div data-type="example"><h1>Optimal Control Formulations for the Double Integrator</h1>

<p>
Consider the double integrator system $$\ddot{q} = u, \quad |u| \le 1.$$   If you would like a mechanical analog of the
system (I always do), then you can think about this as a unit mass
brick moving along the x-axis on a frictionless surface, with a control
input which provides a horizontal force, $u$.
The task is to design a
control system, $u = \pi(\bx,t)$, $\bx=[q,\dot{q}]^T$ to regulate this
brick to $\bx = [0,0]^T$.
<figure>
<img width="70%" src="figures/double_integrator_brick.svg"/>
<figcaption>The double integrator as a unit-mass brick on a frictionless surface</figcaption>
</figure>
</p>

<p>
In order to formulate this control design problem using optimal
control, we must define a scalar objective which scores the
long-term performance of running each candidate control policy,
$\pi(\bx,t)$, from each initial condition, $(\bx_0,t_0)$, and a list
of constraints that must be satisfied.   For the
task of driving the double integrator to the origin, one could imagine
a number of optimal control formulations which would accomplish the
task, e.g.:
<ul>
<li> Minimum time:  $\min_\pi t_f,$ subject to $\bx(t_0) =
  \bx_0,$ $\bx(t_f) = {\bf 0}.$ </li>
<li> Quadratic cost:  $\min_\pi \int_{t_0}^{\infty} \left[ \bx^T(t)
    {\bf Q} \bx(t) \right] dt,$ ${\bf Q}\succ0$.</li>
</ul>
where the first is a constrained optimization formulation which
optimizes time, and the second accrues a penalty at every instance
according to the distance that the state is away from the origin (in a
metric space defined by the matrix ${\bf Q}$), and therefore
encourages trajectories that go more directly towards the goal,
possibly at the expense of requiring a longer time to reach the goal
(in fact it will result in an exponential approach to the goal, where
as the minimum-time formulation will arrive at the goal in finite time).
Note that both optimization problems are only
well defined if it is possible for the system to actually reach the
origin, otherwise the minimum-time problem cannot satisfy the terminal
constraint, and the integral in the quadratic cost would not converge
to a finite value as time approaches infinity (fortunately the double
integrator system is controllable, and therefore can be driven to the
goal in finite time).</p>

<p> Note that the input limits, $|u|\le1$ are also required to make this problem
well-posed; otherwise both optimizations would result in the optimal policy
using infinite control input to approach the goal infinitely fast.  Besides input limits,
another common approach to limiting the control effort is to add an additional
quadratic cost on the input (or "effort"), e.g.
$\int \left[ \bu^T(t) {\bf R} \bu(t) \right] dt,$   ${\bf R}\succ0$.
This could be added to either formulation above.  We will examine many of these formulations
in some details in the examples worked out at the end of this chapter.
</p>

</div> <!-- end example -->

<p>Optimal control has a long history in robotics.  For instance, there has been
a great deal of work on the minimum-time problem for pick-and-place robotic
manipulators, and the linear quadratic regulator (LQR) and linear quadratic
regulator with Gaussian noise (LQG) have become essential tools for any
practicing controls engineer.  With increasingly powerful computers and
algorithms, the popularity of numerical optimal control has grown at an
incredible pace over the last few years.</p>

<div data-type="example"><h1>The minimum time problem for the double integrator</h1>

<p>
For more intuition, let's do an informal derivation of the solution to
the minimum time problem for the double integrator with input constraints:
\begin{align*}
  \minimize_{\pi} \quad & t_f\\
  \subjto \quad & \bx(t_0) = \bx_0, \\
  & \bx(t_f) = {\bf 0}, \\
  & \ddot{q}(t) = u(t), \\
  & |u| \le 1.
\end{align*}
What behavior would you expect an optimal controller exhibit?</p>

<p>
Your intuition might tell you that the best thing that the brick can
do, to reach the goal in minimum time with limited control input, is
to accelerate maximally towards the goal until reaching a critical
point, then hitting the brakes in order to come to a stop exactly at
the goal.  This would be called a <em>bang-bang</em> control policy;
these are often optimal for systems with bounded input, and it is in
fact optimal for the double integrator, although we will not prove it
until we have developed more tools.  <!-- leave the proof to the pontryagin notes -->.</p>

<p>
Let's work out the details of this bang-bang policy.  First, we can
figure out the states from which, when the brakes are fully
applied, the system comes to rest precisely at the origin.  Let's
start with the case where $q(0) < 0$, and $\dot{q}(0)>0$, and "hitting
the brakes" implies that $u=-1$ .
Integrating the equations, we have \begin{gather*} \ddot{q}(t) = u =
  -1 \\\dot{q}(t) = \dot{q}(0) - t \\ q(t) = q(0) + \dot{q}(0) t -
  \frac{1}{2} t^2.  \end{gather*}
Substituting $t = \dot{q}(0) - \dot{q}$ into the solution give $\dot{q}$ reveals
that the system orbits are parabolic arcs:
\[ q = -\frac{1}{2} \dot{q}^2 + c_{-}, \] with $c_{-} = q(0) + \frac{1}{2}\dot{q}^2(0)$.
<figure>
  <img width="80%" src="figures/double_integrator_orbits.svg"/>
  <figcaption>Two solutions for the system with $u=-1$</figcaption>
</figure>
<!-- t = qdot - qdot0,
q = q0 + qdot0(qdot-qdot0) + 1/2(qdot-qdot0)^2
  = q0 + qdot0*qdot - qdot0^2 + 1/2qdot^2 - qdot*qdot0 + 1/2qdot0^2
  = 1/2qdot^2 + (q0 - 1/2 qdot0^2)
-->
Similarly, the solutions for $u=1$ are $q = \frac{1}{2} \dot{q}^2 + c_{+}$, with $c_{+}=q(0)-\frac{1}{2}\dot{q}^2(0)$.
</p>

<p> Perhaps the most important of these orbits are the ones that pass directly
through the origin (e.g., $c_{-}=0$). Following our initial logic, if the system is going
slower than this $\dot{q}$ for any $q$, then the optimal thing to do is to slam
on the accelerator ($u=-\text{sgn}(q)$).  If it's going faster than the
$\dot{q}$ that we've solved for, then still the best thing to do is to brake;
but inevitably the system will overshoot the origin and have to come back.  We
can summarize this policy with: \[ u = \begin{cases}  +1 & \text{if } (\dot{q} < 0 \text{ and } q \le \frac{1}{2} \dot{q}^2) \text{ or } (\dot{q}\ge 0 \text{ and } q < -\frac{1}{2} \dot{q}^2) \\ 0 & \text{if } q=0 \text{
and } \dot{q}=0 \\ -1 & \text{otherwise} \end{cases} \]
<!--This policy is cartooned in
Figure~\ref{fig:mintime_double_int}.  %Trajectories of the system
%executing this policy are also included - the fundamental
%characteristic is that the system is accelerated as quickly as
%possible toward the switching surface, then rides the switching
%surface in to the origin. -->
<figure>
<img width="80%" src="figures/double_integrator_mintime_policy.svg"/>
<figcaption>Candidate optimal (bang-bang) policy for the minimum-time
  double integrator problem.</figcaption>
</figure>
and illustrate some of the optimal solution trajectories:
<figure>
<img width="80%" src="figures/double_integrator_mintime_orbits.svg"/>
<figcaption>Solution trajectories of system using the optimal policy</figcaption>
</figure>

And for completeness, we can compute the optimal time to the goal by solving for
the amount of time required to reach the switching surface plus the amount of
time spent moving along the switching surface to the goal.  With a little
algebra, you will find that the time to goal, $T(\bx)$, is given by
\[ T(\bx) = \begin{cases} 2\sqrt{\frac{1}{2}\dot{q}^2-q} - \dot{q} & \text{for } u=+1 \text{ regime}, \\
0 & \text{for } u=0, \\ \dot{q} + 2\sqrt{\frac{1}{2}\dot{q}^2+q} & \text{for } u=-1, \end{cases} \]<!--
call t_m the time to the surface, then the
time on switching surface = |qdot(t_m)|

for u=1
q0,qdot0 => qm,qdotm with qm = -1/2 qdotm^2
-1/2 qdotm^2 = 1/2 qdotm^2 + c+
qdotm^2 = - c+  , qdotm = sqrt(-c+)
T = (qdotm-qdot0)+qdotm = 2*sqrt(-c+) - qdot0
for u=-1, qdotm^2 = c- , qdotm = -sqrt(c-)
T = (qdot0-qdotm)-qdotm = qdot0 + 2*sqrt(c-)
-->
plotted here:
<figure>
<img width="80%" src="figures/double_integrator_mintime_cost_to_go.svg"/>
<figcaption>Time to the origin using the bang-bang policy</figcaption>
</figure>
Notice that the function is continuous (though not smooth), even though
the policy is discontinous.
</p>

</div> <!-- end of example -->

<section data-type="sect2"><h1>Additive cost</h1>

<p>
As we begin to develop theoretical and algorithmic tools for optimal control,
we will see that some formulations are much easier to deal with than others.
One important example is the dramatic simplification that can come from
formulating objective functions using <em>additive
  cost</em>, because they often yield recursive solutions.  In the additive cost formulation, the long-term "score"
for a trajectory can be written as $$\int_0^T g(x(t),u(t)) dt,$$ where
$g()$ is the instantaneous cost, and $T$ can be either a finite
real number or $\infty$.  We will call a problem specification with a finite $T$ a
"finite-horizon" problem, and $T=\infty$ an "infinite-horizon"
problem.  Problems and solutions for infinite-horizon problems tend to be more elegant, but
care is required to make sure that the integral converges for the
optimal controller (typically
by having a goal state/action pair that allows the robot to accrue zero-cost).</p>

<p> At first glance, our minimum-time problem formulation for the double
integrator does not look like an additive cost problem.  However, we can write
in as an additive cost problem using an infinite horizon and the instantaneous
cost  $$g(x,u) = \begin{cases} 0 & \text{if } x=0, \\ 1 &
\text{otherwise.} \end{cases}$$</p>

<p>
We will examine a number of approaches to solving optimal control
problems throughout the next few chapters.  For the remainder of this chapter, we will
focus on additive cost problems and their solution via <em>dynamic programming</em>.</p>

</section> <!-- end of additive cost -->

</section> <!-- control design as an optimization -->

<section data-type="sect1"><h1>Optimal Control as Graph Search</h1>

<p> For systems with continuous states and continuous actions, dynamic
programming is a set of theoretical ideas surrounding additive cost optimal
control problems. For systems with a finite, discrete set of states and a
finite, discrete set of actions, dynamic programming also represents a set of very
efficient numerical <em>algorithm</em> which can compute optimal feedback
controllers. Many of you will have learned it before as a tool for graph search.
</p>

<p>Imagine you have a directed graph with states (or nodes) $\{s_1,s_2,...\} \in
S$ and "actions" associated with edges labeled as $\{a_1,a_2,...\} \in A$, as in
the following trivial example: <figure><img width="70%"
src="figures/graph_search.svg"/><figcaption>A simple directed
graph.</figcaption></figure> Let us also assume that each edge has an associate
weight or cost, using $g(s,a)$ to denote the cost of being in state $s$ and
taking action $a$. Furthemore we will denote the transition "dynamics" using \[ s[n+1] =
f(s[n],a[n]). \] For instance, in the graph above, $f(s_1,a_1) = s_2$.</p>

<p>There are many algorithms for finding (or approximating) the optimal path
from a start to a goal on directed graphs.  In dynamic programming, the key
insight is that we can find the shortest path from every node by solving
recursively for the optimal <em>cost-to-go</em> from every node to the goal. One
such algorithm starts by initializing an estimate $\hat{J}^*=0$ for all $s_i$,
then proceeds with an iterative algorithm which sets \begin{equation}
\hat{J}^*(s_i) = \min_{a \in A} \left[ g(s_i,a) +
\hat{J}^*\left({f(s_i,a)}\right) \right]. \label{eq:value_update} \end{equation}
In software, $\hat{J}^*$ can be represented as a vector with dimension equal to
the number of discrete states.  This algorithm, appropriately known as <em>value
iteration</em>, is guaranteed to converge to the optimal cost-to-go, $\hat{J}^*
\rightarrow J^*$ <elib>Bertsekas96</elib>, and in practice does so rapidly.
Typically the update is done in <em>batch</em> -- e.g. the estimate is updated
for all $i$ at once -- but the <em>asynchronous</em> version where states are
updated one at a time is also known to converge, so long as every state is
eventually updated infinitely often.  Assuming the graph has a goal state with a
zero-cost self-transition, then this cost-to-go function represents the weighted
shortest distance to the goal. </p>

<p> Value iteration is an amazingly simple algorithm, but it accomplishes
something quite amazing: it efficiently computes the long-term cost of an
optimal policy from <i>every</i> state by iteratively evaluating the one-step
cost.  If we know the optimal cost-to-go, then it's easy to extract the optimal
policy, $a = \pi^*(s)$: \begin{equation} \pi^*(s_i) = \argmin_a \left[ g(s_i,a) +
J^*\left( f(s_i,a) \right) \right]. \label{eq:policy_update} \end{equation}
It's a simple algorithm, but playing with an example can help our intuition.</p>

<div data-type="example"><h1>Grid World</h1>

<p>Imagine a robot living in a grid (finite state) world.  Wants to get to the goal
location.  Possibly has to negotiate cells with obstacles.  Actions
are to move up, down, left, right, or do nothing.  <elib>Sutton98</elib></p>

<figure>
  <a href="figures/gridworld_mintime.swf">
    <img width="80%" src="figures/gridworld_mintime.svg" />
  </a>
  <figcaption>The one-step cost for the grid-world minimum-time problem. The goal state has a cost of zero, the obstacles have a cost of 10, and every other state has a cost of 1. <em>Click the image to watch the value iteration algorithm in action.</em></figcaption>
</figure>

<todo>insert value iteration animation for min time and quadratic cost</todo>

<p>You can experiment with the grid world example in <drake></drake> using
<pre><code class="matlab">
cd(fullfile(getDrakePath,'examples'));
GridWorld.runValueIteration();
</code></pre>
I recommend trying to edit the cost function and obstacles.</p>

</div> <!-- end grid world -->

Graph approximation of a continuous state space.

Stochastic shortest path.

<div data-type="example"><h1>Dynamic Programming for the Double Integrator</h1>

<p>You can run value iteration for the double integrator (using barycentric interpolation to interpolate between nodes) in <drake></drake> using:
<pre><code class="matlab">
cd(fullfile(getDrakePath,'examples'));
DoubleIntegrator.runValueIteration();
</code></pre>
Again, you can easily try different cost functions by editing the code yourself.</p>

</div>

<p>
Let's take a minute to appreciate how amazing this is.  Our solution to finding the
optimal controller for the double integrator wasn't all that hard, but it required
some mechanical intuition and solutions to differential equations.  The resulting
policy was non-trivial -- bang-bang control with a parabolic switching surface.
The value iteration algorithm doesn't use any of this directly -- it's a simple
algorithm for graph search.  But remarkably, it can generate effectively the same
policy with just a few moments of computation.</p>

<p>It's important to note that there <em>are</em> some differences between the computed
  policy and the optimal policy that we derived, due to discretization errors.  We
  will ask you to explore these in the problems.</p>

<p>The real value of this numerical solution, however, is unlike our analytical
solution for the double integrator, we can apply this same algorithm to any number
of dynamical systems virtually without modification.  Let's apply it now to the simple
pendulum, which was intractable analytically.</p>

<div data-type="example"><h1>Dynamic Programming for the Simple Pendulum</h1>

<p>You can run value iteration for the simple pendulum (using barycentric interpolation to interpolate between nodes) in <drake></drake> using:
<pre><code class="matlab">
cd(fullfile(getDrakePath,'examples','Pendulum'));
runValueIteration();
</code></pre>
Again, you can easily try different cost functions by editing the code yourself.</p>

</div>

</section> <!-- end of graph search -->

<section data-type="sect1"><h1>Continuous Dynamic Programming</h1>

<p>
I find the graph search algorithm extremely satisfying as a first step, but also
become quickly frustrated by the limitations of the discretization required to use it.
In many cases, we can do better; coming up with algorithms which work more natively
on continuous dynamical systems.  We'll explore those extensions in this section.</p>

<section data-type="sect2"><h1>The Hamilton-Jacobi-Bellman Equation</h1>

<p>
It's important to note that the value iteration equations, equations (\ref{eq:value_update}) and (\ref{eq:policy_update}),
are more than just an algorithm.  They are also sufficient conditions for optimality:
if we can produce a $J^*$ and $\pi^*$ which satisfy these equations, then $\pi^*$ must
be an optimal controller.  There are an analagous set of conditions for the continuous
systems.  For a system $\dot{\bx} = f(\bx,\bu)$ and an inifinite-horizon
additive cost $\int_0^\infty g(\bx,\bu)dt$, we have:
\begin{gather} 0 = \min_\bu \left[ g(\bx,\bu) + \pd{J^*}{\bx}f(\bx,\bu) \right], \label{eq:HJB} \\ \pi^*(\bx) = \argmin_\bu \left[ g(\bx,\bu) + \pd{J^*}{\bx}f(\bx,\bu) \right]. \end{gather}
Equation \ref{eq:HJB} is known as the <em>Hamilton-Jacobi-Bellman</em> (HJB) equation.  <elib>Bertsekas05</elib> <!-- chapter 3 --> gives an informal derivation
of these equations as the limit of a discrete-time approximation of the dynamics, and also gives
the following sufficiency theorem:
<div data-type="theorem"><h1>HJB Sufficiency Theorem</h1>
<p>The following statement is adapted from Proposition 3.2.1 of <elib>Bertsekas05</elib>.  Suppose $J(\bx)$ is a solution
  to the HJB equation; that is $J$ is continuously differentiable in $\bx$ and is such that
  \[ 0 = \min_{u\in U} \left[ g(\bx,\bu) + \pd{J}{\bx}f(\bx,\bu) \right],\quad \text{for all } \bx. \]
   Suppose also that $\pi^*(\bx)$ attains the minimum in the equation for all $\bx$.  Further
   assume that the differential equation described by $f$ has a unique solution starting
   from any state $\bx$, and that the control input trajectory, $\bu^*(t)$, obtained by evaluating
   $\pi^*$ along any solution is piecewise continuous as a function of $t$.  Then $J$ is equal
   to the optimal cost-to-go function, $J(\bx)=J^*(\bx)$ for all $\bx$, and the control trajectories
   $\bu^*(t)$ are optimal.
</div>
</p>

<p>As a tool for verifying optimality, the HJB equations are actually surprisingly easy to work with:
  we can verify optimality for an infinite-horizon objective without doing any integration; we simply
  have to check a derivative condition on the optimal cost-to-go function $J^*$.  Let's see this
  play out on the double integrator example.</p>

<div data-type="example"><h1>HJB for the Double Integrator</h1>

<p>Consider the problem of regulating the double integrator (this time without
input limits) to the origin   using a quadratic cost: $$ g(\bx,\bu) = q^2 +
\dot{q}^2 + u^2. $$  I claim   (without derivation) that the optimal controller
for this objective is $$\pi(\bx) = -q - \sqrt{3}\dot{q}.$$   To convince you
that this is indeed optimal, I have produced   the following cost-to-go
function: $$J(\bx) = \sqrt{3} q^2 + 2 q \dot{q} + \sqrt{3} \dot{q}.$$</p>

<p>Taking \begin{gather*} \pd{J}{q} = 2\sqrt{3} q + 2\dot{q}, \qquad
\pd{J}{\dot{q}} = 2q + 2\sqrt{3}\dot{q}, \end{gather*}   we can write
\begin{align*} g(\bx,\bu) + \pd{J}{\bx}f(\bx,\bu) &= q^2 + \dot{q}^2 + u^2 +
(2\sqrt{3} q + 2\dot{q}) \dot{q} + (2q + 2\sqrt{3}\dot{q}) u \end{align*}   This
is a positive quadratic function in $u$, so we can find the minimum with respect
to $u$ by finding where the gradient with respect to $u$ evaluates to zero.   \[
\pd{}{u} \left[ g(\bx,\bu) + \pd{J}{\bx} f(\bx,\bu) \right] = 2u + 2q +
2\sqrt{3}\dot{q}. \]  Setting this equal to $0$ and   solving for $u$ yields:
$$u^* = -q - \sqrt{3} \dot{q}.$$  Substituting $u^*$ back into the HJB reveals
that   the right side does in fact simplify to zero.  I hope you are convinced!
</div>

<p>Note that evaluating the HJB for the time-to-go of the minimum-time
  problem for the double integrator will also reveal that the
  HJB is satisfied wherever that gradient is well-defined.  This is
  certainly mounting evidence in support of our bang-bang policy being
  optimal, but since $\pd{J}{\bx}$ is not defined everywhere, it does
  not actually satisfy the requirements of the sufficiency theorem as stated above.</p>

</section> <!-- end HJB -->

<section data-type="sect2"><h1>Solving for the minimizing control</h1>

<p>We still face a few barriers to actually using the HJB in an algorithm.  The first barrier
is the minimization over $u$.  When the action set was discrete, as in the graph search
version, we could evaluate the one-step cost plus cost-to-go for every possible action,
and then simply take the best.  For continuous action spaces, in general we cannot rely on the
strategy of evaluating a finite number of possible $\bu$'s to find the minimizer.</p>

<p>All is not lost.  In the quadratic cost double integrator example above, we
were able   to solve explicitly for the minimizing $\bu$ in terms of the
cost-to-go.  It turns out that   this strategy will actually work for a number
of the problems we're interested in, even   when the system (which we are given)
or cost function (which we are free to pick, but which   should be expressive)
gets more complicated.</p>

<p>Recall that I've already tried to convinced you that a majority of the
systems of interest   are <em>control affine</em>, e.g. I can write \[
f(\bx,\bu) = f_1(\bx) + f_2(\bx)\bu. \]   We can make another dramatic
simplification by restrict ourselves to instantaneous cost functions of the form \[
g(\bx,\bu) = g_1(\bx) + \frac{1}{2} \bu^T {\bf R} \bu, \qquad {\bf R}={\bf R}^T
\succ 0. \] In my view, this is not very restrictive - many of the cost
functions that I find myself choosing to write down can be expressed in this
form.  Given these assumptions, we can write the HJB as \[ 0 = \min_{\bu} \left[
g_1(\bx) + \frac{1}{2} \bu^T {\bf R} \bu + \pd{J}{\bx} \left[ f_1(\bx) +
f_2(\bx)\bu \right]\right]. \]  Since this is a quadratic function in $\bu$, if
the system does not have any constraints on $\bu$, then we can solve in closed-form
for the minimizing $\bu$ by taking the gradient of the right-hand size:
\[ \pd{}{\bu} = 2\bu^T {\bf R} + \pd{J}{\bx} f_2(\bx) = 0, \]
and setting it equal to zero to obtain \[ \bu^* = -{\bf R}^{-1}f_2^T(\bx) \pd{J}{\bx}^T.\]
If there are linear constraints on the input, such as torque limits, then more generally
this could be solved (at any particular $\bx$) as a quadratic program.</p>

<p> What happens in the case where our system is not control affine or if we
really   do need to specify an instantaneous cost function on $\bu$ that is not
simply quadratic?   If the goal is to produce an iterative algorithm, like value
iteration, then one   common approach is to make a quadratic approximation in
$\bu$ of the HJB, and updating   that approximation on every iteration of the
algorithm.  This broad approach is often referred to as <em>differential
dynamic programming</em> (c.f. <elib>Jacobson70</elib>).   </p>

</section> <!-- end solve for u -->

<section data-type="sect2"><h1>Representing the cost-to-go with function approximation</h1>

<p> The other major barrier to using the HJB in a value iteration algorithm is
that  the estimated optimal cost-to-go function, $\hat{J}^*$, must somehow be
represented with a finite set of numbers, but we don't yet know anything about
the potential form it must take. In fact, knowing the time-to-goal solution for
minimum-time problem with the double integrator, we see that this function
might need to be non-smooth for even very simple dynamics and objectives.</p>

<p>If we approximate $J^*$ with a finitely-parameterized function
$\hat{J}_\balpha^*$, with parameter vector $\balpha$, then   this immediately raises
many important questions:
<ul>
  <li>What if the true cost-to-go function does not live in
the prescribed function class; e.g., there does not exist an $\balpha$ which
satisfies the sufficiency conditions for all $\bx$?  (This seems very likely to
be the case.)</li>
<li>What update should we apply in the iterative algorithm?</li>
<li>What can
we say about it's convergence?</li>
</ul>
In general, many of these questions will have to go unanswered.  But there are some answers available
for the special case of <em>linear function approximators</em> (see Appendix B for a brief background on function
approximation).  A linear function approximator takes the form: \[ \hat{J}^*_\balpha(\bx) = \sum_i \alpha_i \psi_i(\bx) = \balpha^T {\bf\psi}(\bx), \]
where ${\bf \psi}(\bx)$ is a vector of potentially nonlinear features.  Common examples of features include
polynomials, radial basis functions, or the barycentric interpolator used above.  The distinguishing
feature of a linear function approximator is the ability to exactly solve for $\balpha$ in order
to represent a desired function optimally, in a least-squares sense.  For instance, if we
have a desired set of $N$ input-output points, $(\bx_n,J^d_n)$, then we easily can write
down the closed form solution to \[ \minimize_\balpha \sum_n \left(\hat{J}_\balpha(\bx_n) - J^d_n \right)^2. \]</p>

<todo> add quote from neuro-dynamic programming preface </todo>

<todo> add citations for convergence results </todo>

</section> <!-- end function approx -->

<todo> try just running snopt on quartic objective </todo>

<section data-type="sect2"><h1>A continuous policy iteration algorithm</h1>

<todo> simple (e.g. one-d example) </todo>

</section> <!-- end policy iteration -->


<section data-type="sect2"><h1>How far can we take this?  (Performance and Scaling)</h1>

<todo>
Errors in bang-bang for double integrator due to discretization.

Limited in number of dimensions.  Function approximation, but lacks convergence results.
</todo>

</section> <!-- end scaling -->

</section> <!-- end continuous DP -->

<section data-type="sect1"><h1>Extensions</h1>

<todo> add section on extensions.  discounting.  finite-horizon / time-varying dynamics or cost.</todo>

</section> <!-- end of extensions -->

</section> <!-- ***************  end of dynamic programming **************   -->


<section data-type="chapter" class="chapter"><h1>Linear Quadratic Regulators</h1>
<todo> consider renaming this to LQR, or linear optimal control?</todo>

<todo> add LQR derivation here</todo>

</section>

<!-- ***************  end of analytical optimal control **************   -->

<section data-type="chapter" class="chapter"><h1>Lyapunov Analysis</h1>

<p>Optimal control provides a powerful framework for formulating control problems
using the language of optimization.  But solving optimal control problems for
nonlinear systems is hard!  In many cases, we don't really care about finding
the <em>optimal</em> controller, but would be satisfied with any controller that
is guaranteed to accomplish the specified task.  In many cases, we still
formulate these problems using computational tools from optimization, and in
this chapter we'll learn about tools that can provide guaranteed control
solutions for systems that are beyond the complexity for which we can find
the optimal feedback.</p>

<p>There are many excellent books on Lyapunov analysis; for instance
  <elib>Slotine90</elib> is an excellent and very readable reference and <elib>Khalil01</elib>
can provide a rigorous treatment.  In this chapter I will summarize (without proof) some of the key theorems
from Lyapunov analysis, but then will also introduce a number of numerical
algorithms... many of which are new enough that they have
not yet appeared in any mainstream textbooks.</p>


<section data-type="sect1"><h1>Lyapunov Functions</h1>

<p>Let's start
with our favorite simple example. </p>

<div data-type="example"><h1>Stability of the Damped Pendulum</h1>

<p>
<center><img width="25%" src="figures/simple_pend.svg"/></center></p>

<p>Recall that the equations of motion of the
damped simple pendulum are given by \[ ml^2 \ddot{\theta} + mgl\sin\theta = -b\dot{\theta}, \]
which I've written with the damping on the right-hand side to remind us that
it is an external torque that we've modeled.</p>

<p>These equations represent a simple
second-order differential equation; in chapter 2 we discussed at some length
what was known about the solutions to this differential equation--even without
damping the equation for $\theta(t)$ as a function of $\theta(0)$, $\dot{theta}(0$
involves elliptic integrals of the first kind, and with damping we don't even
have that.  Since we couldn't provide a solution analytically, in chapter 2
we resorted to a graphical analysis, and confirmed the intuition that there
are fixed points in the system (at $\theta = k\pi$ for every integer $k$) and that
the fixed points at $\theta = 2\pi k$ are asympotitically stable with a large basin of
attraction.  The graphical analysis gave us this intuition, but can we actually
prove this stability property?  In a way that might also work for much more
complicated systems?</p>

<p>One route forward was from looking at the total system energy (kinetic + potential),
  which we can write down: \[ E(\theta,\dot{\theta}) = ml^2\dot{\theta}^2 - mgl \cos\theta. \]
Recall that the contours of this energy function are the orbits of the undamped
pendulum.  <center><img width="50%" src="figures/pend_undamped_phase.svg"/></center>
A natural route to proving the stability of the downward fixed points is by arguing
that energy decreases for the damped pendulum (with $b>0$) and so the system will
eventually come to rest at the minimum energy, $E = -mgl\cos\theta$, which
happens at $\theta=2\pi k$.  Let's make that argument precise.
</p>

<p>Evaluating the time derivative of the energy reveals \[ \frac{d}{dt} E = - b\dot\theta^2 \le 0. \]
This is sufficient to demonstrate that the energy will never increase, but it
doesn't actually prove that the energy will converge to the minimum.  To take
that extra step, we must observe that for most of the states with $\dot{E} = 0$, which
for $b>0$ is the manifold where $\dot\theta=0$, the system can't actually remain
in that state, but will rather pass through and enter another region where
it continues to lose energy.  In fact, the fixed points are the only subset of this
$\dot{E}=0$ manifold where the system can stay on the manifold.  Therefore, unless
the system is at a fixed point, it will in fact dissipate energy.  And therefore
we can conclude that as $t\rightarrow \infty$, the system will indeed come to rest in a fixed point with an energy
less than or equal to the initial energy in the system, $E(0)$.</p>

</div>

<p>This is an important example.  It demonstrated that we could use a relatively simple
function, the system energy, to describe something about the long-term dynamics
of the pendulum even though the actual trajectories of the system are (analytically)
very complex.  It also demonstrated one of the subtleties of using an energy-like function
that is non-increasing (instead of strictly decreasing) to prove asympototic stability.</p>

<p>Lyapunov functions generalize this notion of an energy function to more general
  systems, which might not be stable in the sense of some mechanical energy.  If
  I can find any positive function, call it $V(\bx)$, that gets smaller over
  time as the system evolves, then I can potentially use $V$ to make a statement
  about the long-term behavior of the system.  $V$ is called a <em>Lyapunov function</em>.
  Let's give the first theorem.

<div data-type="theorem"><h1>Lyapunov analysis for global asymptotic stability</h1>

<p>Given a system $\dot{\bx} = f(\bx)$, with $f$ continuous, if I can produce a scalar,
  continuously-differentiable function
$V(\bx)$, such that \begin{gather*} V(\bx) > 0, \forall \bx \ne 0 \quad V(0) = 0 \\
\dot{V}(\bx) = \pd{V}{\bx} f(\bx) < 0, \forall \bx \ne 0 \quad \dot{V}(0) = 0, \text{ and} \\
V(\bx) \rightarrow \infty \text{ whenever } ||x||\rightarrow \infty,\end{gather*}
then the origin $(\bx = 0)$ is globally asymptotically stable (G.A.S.).</p>

</div>

<p>Note that for the sequel we will use the notation $V \succ 0$ to denote a
<em>positive-definite function</em>, meaning that $V(0)=0$ and $V(\bx)>0$ for
all $\bx\ne0$ (and $V \prec 0$ for negative-definite functions).</p>

<p>This is perhaps the simplest of the Lyapunov theorems, since $\dot{V}$ is strictly
decreasing everywhere except at the origin.  The last condition, on the behavior
as $||\bx|| \rightarrow \infty$ is known as "radially unbounded",
and is required to make sure that trajectories cannot diverge to infinity even
as $V$ decreases; it is only required for global stability analysis.</p>

<p>
Notice that the system analyzed above, $\dot{\bx}=f(\bx)$, did not have any control inputs.
Therefore, Lyapunov analysis is used to study either the passive dynamics of a system
or the dynamics of a closed-loop system (system + control in feedback).  We will see
generalizations of the Lyapunov functions to input-output systems later in the text.
</p>

<p> At this point, you might be wondering if there is any relationship between
Lyapunov functions and the cost-to-go functions that we discussed in the context
of dynamic programming.  After all, the cost-to-go functions also captured a
great deal about the long-term dynamics of the system in a scalar function.  We
can see the connection if we re-examine the HJB equation \[ 0 = \min_\bu \left[
g(\bx,\bu) + \pd{J}{\bx}f(\bx,\bu). \right] \]Let's imagine that we can solve for
the optimizing $\bu^*(\bx)$, then  we are left with $ 0 = g(\bx,\bu^*) +
\pd{J}{\bx}f(\bx,\bu^*) $ or simply \[ \dot{J}  = -g(\bx,\bu^*). \]  In other
words, in optimal control we must find a cost-to-go function which matches this
gradient for every $\bx$; that's very difficult and involves solving
a potentially high-dimensional partial differential equation.  By contrast, Lyapunov analysis
is asking for much less - any function which is going downhill (at any rate) for
all states.  This can be much easier!  Also note that if we do manage
to find the optimal cost-to-go, $J^*(\bx)$, then it can also serve as a
Lyapunov function so long as $\forall \bx, g(\bx,\bu^*(\bx))>0$</p>


<section data-type="sect2"><h1>Lyapunov analysis for linear systems</h1>

<p>Let's take a moment to see how things play out for linear systems.</p>

<div data-type="theorem"><h1>Lyapunov analysis for stable linear systems</h1>

<p>Imagine you
have a linear system, $\dot\bx = {\bf A}\bx$, and can find a Lyapunov function
$$V(\bx) = \bx^T {\bf P} \bx, \quad {\bf P} = {\bf P^T} \succ 0,$$ which also
satisfies
$$\dot{V}(\bx) = \bx^T {\bf PA} \bx + \bx^T {\bf A}^T {\bf P}\bx \prec 0.$$
Then the original is globally asymptotically stable.</p>
</div>

<p>Note that the radially-unbounded condition is satisfied by ${\bf P} \succ 0$,
and that the derivative condition is equivalent to the matrix condition
$${\bf PA} +  {\bf A}^T {\bf P} \prec 0.$$</p>

<p>For stable linear systems the existence of a quadratic Lyapunov function
is actually a necessary (as well as sufficient) condition. Furthermore,
a Lyapunov function can be always be found by finding the positive-definite
solution to the matrix Lyapunov equation
$${\bf PA} + {\bf A}^T{\bf P} = - {\bf Q},$$ for any ${\bf Q}={\bf Q}^T\succ 0$.</p>

<todo> add an example here. double integrator? re-analyze the LQR output? </todo>

<p>This is a very powerful result - for nonlinear systems it can be potentially
  difficult to find a Lyapunov function, but for linear systems it is straight-forward.
  In fact, this results is often used to propose candidates for non-linear systems,
  e.g., by linearizing the equations and solving a local linear Lyapunov function
  or by proposing nonlinear Lyapunov functions which match their linear counterparts
  in the vicinity of a fixed point.</p>

</section> <!-- lyapunov for linear -->

<section data-type="sect2"><h1>Lyapunov analysis for local stability</h1>

<p>Recall that we defined three separate notions for stability of a fixed-point
  of a nonlinear system: stability i.s.L., asymptotic stability, and exponential
  stability.  We can use Lyapunov functions to demonstrate each of these, in turn.</p>

<div data-type="theorem"><h1>Local stability in the sense of Lyapunov
(i.s.L.)</h1>   <p>     Given a system $\dot{\bx} = f(\bx)$ with $f$ continuous
and a fixed point $\bx^*$.     If we can produce a scalar function $V(\bx)$ with
continuous derivatives for      which in some ball, ${\cal B}$, around $\bx^*$ we
have $V(\bx-\bx^*) \succ 0$ and $\dot{V}(\bx-\bx^*) \preceq 0$, then the fixed
point $\bx^*$ is locally stable i.s.L. (e.g. for every $\epsilon>0$ there exists
a $\delta>0$  such that $||\bx(0)-\bx^*|| < \delta$ implies that $\forall t>0,
||\bx(t) - \bx^*|| < \epsilon$.)</p>
</div>

<todo> add a picture of the delta and epsilon balls? </todo>

<p>Note that in the statement above, we required only that $\dot{V}\preceq 0$.
The intuition here is pretty clean--given this negative semi-definiteness,   we
know that $V$ will be non-increasing.  Therefore, in order to stay in the
$\epsilon$-ball we have to start with a value of $V$ such that an entire
<em>sub-level set</em>, $\{\bx | V(\bx)\le\rho\}$ with $\rho$ a positive constant,
must be contained within the $\epsilon$-ball.  We can accomplish this by choosing
a $\delta$-ball which is completley contained within that sub-level set.</p>

<p>You should also note that when the stability argument is confined to a statement
  about local stability (or stability over a bounded region), then the requirement
  that the function be radially unbounded also disappears, as we no longer have
  to worry about the behavior at infinity.</p>

<p>I suspect that you are not surprised to see that we can demonstrate stability
  <em>in the sense of Lyapunov</em> using a Lyapunov function. But it turns out that
  we can also use Lyapunov functions to demonstrate the other forms of stability, too.</p>

<div data-type="theorem"><h1>Local asymptotic stability</h1>   <p>     Given a system $\dot{\bx} = f(\bx)$ with $f$ continuous
and a fixed point $\bx^*$.     If we can produce a scalar function $V(\bx)$ with
continuous derivatives for      which in some ball, ${\cal B}$, around $\bx^*$ we
have $V(\bx-\bx^*) \succ 0$ and $\dot{V}(\bx-\bx^*) \prec 0$, then the fixed
point $\bx^*$ is locally asymptotically stable.</p>
</div>

<p>Note that the only change compared to the argument for stability i.s.L. was the
  stronger requirement here that $\dot{V}$ had to be negative definite (instead of just
  negative semi-definite).</p>

<div data-type="theorem"><h1>Local exponential stability</h1>   <p>     Given a system $\dot{\bx} = f(\bx)$ with $f$ continuous
and a fixed point $\bx^*$.     If we can produce a scalar function $V(\bx)$ with
continuous derivatives for      which in some ball, ${\cal B}$, around $\bx^*$ we
have $V(\bx-\bx^*) \succ 0$ and $\dot{V}(\bx-\bx^*) \preceq -\epsilon V(\bx-\bx^*)$, then the fixed
point $\bx^*$ is locally exponentially stable.</p>
</div>

<p> The exponential condition is implied by the fact that $\forall t>0, V(\bx(t)) \le V(\bx(0)) e^{-\epsilon t}$.</p>

<p>There are many variations on these arguments that can be applied for local stability,
  stability over a bounded region, or global stability. </p>

</section> <!-- local stability -->

<section data-type="sect2"><h1>LaSalle's Invariance Principle</h1>

<p>Perhaps you noticed the disconnect between the statements above and the argument
  that we made for the stability of the pendulum.  In the pendulum example,
  using the mechanical energy resulted in a Lyapunov function that was only
  negative semi-definite, but we eventually argued that the fixed points were
  asymptotically stable.  That took a little extra work, involving an argument
  about the fact that the fixed points were the only place that the system could
  stay with $\dot{V}=0$; every other state with $\dot{V}=0$ was only transient.
  We can formalize this idea for the more general Lyapunov function statements--it is
  known as LaSalle's Theorem.</p>

<div data-type="theorem"><h1>LaSalle's Theorem</h1>

  <p>Given a system $\dot{\bx} =
f(\bx)$ with $f$ continuous. If we can produce a scalar function $V(\bx)$ with
continuous derivatives for which we have
$V(\bx) \succ 0$, $\dot{V}(\bx) \preceq 0$, and $V(\bx)\rightarrow \infty$
as $||\bx||\rightarrow \infty$, then $\bx$ will converge to the largest invariant
set where $\dot{V}(\bx) = 0$.</p>
</div>

<p>To be clear, an <em>invariant set</em>, ${\cal G}$, of the dynamical systemis a set
for which $\bx(0)\in{\cal G} \Rightarrow \forall t>0, \bx(t) \in {\cal G}$.  In other words,
once you enter the set you never leave.  The "largest invariant set" need not be connected;
in fact for the pendulum example each fixed point is an invariant set, so the largest
invariant set is the <em>union</em> of all the fixed points of the system.</p>

</section> <!-- lasalle -->

<section data-type="sect2"><h1>Lyapunov functions for estimating regions of attraction</h1>

<p>There is another very important connection between Lypaunov functions and the
concept of an invariant set: <em>any sub-level set of a Lyapunov function is
also an invariant set</em>.

<div data-type="theorem"><h1>Lyapunov invariant set and region of attraction theorem</h1>

<p>Given a system $\dot{\bx} = f(\bx)$ with $f$ continuous, if we can find a
scalar function $V(\bx) \succ 0$ and a sub-level set $${\cal G}: \{ \bx | V(\bx)
\le \rho \}$$ on which $$\forall \bx \in {\cal G}, \dot{V}(\bx) \preceq 0,$$ then
${\cal G}$ is an invariant set.  By LaSalle, $\bx$ will converge to the
largest invariant subset of ${\cal G}$ on which $\dot{V}=0$.<p>

<p>Furthermore, if $\dot{V}(\bx) \prec 0$ in ${\cal G}$, then the origin is locally
  asymptotically stable and the set ${\cal G}$ is inside the region of attraction
  of this fixed point.  Alternatively, if $\dot{V}(\bx) \preceq 0$ in ${\cal G}$
  and $\bx = 0$ is the only invariant subset of ${\cal G}$ where $\dot{V}=0$, then
  the origin is asympotically stable and the set ${\cal G}$ is inside the region of
  attraction of this fixed point.
</p>

</div>

</section>  <!-- end ROA -->

</section>  <!-- end Lyapunov functions -->


</section>

<!-- ***************  end of verification **************   -->

<section data-type="chapter" class="chapter"><h1>Trajectory Optimization</h1>

<p>So far I've argued that optimal control is a powerful framework for specifying
  complex behaviors with simple objective functions, letting the dynamics and
  constraints on the system shape the resulting feedback controller (and vice versa!).
  But the computational tools that we've provided so far have been limited in
  some important ways.  The numerical approaches to dynamic programming which
  involve putting a mesh over the state space do not scale well to systems with
  state dimension more than four or five.  Linearization around a nominal operating
  point (or trajectory) allowed us to solve for locally optimal control policies
  (e.g. using LQR) for even very high-dimensional systems, but the effectiveness
  of the resulting controllers is limited to the region of state space where the linearization
  is a good approximation of the nonlinear dynamics.  The computational
  tools for Lyapunov analysis from the last chapter can provide, among other things,
  an effective way to compute estimates of those regions.  But we have not
  yet provided any real computational tools for approximate optimal control
  that work for high-dimensional systems beyond the linearization around a goal.
  That is precisely the goal for this chapter.</p>

<p>The big change that is going to allow us to scale to high-dimensional systems
  is that we are going to give up the goal of solving for the optimal feedback
  controller for the entire state space, and instead attempt to find an optimal
  control solution that is valid from only a single initial condition.  Instead
  of representing this as a feedback control function, we can represent this
  solution as a <em>trajectory</em>, $\bx(\cdot), \bu(\cdot)$, typically
  defined over a finite interval.  In our graph-search dynamic programming algorithms, we discretized the dynamics
    of the system on a mesh spread across the state space.  This does not scale
    to high-dimensional systems, and it was difficult to bound the errors
    due to the discretization.  If we instead restrict ourselves to optimizing
    only a single initial condition, then a different discretization scheme emerges:
    we can discretize the state and input trajectories <em>over time</em>.</p>


<section data-type="sect1"><h1>Problem Formulation</h1>

<p>Given an initial condition, $\bx_0$,   and an input trajectory $u(t)$ defined
over a finite interval, $t\in[t_0,t_f]$,   we can compute the long-term
(finite-horizon) cost of executing that trajectory   using the standard
additive-cost optimal control objective, \[ J_{\bu(\cdot)}(\bx_0) =
\int_{t_0}^{t_f} g(\bx(t),\bu(t)) dt. \]  We will write the trajectory optimization
problem as \begin{align*}
  \minimize_{u(\cdot)} \quad & \int_{t_0}^{t_f} g(\bx(t),\bu(t)) dt \\
  \subjto \quad
  & \forall t, \dot{\bx}(t) = f(\bx(t),\bu(t)), \\
  & \bx(t_0) = \bx_0. \\
  \end{align*}
Some trajectory optimization problems may also include additional constraints,
such as collision avoidance ($\bx$ can not cause the robot to be inside an
obstacle) or input limits (e.g. $\bu_{min} \le \bu \le \bu_{max}$ ), which
can be defined for all time or some subset of the trajectory.</p>

<p> As written, the optimization above is an optimization over continuous
  trajectories.  In order to formulate this as a numerical optimization, we must
  parameterize it with a finite set of numbers.  Perhaps not surprisingly, there are
  many different ways to write down this parameterization, with a variety
  of different properties in terms of speed, robustness, and accuracy of the results.
  We will outline just a few of the most popular below.
</p>
<!--  -->

</section>

<section data-type="sect1"><h1>Computational Tools for Nonlinear Optimization</h1>

<p>Before we dive in, we need to take a moment to understand the optimization tools
  that we will be using.  In the graph-search dynamic programming algorithm,
  we magically were able to provide an iterative algorithm that was known to
  converge to optimal cost-to-go function.  With LQR we were able to solve
  the optimization problem in closed-form.  In the Lyapunov analysis chapter,
  we were able to formulate a very specific kind of optimization problem--a
  semi-definite program (or SDP)--which is a subset of convex optimization,
  and relied on custom solvers like SeDuMi or Mosek to solve the problems for us.
  Convex optimization is a hugely important subset of nonlinear optimization,
  in which we can guarantee that the optimization has no "local minima".
  In this chapter we won't be so lucky, the optimizations that we formulate
  may have local minima and the solution techniques will at best only guarantee
  that they give a locally optimal solution.</p>

<p>The generic formulation of a nonlinear optimization problem is     \[ \minimize_z
c(z) \quad \subjto \quad {\bf \phi}(z) \le 0, \]     where $z$ is a vector
of <em>decision variables</em>, $c$ is a scalar
<em>objective function</em> and $\phi$ is a vector     of <em>constraints</em>.  Note that,
although we write $\phi \le 0$, this formulation     captures positivity
constraints on the decision variables (simply mulitply the constraint by $-1$)
and equality constraints (simply list both $\phi\le0$ and $-\phi\le0$) as well.
</p>

<p>The picture that you should have in your head is a nonlinear, potentially non-convex
  objective function defined over (multi-dimensional) $z$, with a subset of
  possible $z$ values satisfying the constraints.
  <figure>
    <img width="80%" src="figures/nonlinear_optimization_w_minima.svg"/>
    <figcaption>One-dimensional cartoon of a nonlinear optimization problem.  The
      red dots represent local minima.  The blue dot represents the optimal solution.</figcaption>
  </figure>
  Note that minima can be the result of the objective function having zero-derivative
  <em>or</em> due to the a sloped objective up against a constraint.</p>

<p>Numerical methods for solving these optimization problems require an initial guess, $\hat{z}$,
  and proceed by trying to move down the objective function to a minima.  Common approaches
  include <em>gradient descent</em>, in which the gradient of the objective function is computed or estimated, or
  second-order methods such as <em>sequential quadratic programming (SQP)</em> which attempts to make a local quadratic approximation of the objective function
  and local linear approximations of the constraints and solves a quadratic program on each iteration to jump
  directly to the minimum of the local approximation.</p>

<p>When I started out, I was of the opinion that there is nothing difficult about
  implementing gradient descent or even a second-order method, and I wrote all
  of the solvers myself.  I now realize that I was wrong.  The commercial solvers
  available for nonlinear programming are substantially higher performance than
  anything I wrote myself, with a substantial number of tricks, subtleties, and parameter
  choices that can make a huge difference in practice.  Some of these solvers can
  exploit sparsity in the problem (e.g., if the constraints in a sparse way on the decision variables).
  Nowadays, we make heaviest use of SNOPT <elib>Gill06</elib>, which now
  comes bundled with the precompiled distributions of <drake></drake>, but also support
  <code>fmincon</code> from the Optimization Toolbox in MATLAB.  Note that while I do
  advocate using these tools, you do not need to use them as a black box.  In many
  cases you can improve the optimization performance by understanding and selecting non-default
  configuration parameters.</p>


</section> <!-- nonlinear optimization -->


<section data-type="sect1"><h1>Direct Transcription</h1>

<p> Let
  us start by representing the finite-time trajectories, $\bx(t)$ and $\bu(t)$ $\forall t\in[t_0,t_f]$,
  by their values at a series of <em>knot points</em>, $t_0,t_1,t_2,t_3$, and denoting
  the values at those knot points $\bx_0,...,\bx_N$ and $\bu_0,...,\bu_N$, respectively.</p>

<p>Then perhaps the simplest mapping of the trajectory optimization problem onto
  a nonlinear program is fix the knot points at even intervals, $dt$, and use
  Euler integration to write \begin{align*} \minimize_{\bx_1,...,\bx_N,\bu_0,...,\bu_{N-1}}
  & \sum_{n=0}^N g(\bx_n,\bu_n)dt \\
  & \bx_{n+1} = \bx_n + f(\bx_n,\bu_n)dt, \quad \forall n\in [0,N-1]. \end{align*}
  Note that the decision variables here are $(\bx_1,...,\bx_N,\bu_0,...,\bu_{N-1})$, because
$\bx_0$ is given, and $\bu_{N}$ does not appear in the cost nor any of the constraints.  It is
easy to generalize this approach to higher order integration schemes or to add additional
constraints to $\bx$ and/or $\bu$. Also note that this formulation does not actually benefit
from the additive cost structure, so more general cost formulations are also possible.</p>

<div data-type="example"><h1>Direct Transcription for the Double Integrator</h1>

We have implemented an optimization class hierarchy in Drake which makes it easy to try
out these algorithms.  Watching the way that they perform on our simple problems is a very
nice way to gain intuition.  Here is some simple code to solve the (time-discretized) minimum-time problem
for the double integrator.

<pre><code class="matlab">
% note: requires Drake ver >= 0.9.7

cd(fullfile(getDrakePath,'examples'));
DoubleIntegrator.runDirtran;

% make sure you take a look at the code!
edit('DoubleIntegrator.runDirtran')
</code></pre>

<p>Nothing compares to running it yourself, and poking around in the code.  But you can also click <a target="_blank" href="figures/dirtran_mintime_double_integrator.swf">here</a> to watch the result.
I hope that you recognize the parabolic trajectory from the initial condition up to the
switching surface, and then the second parabolic trajectory down to the origin.  You should
also notice that the transition between $u=1$ and $u=-1$ is imperfect, due to the discretization
error.  As an exercise, try increasing the number of knot points (the variable <code>N</code> in the code)
to see if you can get a sharper response, like <a target="_blank" href="figures/dirtran_mintime_double_integrator_fine.swf">this.</a></p>

</div>

</section> <!-- end dirtran -->

<section data-type="sect1"><h1>Direct Collocation</h1>

</section> <!-- end dircol -->


<section data-type="sect1"><h1>Shooting Methods</h1>

</section> <!-- end shooting -->

<section data-type="sect1"><h1>Pontraygin's Minimum Principle</h1>

</section> <!-- end pontryagin -->

</section>

<!-- ***************  end of trajectory optimization **************   -->

<section data-type="chapter" class="chapter"><h1>Feasible Motion Planning</h1>

</section>

<!-- ***************  end of feasible motion planning **************   -->



<section data-type="chapter" class="chapter"><h1>Feedback Motion Planning</h1>

</section>

<!-- ***************  end of feedback motion planning **************   -->

<section data-type="chapter" class="chapter"><h1>Robust and Stochastic Control</h1>

</section>

<!-- ***************  end of robust control **************   -->

<section data-type="chapter" class="chapter"><h1>Planning Under Uncertainty</h1>

</section>

<!-- ***************  end of planning under uncertainty **************   -->

<!-- START OF PART III - Estimation and Learning -->


<appendix>

<section data-type="chapter" class="chapter" id="ch:manipulator"><h1>Robot Dynamics</h1>

<section data-type="sect1"><h1>Deriving the equations of motion (an example)</h1>

<p>The equations of motion for a standard robot can be derived using the method
of Lagrange.  Using $T$ as the total kinetic energy of the system, and $U$ as
the total potential energy of the system, $L = T-U$, and $Q_i$ as the
generalized force corresponding to $q_i$, the Lagrangian dynamic equations are:
$$\frac{d}{dt}\pd{L}{\dot{q}_i} - \pd{L}{q_i} = Q_i.$$ If you are not
comfortable with these equations, then any good book chapter on rigid body
mechanics can bring you up to speed -- try <elib>Craig89</elib> for a very
practical guide to robot kinematics/dynamics, <elib>Goldstein02</elib> for a
more hard-core dynamics text or <elib>Thornton03</elib> for a classical dynamics
text which is a nice read -- for now you can take them as a handle that you can
crank to generate equations of motion. </p>

<div data-type="example"><h1>Simple Double Pendulum</h1>

<figure>
  <img style="width:250px;" src="figures/simple_double_pend.svg"/>
  <figcaption>Simple double pendulum</figcaption>
</figure>

<p>
Consider the simple double pendulum with torque actuation at both joints and all
of the mass concentrated in two points (for simplicity).  Using $\bq =
[\theta_1,\theta_2]^T$, and ${\bf p}_1,{\bf p}_2$ to denote the locations of
$m_1,m_2$, respectively, the kinematics of this system are:

\begin{eqnarray*}
{\bf p}_1 =& l_1\begin{bmatrix} s_1 \\ - c_1 \end{bmatrix}, &{\bf p}_2  =
{\bf p}_1 + l_2\begin{bmatrix} s_{1+2} \\ - c_{1+2} \end{bmatrix} \\
\dot{{\bf p}}_1 =& l_1 \dot{q}_1\begin{bmatrix} c_1 \\ s_1 \end{bmatrix},
&\dot{{\bf p}}_2 = \dot{{\bf p}}_1 + l_2 (\dot{q}_1+\dot{q}_2) \begin{bmatrix} c_{1+2} \\ s_{1+2} \end{bmatrix}
\end{eqnarray*}

Note that $s_1$ is shorthand for $\sin(q_1)$, $c_{1+2}$ is shorthand for
$\cos(q_1+q_2)$, etc. From this we can write the kinetic and potential
energy:

\begin{align*}
T =& \frac{1}{2} m_1 \dot{\bf p}_1^T \dot{\bf p}_1 + \frac{1}{2} m_2
\dot{\bf p}_2^T \dot{\bf p}_2 \\
=& \frac{1}{2}(m_1 + m_2) l_1^2 \dot{q}_1^2 + \frac{1}{2} m_2 l_2^2 (\dot{q}_1 + \dot{q}_2)^2 + m_2 l_1 l_2 \dot{q}_1 (\dot{q}_1 + \dot{q}_2) c_2 \\
U =& m_1 g y_1 + m_2 g y_2 = -(m_1+m_2) g l_1 c_1 - m_2 g l_2 c_{1+2}
\end{align*}

Taking the partial derivatives $\pd{T}{q_i}$, $\pd{T}{\dot{q}_i}$, and
$\pd{U}{q_i}$ ($\pd{U}{\dot{q}_i}$ terms are always zero), then
$\frac{d}{dt}\pd{T}{\dot{q}_i}$, and plugging them into the Lagrangian, reveals
the equations of motion:

\begin{align*}
(m_1 + m_2) l_1^2 \ddot{q}_1& + m_2 l_2^2 (\ddot{q}_1 + \ddot{q}_2) + m_2 l_1 l_2 (2 \ddot{q}_1 + \ddot{q}_2) c_2 \\
&- m_2 l_1 l_2 (2 \dot{q}_1 + \dot{q}_2) \dot{q}_2 s_2 + (m_1 + m_2) l_1 g s_1 + m_2 g l_2 s_{1+2} = \tau_1 \\
m_2 l_2^2 (\ddot{q}_1 + \ddot{q}_2)& + m_2 l_1 l_2 \ddot{q}_1 c_2 + m_2 l_1 l_2
\dot{q}_1^2 s_2 + m_2 g l_2 s_{1+2} = \tau_2
\end{align*}

As we saw in chapter 1, numerically integrating (and animating) these equations
in MATLAB produces the expected result. </p>

</div>

</section>

<section data-type="sect1"><h1>The Manipulator Equations</h1>

<p> If you crank through the Lagrangian dynamics for a few simple robotic
manipulators, you will begin to see a pattern emerge - the resulting equations
of motion all have a characteristic form.  For example, the kinetic energy of
your robot can always be written in the form: $$T = \frac{1}{2} \dot{\bq}^T {\bf
H}(\bq) \dot{\bq},$$ where ${\bf H}$ is the state-dependent inertial matrix.
This abstraction affords some insight into general manipulator dynamics - for
example we know that ${\bf H}$ is always positive definite, and
symmetric<elib>Asada86</elib>(p.107) and has a beautiful sparsity
pattern<elib>Featherstone05</elib> that we'll attempt to take advantage of in
our algorithms.</p>


<p>   Continuing our abstractions, we find that the equations of motion of a
general robotic manipulator (sans kinematic loops) take the form $${\bf
H}(\bq)\ddot{\bq} + {\bf C}(\bq,\dot{\bq})\dot{\bq} + {\bf G}(\bq) = {\bf
B}(\bq)\bu,$$ where $\bq$ is the state vector, ${\bf H}$ is the inertial matrix,
${\bf C}$ captures Coriolis forces, and ${\bf G}$ captures potentials (such as
gravity).  The matrix ${\bf B}$ maps control inputs $\bu$ into generalized
forces.</p>

<div data-type="example"><h1>Manipulator Equation form of the Simple Double Pendulum</h1>
The equations of motion from Example 1 can be written compactly as:
\begin{align*}
{\bf H}(\bq) =& \begin{bmatrix} (m_1 + m_2)l_1^2 + m_2 l_2^2 + 2 m_2 l_1l_2 c_2 & m_2 l_2^2 + m_2 l_1 l_2 c_2 \\ m_2 l_2^2 + m_2 l_1 l_2 c_2 & m_2 l_2^2 \end{bmatrix} \\
{\bf C}(\bq,\dot\bq) =& \begin{bmatrix} 0 & -m_2 l_1 l_2 (2\dot{q}_1 + \dot{q}_2)s_2 \\ m_2 l_1 l_2 \dot{q}_1 s_2 & 0 \end{bmatrix} \\
{\bf G}(\bq) =& g \begin{bmatrix} (m_1 + m_2) l_1 s_1 + m_2 l_2
  s_{1+2} \\ m_2 l_2 s_{1+2} \end{bmatrix} , \quad {\bf B}
= \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}
\end{align*}
Note that this choice of the ${\bf C}$ matrix was not unique.
</div>

<p> The manipulator equations are very general, but they do define some
important characteristics.  For example, $\ddot{\bq}$ is (state-dependent)
linearly related to the control input, $\bu$.   This observation justifies the
control-affine form of the dynamics assumed throughout the notes.</p> Note that we have
chosen to use the notation of second-order systems (with $\dot{\bq}$ and
$\ddot{\bq}$ appearing in the equations) throughout this book.  Although I
believe it provides more clarity, there is an important limitation to this
notation: it is impossible to describe 3D rotations in "minimal coordinates"
using this notation without introducing kinematic singularities (like the famous
"gimbal lock"). For instance, a common singularity-free choice for representing
a 3D rotation is the unit quaternion, described by 4 real values (plus a norm
constraint).  However we can still represent the rotational velocity without
singularities using just 3 real values.  This means that the length of our
velocity vector is no longer the same as the length of our position vector.  For
this reason, you will see that most of the software in <drake></drake> uses the more
general notation with $\bv$ to represent velocity, $\bq$ to represent positions,
and the manipulator equations are written as \[ {\bf H}(\bq) \dot{\bv} + {\bf
C}(\bq,\bv)\bv + {\bf G}(\bq) = {\bf B}(\bq) \bu, \] which is not necessarily a
second-order system.  See <elib>Duindam06</elib> for a nice discussion of this
topic.</p>

</section>

<section data-type="sect1"><h1>Recursive Rigid-Body Dynamics Algorithms</h1>

<p>
The equations of motions for our machines get complicated quickly.
Fortunately, for robots with a tree-link kinematic structure, there
are very efficient and natural recursive algorithms for generating
these equations of motion.  For a detailed reference on these methods,
see <elib>Featherstone07</elib>; some people prefer reading about the
Articulated Body Method in <elib>Mirtich96</elib>.  The software libraries
distributed with this text include a complete implementation; you can
also some matlab algorithms available on Roy Featherstone's
website, or try googling "Open Dynamics Engine" or "Simulation
Construction Set".
</p>


</section> <!-- end deriving -->

</section> <!-- end robot dynamics chapter -->

<section data-type="chapter" class="chapter"><h1>Optimization Preliminaries</h1>

<p>nonlinear optimization.  convex optimization.  linear program.  quadratic
program.  (QCQP? SOCP?).  semi-definite program.   sums-of-squares optimization.
mixed-integer optimization.  actually show the optimization hierarchy in drake and work through it.</p>

<p>solution techniques/solvers.  gradient descent.  SQP.  interior point.  homotopy. </p>

</section> <!-- end optimization chapter -->


<section data-type="chapter" class="chapter"><h1>Machine Learning Preliminaries</h1>

<section data-type="sect1"><h1>Function approximation</h1>

</section> <!-- end function approximation -->

</section> <!-- end machine learning -->

</appendix>


<div id="footer">
<hr>
<table style="width:100%;">
  <tr><td><em>Underactuated Robotics</em></td><td align="right">&copy; Russ Tedrake, 2014</td></tr>
</table>
</div>

</body>
</html>
