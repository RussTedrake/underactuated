{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TKvYiJgnYExi"
   },
   "source": [
    "This notebook provides examples to go along with the [textbook](https://underactuated.csail.mit.edu/dp.html).  I recommend having both windows open, side-by-side!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "A4QOaw_zYLfI"
   },
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import HTML, clear_output, display\n",
    "from matplotlib import cm\n",
    "from pydrake.all import (DiagramBuilder, DiscreteAlgebraicRiccatiEquation,\n",
    "                         DynamicProgrammingOptions, FittedValueIteration,\n",
    "                         InputPortIndex, LeafSystem, LinearSystem,\n",
    "                         LogVectorOutput, MathematicalProgram,\n",
    "                         MeshcatVisualizer, MultilayerPerceptron,\n",
    "                         PerceptronActivationType, PeriodicBoundaryCondition,\n",
    "                         RandomGenerator, Rgba, RigidTransform, RotationMatrix,\n",
    "                         SceneGraph, Simulator, Solve, StartMeshcat,\n",
    "                         SymbolicVectorSystem, Variable, Variables,\n",
    "                         WrapToSystem, ZeroOrderHold)\n",
    "from pydrake.examples import (AcrobotGeometry, AcrobotPlant, PendulumGeometry,\n",
    "                              PendulumParams, PendulumPlant)\n",
    "from pydrake.symbolic import Polynomial\n",
    "\n",
    "from underactuated.double_integrator import DoubleIntegratorVisualizer\n",
    "from underactuated.jupyter import AdvanceToAndVisualize, running_as_notebook\n",
    "from underactuated.meshcat_cpp_utils import interact, plot_surface\n",
    "from underactuated.optimizers import Adam\n",
    "from underactuated.pendulum import PendulumVisualizer\n",
    "\n",
    "plt.rcParams.update({\"savefig.transparent\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the visualizer (run this cell only once, each instance consumes a port)\n",
    "meshcat = StartMeshcat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Grid World\n",
    "\n",
    "The setup here is *almost* identical as the simplest version described in the notes.  The only difference is that this agent is allowed to move diagonally in a single step; this is slightly easier to code since I can have two actions (one for left/right, and another for up/down), and write the dynamics as the trivial linear system ${\\bf x}[n+1] = {\\bf u}[n].$  Only the value iteration code needs to know that the states and actions are actually restricted to the integers. I also add a very large cost when the action would be diagonal, so that it is never chosen.\n",
    "\n",
    "The obstacle (pit of despair) is provided by the method below.  Play around with it!  The rest of the code is mostly to support visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_world_example():\n",
    "    time_step = 1\n",
    "    # TODO(russt): Support discrete-time systems in the dynamic programming code, and use this properly.\n",
    "    #plant = LinearSystem(A=np.eye(2), B=np.eye(2), C=np.eye(2), D=np.zeros((2,2)), time_period=time_step)\n",
    "    # for now, just cheat because I know how to make the discrete system as a continuous that will be discretized.\n",
    "    plant = LinearSystem(A=np.zeros((2,2)), B=np.eye(2), C=np.eye(2), D=np.zeros((2,2)))\n",
    "    simulator = Simulator(plant)\n",
    "    options = DynamicProgrammingOptions()\n",
    "\n",
    "    xbins = range(0, 21)\n",
    "    ybins = range(0, 16)\n",
    "    state_grid = [set(xbins), set(ybins)]\n",
    "\n",
    "    input_grid = [set([-1, 0, 1]), set([-1, 0, 1])]\n",
    "\n",
    "    goal = [2, 8]\n",
    "\n",
    "    def obstacle(x):\n",
    "        return x[0]>=6 and x[0]<=8 and x[1]>=4 and x[1]<=7\n",
    "\n",
    "    [X, Y] = np.meshgrid(xbins, ybins)\n",
    "\n",
    "    frames=[]\n",
    "    def draw(iteration, mesh, cost_to_go, policy):\n",
    "        J = np.reshape(cost_to_go, X.shape)\n",
    "        artists = [ax.imshow(J, cmap=cm.jet)]\n",
    "        artists += [\n",
    "            ax.quiver(X,\n",
    "                      Y,\n",
    "                      np.reshape(policy[0], X.shape),\n",
    "                      np.reshape(policy[1], Y.shape),\n",
    "                      scale=1.4,\n",
    "                      scale_units='x')\n",
    "        ]\n",
    "        frames.append(artists)\n",
    "\n",
    "    if running_as_notebook:\n",
    "        options.visualization_callback = draw\n",
    "\n",
    "    def min_time_cost(context):\n",
    "        x = context.get_continuous_state_vector().CopyToVector()\n",
    "        x = np.round(x)\n",
    "        state_cost = 1\n",
    "        if obstacle(x):\n",
    "            state_cost = 10\n",
    "        if np.array_equal(x, goal):\n",
    "            state_cost = 0\n",
    "        u = plant.get_input_port(0).Eval(context)\n",
    "        action_cost = np.linalg.norm(u, 1)\n",
    "        if action_cost > 1:\n",
    "            action_cost = 10\n",
    "        return state_cost + action_cost\n",
    "\n",
    "    cost_function = min_time_cost\n",
    "    options.convergence_tol = .1;\n",
    "\n",
    "    (fig,ax) = plt.subplots(figsize=(10,6))\n",
    "    ax.set_xlabel(\"x\")\n",
    "    ax.set_ylabel(\"y\")\n",
    "    ax.set_title(\"Cost-to-Go\")\n",
    "\n",
    "    policy, cost_to_go = FittedValueIteration(simulator, cost_function,\n",
    "                                              state_grid, input_grid, time_step,\n",
    "                                              options)\n",
    "\n",
    "    draw('Final', None, cost_to_go, policy.get_output_values())\n",
    "\n",
    "    ax.invert_yaxis()\n",
    "    plt.colorbar(frames[-1][0])\n",
    "\n",
    "    print(\"generating animation...\")\n",
    "    # create animation using the animate() function\n",
    "    ani = animation.ArtistAnimation(fig, frames, interval=200, blit=True, repeat=False)\n",
    "    plt.close('all')\n",
    "\n",
    "    display(HTML(ani.to_jshtml()))\n",
    "\n",
    "grid_world_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your turn.  Change the cost.  Change the obstacles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Iteration for the Double Integrator\n",
    "\n",
    "Note that I've inserted a sleep command in the draw method to intentionally slow down the algorithm, so that you can watch the convergence in the visualizer.  If you take out the pause, it's quite fast!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DoubleIntegrator():\n",
    "    return LinearSystem(A=np.mat('0 1; 0 0'),\n",
    "                        B=np.mat('0; 1'),\n",
    "                        C=np.eye(2),\n",
    "                        D=np.zeros((2,1)))\n",
    "meshcat.Delete()\n",
    "meshcat.SetProperty('/Background', \"visible\", False)\n",
    "plant = DoubleIntegrator()\n",
    "\n",
    "def double_integrator_example(cost_function,\n",
    "                              convergence_tol,\n",
    "                              animate=True,\n",
    "                              plot=True,\n",
    "                              draw_iterations=True):\n",
    "    simulator = Simulator(plant)\n",
    "    options = DynamicProgrammingOptions()\n",
    "\n",
    "    qbins = np.linspace(-3., 3., 31)\n",
    "    qdotbins = np.linspace(-4., 4., 51)\n",
    "    state_grid = [set(qbins), set(qdotbins)]\n",
    "\n",
    "    input_limit = 1.\n",
    "    input_grid = [set(np.linspace(-input_limit, input_limit, 9))]\n",
    "    timestep = 0.01\n",
    "\n",
    "    [Q, Qdot] = np.meshgrid(qbins, qdotbins)\n",
    "\n",
    "    def draw(iteration, mesh, cost_to_go, policy):\n",
    "        # Don't draw every frame.\n",
    "        if iteration % 20 != 0:\n",
    "            return\n",
    "\n",
    "        # TODO: color by z value (e.g. cm.jet)\n",
    "        plot_surface(meshcat,\n",
    "                     'Cost-to-go',\n",
    "                     Q,\n",
    "                     Qdot,\n",
    "                     np.reshape(cost_to_go, Q.shape),\n",
    "                     wireframe=True)\n",
    "        plot_surface(meshcat,\n",
    "                     'Policy',\n",
    "                     Q,\n",
    "                     Qdot,\n",
    "                     np.reshape(policy, Q.shape),\n",
    "                     rgba=Rgba(.3, .3, .5))\n",
    "\n",
    "        # Slow down the algorithm so we can visualize the convergence.\n",
    "        sleep(0.1)\n",
    "\n",
    "    def simulate(policy):\n",
    "        # Animate the resulting policy.\n",
    "        builder = DiagramBuilder()\n",
    "        plant = builder.AddSystem(DoubleIntegrator())\n",
    "\n",
    "        vi_policy = builder.AddSystem(policy)\n",
    "        builder.Connect(plant.get_output_port(0), vi_policy.get_input_port(0))\n",
    "        builder.Connect(vi_policy.get_output_port(0), plant.get_input_port(0))\n",
    "\n",
    "        visualizer = builder.AddSystem(DoubleIntegratorVisualizer(show=False))\n",
    "        builder.Connect(plant.get_output_port(0), visualizer.get_input_port(0))\n",
    "\n",
    "        diagram = builder.Build()\n",
    "        simulator = Simulator(diagram)\n",
    "\n",
    "        simulator.get_mutable_context().SetContinuousState([-10.0, 0.0])\n",
    "\n",
    "        AdvanceToAndVisualize(simulator, visualizer, 10.)\n",
    "\n",
    "    if running_as_notebook and draw_iterations:\n",
    "        options.visualization_callback = draw\n",
    "    options.convergence_tol = convergence_tol\n",
    "\n",
    "    policy, cost_to_go = FittedValueIteration(simulator, cost_function,\n",
    "                                              state_grid, input_grid, timestep,\n",
    "                                              options)\n",
    "\n",
    "    J = np.reshape(cost_to_go, Q.shape)\n",
    "\n",
    "    plot_surface(meshcat, 'Cost-to-go', Q, Qdot, J, wireframe=True)\n",
    "\n",
    "    if animate:\n",
    "        print('Simulating...')\n",
    "        simulate(policy)\n",
    "\n",
    "    if plot:\n",
    "        fig = plt.figure(1, figsize=(9, 4))\n",
    "        ax1, ax2 = fig.subplots(1, 2)\n",
    "        ax1.set_xlabel(\"q\")\n",
    "        ax1.set_ylabel(\"qdot\")\n",
    "        ax1.set_title(\"Cost-to-Go\")\n",
    "        ax2.set_xlabel(\"q\")\n",
    "        ax2.set_ylabel(\"qdot\")\n",
    "        ax2.set_title(\"Policy\")\n",
    "        ax1.imshow(J,\n",
    "                   cmap=cm.jet,\n",
    "                   extent=(qbins[0], qbins[-1], qdotbins[-1], qdotbins[0]))\n",
    "        ax1.invert_yaxis()\n",
    "        Pi = np.reshape(policy.get_output_values(), Q.shape)\n",
    "        ax2.imshow(Pi,\n",
    "                   cmap=cm.jet,\n",
    "                   extent=(qbins[0], qbins[-1], qdotbins[-1], qdotbins[0]))\n",
    "        ax2.invert_yaxis()\n",
    "        display(plt.show())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_time_cost(context):\n",
    "    x = context.get_continuous_state_vector().CopyToVector()\n",
    "    if x.dot(x) < .05:\n",
    "        return 0.\n",
    "    return 1.\n",
    "\n",
    "\n",
    "double_integrator_example(cost_function=min_time_cost,\n",
    "                          convergence_tol=0.001,\n",
    "                          animate=True);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quadratic_regulator_cost(context):\n",
    "    x = context.get_continuous_state_vector().CopyToVector()\n",
    "    u = plant.EvalVectorInput(context, 0).CopyToVector()\n",
    "    return x.dot(x) + u.dot(u)\n",
    "\n",
    "double_integrator_example(cost_function=quadratic_regulator_cost, convergence_tol=0.1, animate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Iteration for the Simple Pendulum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def pendulum_swingup_example(min_time=True, animate=True):\n",
    "    plant = PendulumPlant()\n",
    "    simulator = Simulator(plant)\n",
    "    options = DynamicProgrammingOptions()\n",
    "\n",
    "    qbins = np.linspace(0., 2. * np.pi, 51)\n",
    "    qdotbins = np.linspace(-10., 10., 51)\n",
    "    state_grid = [set(qbins), set(qdotbins)]\n",
    "    options.periodic_boundary_conditions = [\n",
    "        PeriodicBoundaryCondition(0, 0., 2. * np.pi),\n",
    "    ]\n",
    "    options.discount_factor = .999\n",
    "    input_limit = 3.\n",
    "    input_grid = [set(np.linspace(-input_limit, input_limit, 9))]\n",
    "    timestep = 0.01\n",
    "\n",
    "    [Q, Qdot] = np.meshgrid(qbins, qdotbins)\n",
    "\n",
    "    meshcat.Delete()\n",
    "    meshcat.SetProperty(\"/Background\", \"visible\", False)\n",
    "\n",
    "    def draw(iteration, mesh, cost_to_go, policy):\n",
    "        # Don't draw every frame.\n",
    "        if iteration % 20 != 0:\n",
    "            return\n",
    "\n",
    "        plot_surface(meshcat,\n",
    "                     'Cost-to-go',\n",
    "                     Q,\n",
    "                     Qdot,\n",
    "                     np.reshape(cost_to_go, Q.shape),\n",
    "                     wireframe=True)\n",
    "        plot_surface(meshcat,\n",
    "                     'Policy',\n",
    "                     Q,\n",
    "                     Qdot,\n",
    "                     np.reshape(policy, Q.shape),\n",
    "                     rgba=Rgba(.3, .3, .5))\n",
    "\n",
    "        # Slow down the algorithm so we can visualize the convergence.\n",
    "        sleep(0.1)\n",
    "\n",
    "    def simulate(policy):\n",
    "        # Animate the resulting policy.\n",
    "        builder = DiagramBuilder()\n",
    "        pendulum = builder.AddSystem(PendulumPlant())\n",
    "\n",
    "        wrap = builder.AddSystem(WrapToSystem(2))\n",
    "        wrap.set_interval(0, 0, 2*np.pi)\n",
    "        builder.Connect(pendulum.get_output_port(0), wrap.get_input_port(0))\n",
    "        vi_policy = builder.AddSystem(policy)\n",
    "        builder.Connect(wrap.get_output_port(0), vi_policy.get_input_port(0))\n",
    "        builder.Connect(vi_policy.get_output_port(0),\n",
    "                        pendulum.get_input_port(0))\n",
    "\n",
    "        visualizer = builder.AddSystem(\n",
    "            PendulumVisualizer(show=False))\n",
    "        builder.Connect(pendulum.get_output_port(0),\n",
    "                        visualizer.get_input_port(0))\n",
    "\n",
    "        diagram = builder.Build()\n",
    "        simulator = Simulator(diagram)\n",
    "        simulator.get_mutable_context().SetContinuousState([0.1, 0.0])\n",
    "\n",
    "        AdvanceToAndVisualize(simulator, visualizer, 8.)\n",
    "\n",
    "    if running_as_notebook:\n",
    "        options.visualization_callback = draw\n",
    "\n",
    "    def min_time_cost(context):\n",
    "        x = context.get_continuous_state_vector().CopyToVector()\n",
    "        x[0] = x[0] - np.pi\n",
    "        if x.dot(x) < .05:\n",
    "            return 0.\n",
    "        return 1.\n",
    "\n",
    "    def quadratic_regulator_cost(context):\n",
    "        x = context.get_continuous_state_vector().CopyToVector()\n",
    "        x[0] = x[0] - np.pi\n",
    "        u = plant.EvalVectorInput(context, 0).CopyToVector()\n",
    "        return 2 * x.dot(x) + u.dot(u)\n",
    "\n",
    "    if min_time:\n",
    "        cost_function = min_time_cost\n",
    "        options.convergence_tol = 0.001\n",
    "    else:\n",
    "        cost_function = quadratic_regulator_cost\n",
    "        options.convergence_tol = 0.1\n",
    "\n",
    "    policy, cost_to_go = FittedValueIteration(simulator, cost_function,\n",
    "                                              state_grid, input_grid, timestep,\n",
    "                                              options)\n",
    "\n",
    "    J = np.reshape(cost_to_go, Q.shape)\n",
    "\n",
    "    plot_surface(meshcat, 'Cost-to-go', Q, Qdot, J, wireframe=True)\n",
    "\n",
    "    if animate:\n",
    "        print('Simulating...')\n",
    "        simulate(policy)\n",
    "\n",
    "    fig = plt.figure(figsize=(9, 4))\n",
    "    ax1, ax2 = fig.subplots(1, 2)\n",
    "    ax1.set_xlabel(\"q\")\n",
    "    ax1.set_ylabel(\"qdot\")\n",
    "    ax1.set_title(\"Cost-to-Go\")\n",
    "    ax2.set_xlabel(\"q\")\n",
    "    ax2.set_ylabel(\"qdot\")\n",
    "    ax2.set_title(\"Policy\")\n",
    "    ax1.imshow(J,\n",
    "               cmap=cm.jet, aspect='auto',\n",
    "               extent=(qbins[0], qbins[-1], qdotbins[-1], qdotbins[0]))\n",
    "    ax1.invert_yaxis()\n",
    "    Pi = np.reshape(policy.get_output_values(), Q.shape)\n",
    "    ax2.imshow(Pi,\n",
    "               cmap=cm.jet, aspect='auto',\n",
    "               extent=(qbins[0], qbins[-1], qdotbins[-1], qdotbins[0]))\n",
    "    ax2.invert_yaxis()\n",
    "    display(plt.show())\n",
    "\n",
    "\n",
    "pendulum_swingup_example(min_time=False, animate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Fitted Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the double integrator\n",
    "A = np.array([[0., 1.], [0., 0.]])\n",
    "B = np.array([[0.], [1.]])\n",
    "Q = 0.1*np.eye(2)\n",
    "R = np.eye(1)\n",
    "\n",
    "# vectorized\n",
    "def min_time_cost(x, u):\n",
    "    return 1.0 - np.isclose(x, np.zeros((2,1))).all(axis=0)\n",
    "\n",
    "def quadratic_regulator_cost(x, u):\n",
    "    return (x * (Q @ x)).sum(axis=0) + (u * (R @ u)).sum(axis=0)\n",
    "\n",
    "def min_time_solution(x):\n",
    "    # Caveat: this does not take the time discretization (zero-order hold on u) into account.\n",
    "    q = x[0,:]\n",
    "    qdot = x[1,:]\n",
    "    # mask indicates that we are in the regime where u = +1.\n",
    "    mask = ((qdot < 0) & (2 * q <=\n",
    "                          (qdot**2))) | ((qdot >= 0) & (2 * q < -(qdot**2)))\n",
    "    T = np.empty(q.size)\n",
    "    T[mask] = 2*np.sqrt(.5*qdot[mask]**2 - q[mask]) - qdot[mask]\n",
    "    T[~mask] = qdot[~mask] + 2*np.sqrt(.5*qdot[~mask]**2 + q[~mask])\n",
    "    return T\n",
    "\n",
    "def quadratic_regulator_solution(x, timestep, gamma=1):\n",
    "    S = DiscreteAlgebraicRiccatiEquation(A=np.sqrt(gamma) *\n",
    "                                         (np.eye(2) + timestep * A),\n",
    "                                         B=timestep * B,\n",
    "                                         Q=timestep * Q,\n",
    "                                         R=timestep * R / gamma)\n",
    "    return (x * (S @ x)).sum(axis=0)\n",
    "\n",
    "def plot_and_compare(mlp, context, running_cost, timestep, gamma=1.0):\n",
    "    x1s = np.linspace(-5,5,31)\n",
    "    x2s = np.linspace(-4,4,51)\n",
    "    X1s, X2s = np.meshgrid(x1s, x2s)\n",
    "    N = X1s.size\n",
    "    X = np.vstack((X1s.flatten(), X2s.flatten()))\n",
    "    J = np.zeros((1,N))\n",
    "\n",
    "    mlp.BatchOutput(context, X, J)\n",
    "\n",
    "    plot_surface(meshcat,\n",
    "                 \"Jhat\",\n",
    "                 X1s,\n",
    "                 X2s,\n",
    "                 J.reshape(X1s.shape),\n",
    "                rgba=Rgba(0,0,1),\n",
    "                 wireframe=True)\n",
    "\n",
    "    if running_cost == min_time_cost:\n",
    "        Jd = min_time_solution(X)\n",
    "    elif running_cost == quadratic_regulator_cost:\n",
    "        Jd = quadratic_regulator_solution(X, timestep, gamma)\n",
    "\n",
    "    plot_surface(meshcat,\n",
    "                 \"J_desired\",\n",
    "                 X1s,\n",
    "                 X2s,\n",
    "                 Jd.reshape(X1s.shape),\n",
    "                 rgba=Rgba(1, 0, 0),\n",
    "                 wireframe=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's simply evaluate how well the network can fit the known cost-to-go functions (using supervised learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SupervisedDemo(running_cost, timestep):\n",
    "    x1s = np.linspace(-5,5,51)\n",
    "    x2s = np.linspace(-4,4,51)\n",
    "    X1s, X2s = np.meshgrid(x1s, x2s)\n",
    "    N = X1s.size\n",
    "    X = np.vstack((X1s.flatten(), X2s.flatten()))\n",
    "\n",
    "    if running_cost == min_time_cost:\n",
    "        Jd = min_time_solution(X)\n",
    "    elif running_cost == quadratic_regulator_cost:\n",
    "        Jd = quadratic_regulator_solution(X, timestep)\n",
    "\n",
    "    Jd = Jd.reshape((1,N))\n",
    "\n",
    "    mlp = MultilayerPerceptron(\n",
    "        [2,100,100,1],\n",
    "        [PerceptronActivationType.kReLU, \n",
    "         PerceptronActivationType.kReLU,\n",
    "         PerceptronActivationType.kIdentity])\n",
    "    context = mlp.CreateDefaultContext()\n",
    "    generator = RandomGenerator(152)\n",
    "    mlp.SetRandomContext(context, generator)\n",
    "\n",
    "    optimizer = Adam(mlp.GetMutableParameters(context))\n",
    "\n",
    "    dloss_dparams = np.zeros(mlp.num_parameters())\n",
    "    last_loss = np.inf\n",
    "    for epoch in range(1000 if running_as_notebook else 2):\n",
    "        loss = mlp.BackpropagationMeanSquaredError(context, X, Jd,\n",
    "                                                    dloss_dparams)\n",
    "        clear_output(wait=True)\n",
    "        print(f\"loss = {loss}\")\n",
    "        if np.linalg.norm(last_loss - loss) < 0.0001:\n",
    "            break\n",
    "        last_loss = loss\n",
    "        optimizer.step(loss, dloss_dparams)\n",
    "\n",
    "    plot_and_compare(mlp, context, running_cost, timestep)\n",
    "\n",
    "meshcat.Delete()\n",
    "SupervisedDemo(min_time_cost, 0.1)\n",
    "#SupervisedDemo(quadratic_regulator_cost, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrete time, continuous state, discrete action\n",
    "\n",
    "This is the standard \"fitted value iteration\" algorithm with a multilayer perceptron (MLP) as the function approximator, and a single step of gradient descent performed on each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FittedValueIteration(running_cost, timestep):\n",
    "    x1s = np.linspace(-5,5,31)\n",
    "    x2s = np.linspace(-4,4,31)\n",
    "    us = np.linspace(-1,1,9)\n",
    "    Us, X1s, X2s = np.meshgrid(us, x1s, x2s, indexing='ij')\n",
    "    XwithU = np.vstack((X1s.flatten(), X2s.flatten()))\n",
    "    UwithX = Us.flatten().reshape(1,-1)\n",
    "    Nx = x1s.size * x2s.size\n",
    "    X = XwithU[:,:Nx]\n",
    "    N = X1s.size\n",
    "\n",
    "    Xnext = XwithU + timestep * (A @ XwithU + B @ UwithX)\n",
    "    G = timestep*running_cost(XwithU, UwithX)\n",
    "    Jnext = np.zeros((1,N))\n",
    "    Jd = np.zeros((1,Nx))\n",
    "\n",
    "    mlp = MultilayerPerceptron(\n",
    "        [2,100,100,1] if running_cost == min_time_cost else [2, 16, 16, 1],\n",
    "        [PerceptronActivationType.kReLU,\n",
    "         PerceptronActivationType.kReLU,\n",
    "         PerceptronActivationType.kIdentity])\n",
    "    context = mlp.CreateDefaultContext()\n",
    "    generator = RandomGenerator(123)\n",
    "    mlp.SetRandomContext(context, generator)\n",
    "\n",
    "    optimizer = Adam(mlp.GetMutableParameters(context))\n",
    "\n",
    "    gamma = 0.9\n",
    "    plot_and_compare(mlp, context, running_cost, timestep, gamma)\n",
    "    dloss_dparams = np.zeros(mlp.num_parameters())\n",
    "    last_loss = np.inf\n",
    "    for epoch in range(500 if running_as_notebook else 2):\n",
    "        mlp.BatchOutput(context, Xnext, Jnext)\n",
    "        Jd[:] = np.min((G + gamma*Jnext).reshape(us.size, Nx), axis=0)\n",
    "        for i in range(100 if running_as_notebook else 2):\n",
    "            loss = mlp.BackpropagationMeanSquaredError(\n",
    "                context, X, Jd, dloss_dparams)\n",
    "            optimizer.step(loss, dloss_dparams)\n",
    "        if np.linalg.norm(last_loss - loss) < 1e-8:\n",
    "            break\n",
    "        last_loss = loss\n",
    "        clear_output(wait=True)\n",
    "        print(f\"epoch {epoch}: loss = {loss}\")\n",
    "        if epoch%10 == 0:\n",
    "            plot_and_compare(mlp, context, running_cost, timestep, gamma)\n",
    "\n",
    "    plot_and_compare(mlp, context, running_cost, timestep, gamma)\n",
    "\n",
    "#FittedValueIteration(min_time_cost, 0.1)\n",
    "FittedValueIteration(quadratic_regulator_cost, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous-time, state, and actions\n",
    "\n",
    "I've written this to take an arbitrary system as the input.  It requires that the system has only continuous-time dynamics, and it assumes (currently without checking) that the system is control affine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ContinuousFittedValueIteration(plant,\n",
    "                                   plant_context,\n",
    "                                   value_mlp,\n",
    "                                   state_cost_function,\n",
    "                                   R_diag,\n",
    "                                   state_samples,\n",
    "                                   time_step=0.01,\n",
    "                                   discount_factor=1.0,\n",
    "                                   input_port_index=0,\n",
    "                                   lr=0.001,\n",
    "                                   minibatch=None,\n",
    "                                   epochs=1000,\n",
    "                                   optim_steps_per_epoch=25,\n",
    "                                   input_limits=None):\n",
    "    input_port = plant.get_input_port(input_port_index)\n",
    "    num_states = plant.num_continuous_states()\n",
    "    num_inputs = input_port.size()\n",
    "    N = state_samples.shape[1]\n",
    "\n",
    "    #assert plant.ValidateContext(plant_context)  # TODO(russt): bind this\n",
    "    assert plant_context.has_only_continuous_state()\n",
    "    assert value_mlp.get_input_port().size() == num_states\n",
    "    assert value_mlp.layers()[-1] == 1\n",
    "    assert R_diag.shape == (num_inputs,)\n",
    "    assert state_samples.shape[0] == num_states\n",
    "    assert time_step > 0.0\n",
    "    assert discount_factor > 0.0 and discount_factor <= 1.0\n",
    "    if input_limits != None:\n",
    "        assert num_inputs == 1, \"Input limits are only supported for scalar inputs (for now)\"\n",
    "        assert len(input_limits) == 2\n",
    "\n",
    "    mlp_context = value_mlp.CreateDefaultContext()\n",
    "    generator = RandomGenerator(123)\n",
    "    value_mlp.SetRandomContext(mlp_context, generator)\n",
    "\n",
    "    state_cost = state_cost_function(state_samples)\n",
    "    state_dynamics_x = np.empty((N, num_states))\n",
    "    dstate_dynamics_du = [np.empty((num_states, N))]*num_inputs\n",
    "    Rinv = 1/R_diag\n",
    "\n",
    "    state = plant_context.get_mutable_continuous_state_vector()\n",
    "\n",
    "    # Precompute dynamics and cost (TODO: parallelize this).\n",
    "    for i in range(N):\n",
    "        u = np.zeros(num_inputs)\n",
    "        input_port.FixValue(plant_context, u)\n",
    "        state.SetFromVector(state_samples[:, i])\n",
    "        state_dynamics_x[i] = plant.EvalTimeDerivatives(\n",
    "            plant_context).CopyToVector()\n",
    "        for j in range(num_inputs):\n",
    "            u[j] = 1\n",
    "            input_port.FixValue(plant_context, u)\n",
    "            dstate_dynamics_du[j][:, i] = plant.EvalTimeDerivatives(\n",
    "                plant_context).CopyToVector() - state_dynamics_x[i]\n",
    "            u[j] = 0\n",
    "\n",
    "    optimizer = Adam(value_mlp.GetMutableParameters(mlp_context), lr=lr)\n",
    "\n",
    "    M = minibatch if minibatch else N\n",
    "    J = np.zeros((1,M))\n",
    "    Jnext = np.zeros((1,M))\n",
    "    Jd = np.zeros((1,M))\n",
    "    dJdX = np.asfortranarray(np.zeros((num_states, M)))\n",
    "    dloss_dparams = np.zeros(value_mlp.num_parameters())\n",
    "    last_loss = np.inf\n",
    "    for epoch in range(epochs if running_as_notebook else 2):\n",
    "        if minibatch:\n",
    "            batch = np.random.randint(0, N, minibatch)\n",
    "        else:\n",
    "            batch = range(N)\n",
    "        value_mlp.BatchOutput(mlp_context, state_samples[:,batch], J, dJdX)\n",
    "        Xnext = state_samples[:,batch] + time_step*state_dynamics_x[batch,:].T\n",
    "        G = state_cost[batch]\n",
    "        for i in range(num_inputs):\n",
    "            ui = -0.5*Rinv[i] * np.sum(dstate_dynamics_du[i][:,batch] * dJdX, 0)\n",
    "            if input_limits != None:\n",
    "                ui = np.minimum(np.maximum(ui, input_limits[0]),\n",
    "                                input_limits[1])\n",
    "            G += R_diag[i]*ui**2\n",
    "            Xnext += time_step*dstate_dynamics_du[i][:,batch]*ui\n",
    "        value_mlp.BatchOutput(mlp_context, Xnext, Jnext)\n",
    "        Jd[:] = G*time_step + discount_factor*Jnext\n",
    "        for i in range(optim_steps_per_epoch if running_as_notebook else 2):\n",
    "            loss = value_mlp.BackpropagationMeanSquaredError(\n",
    "                mlp_context, state_samples[:,batch], Jd, dloss_dparams)\n",
    "            optimizer.step(loss, dloss_dparams)\n",
    "        if not minibatch and np.linalg.norm(last_loss - loss) < 1e-8:\n",
    "            break\n",
    "        last_loss = loss\n",
    "        if epoch % 20 == 0:\n",
    "            clear_output(wait=True)\n",
    "            print(f\"epoch {epoch}: loss = {loss}\")\n",
    "\n",
    "    return mlp_context\n",
    "\n",
    "class ContinuousFittedValueIterationPolicy(LeafSystem):\n",
    "\n",
    "    def __init__(self,\n",
    "                 plant,\n",
    "                 value_mlp,\n",
    "                 value_mlp_context,\n",
    "                 R_diag,\n",
    "                 input_port_index=0,\n",
    "                 input_limits=None):\n",
    "        LeafSystem.__init__(self)\n",
    "\n",
    "        num_plant_states = value_mlp.get_input_port().size()\n",
    "        self._plant = plant\n",
    "        self._plant_context = plant.CreateDefaultContext()\n",
    "\n",
    "        self.value_mlp = value_mlp\n",
    "        self.value_mlp_context = value_mlp_context\n",
    "        self.J = np.zeros((1,1))\n",
    "        self.dJdX = np.asfortranarray(np.zeros((num_plant_states, 1)))\n",
    "\n",
    "        self.Rinv = 1/R_diag\n",
    "        self.input_limits = input_limits\n",
    "        self.DeclareVectorInputPort(\"plant_state\", num_plant_states)\n",
    "        self._plant_input_port = self._plant.get_input_port(input_port_index)\n",
    "        self.DeclareVectorOutputPort(\"output\", self._plant_input_port.size(),\n",
    "                                     self.CalcOutput)\n",
    "\n",
    "    def CalcOutput(self, context, output):\n",
    "        num_inputs = self._plant_input_port.size()\n",
    "        u = np.zeros(num_inputs)\n",
    "        plant_state = self.get_input_port().Eval(context)\n",
    "\n",
    "        self.value_mlp.BatchOutput(self.value_mlp_context,\n",
    "                                   np.atleast_2d(plant_state).T, self.J,\n",
    "                                   self.dJdX)\n",
    "\n",
    "        self._plant_context.SetContinuousState(plant_state)\n",
    "        self._plant_input_port.FixValue(self._plant_context, u)\n",
    "        state_dynamics_x = self._plant.EvalTimeDerivatives(\n",
    "            self._plant_context).CopyToVector()\n",
    "        for i in range(num_inputs):\n",
    "            u[i] = 1\n",
    "            self._plant_input_port.FixValue(self._plant_context, u)\n",
    "            dstate_dynamics_dui = self._plant.EvalTimeDerivatives(\n",
    "                self._plant_context).CopyToVector() - state_dynamics_x\n",
    "            ui = -0.5 * self.Rinv[i] * dstate_dynamics_dui.dot(self.dJdX)\n",
    "            if self.input_limits != None:\n",
    "                ui = np.minimum(np.maximum(ui, self.input_limits[0]),\n",
    "                                self.input_limits[1])\n",
    "            output.SetAtIndex(i, ui)\n",
    "            u[i] = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double Integrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[0., 1.], [0., 0.]])\n",
    "B = np.array([[0.], [1.]])\n",
    "plant = LinearSystem(A,B, np.empty((0, 2)), np.empty((0, 1)))\n",
    "plant_context = plant.CreateDefaultContext()\n",
    "\n",
    "Q = np.eye(2)\n",
    "def quadratic_regulator_state_cost(x):\n",
    "    return (x * (Q @ x)).sum(axis=0)\n",
    "R_diag = np.array([1])\n",
    "R = np.eye(1)\n",
    "time_step = 0.01\n",
    "discount_factor = 0.9\n",
    "\n",
    "value_mlp = MultilayerPerceptron(\n",
    "        [2, 16, 16, 1],\n",
    "        [PerceptronActivationType.kReLU,\n",
    "         PerceptronActivationType.kReLU,\n",
    "         PerceptronActivationType.kIdentity])\n",
    "\n",
    "x1s = np.linspace(-5,5,31)\n",
    "x2s = np.linspace(-4,4,31)\n",
    "X1s, X2s = np.meshgrid(x1s, x2s, indexing='ij')\n",
    "state_samples = np.vstack((X1s.flatten(), X2s.flatten()))\n",
    "value_mlp_context = ContinuousFittedValueIteration(plant,\n",
    "                                             plant_context,\n",
    "                                             value_mlp,\n",
    "                                             quadratic_regulator_state_cost,\n",
    "                                             R_diag,\n",
    "                                             state_samples,\n",
    "                                             time_step=time_step,\n",
    "                                             discount_factor=discount_factor)\n",
    "\n",
    "meshcat.Delete()\n",
    "meshcat.ResetRenderMode()\n",
    "plot_and_compare(value_mlp, value_mlp_context, quadratic_regulator_cost,\n",
    "                 time_step, discount_factor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pendulum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plant = PendulumPlant()\n",
    "plant_context = plant.CreateDefaultContext()\n",
    "\n",
    "Q = np.diag([10, 1])\n",
    "def quadratic_regulator_state_cost(x):\n",
    "    err = np.copy(x)\n",
    "    err[0] -= np.pi\n",
    "    return (err * (Q @ err)).sum(axis=0)\n",
    "R_diag = np.array([1])\n",
    "R = np.diag(R_diag)\n",
    "\n",
    "value_mlp = MultilayerPerceptron(\n",
    "        [True, False], [100, 100, 1],\n",
    "        [PerceptronActivationType.kReLU,\n",
    "         PerceptronActivationType.kReLU,\n",
    "         PerceptronActivationType.kIdentity])\n",
    "\n",
    "qs = np.linspace(0., 2. * np.pi, 51)\n",
    "qdots = np.linspace(-10., 10., 41)\n",
    "Qs, Qdots = np.meshgrid(qs, qdots)\n",
    "state_samples = np.vstack((Qs.flatten(), Qdots.flatten()))\n",
    "time_step = 0.01\n",
    "discount_factor = .999\n",
    "torque_limit = 3\n",
    "value_mlp_context = ContinuousFittedValueIteration(\n",
    "    plant,\n",
    "    plant_context,\n",
    "    value_mlp,\n",
    "    quadratic_regulator_state_cost,\n",
    "    R_diag,\n",
    "    state_samples,\n",
    "    time_step=time_step,\n",
    "    discount_factor=discount_factor,\n",
    "    minibatch=32,\n",
    "    lr=1e-5,\n",
    "    epochs=3000,\n",
    "    optim_steps_per_epoch=100,\n",
    "    input_limits=[-torque_limit, torque_limit])\n",
    "\n",
    "J = value_mlp.BatchOutput(value_mlp_context, state_samples)\n",
    "fig = plt.figure(1, figsize=(9, 4))\n",
    "ax = fig.subplots()\n",
    "ax.set_xlabel(\"q\")\n",
    "ax.set_ylabel(\"qdot\")\n",
    "ax.set_title(\"Cost-to-Go\")\n",
    "ax.imshow(J.reshape(qdots.size, qs.size),\n",
    "          cmap=cm.jet,\n",
    "          extent=(qs[0], qs[-1], qdots[-1], qdots[0]))\n",
    "ax.invert_yaxis()\n",
    "ax.axis('auto')\n",
    "display(plt.show());\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate(value_mlp, value_mlp_context, R_diag):\n",
    "    builder = DiagramBuilder()\n",
    "\n",
    "    scene_graph = builder.AddSystem(SceneGraph())\n",
    "    plant = builder.AddSystem(PendulumPlant())\n",
    "    PendulumGeometry.AddToBuilder(builder, plant.get_state_output_port(),\n",
    "                                  scene_graph)\n",
    "\n",
    "    policy = builder.AddSystem(\n",
    "        ContinuousFittedValueIterationPolicy(\n",
    "            plant,\n",
    "            value_mlp,\n",
    "            value_mlp_context,\n",
    "            R_diag,\n",
    "            input_limits=[-torque_limit, torque_limit]))\n",
    "    builder.Connect(plant.get_state_output_port(), policy.get_input_port())\n",
    "\n",
    "    zoh = builder.AddSystem(ZeroOrderHold(time_step, 1))\n",
    "    builder.Connect(policy.get_output_port(), zoh.get_input_port())\n",
    "    builder.Connect(zoh.get_output_port(), plant.get_input_port())\n",
    "\n",
    "    meshcat.Delete()\n",
    "    meshcat.Set2dRenderMode(\n",
    "        X_WC=RigidTransform(RotationMatrix.MakeZRotation(np.pi), [0, 1, 0]))\n",
    "    vis = MeshcatVisualizer.AddToBuilder(builder, scene_graph, meshcat)\n",
    "\n",
    "    diagram = builder.Build()\n",
    "    simulator = Simulator(diagram)\n",
    "    context = simulator.get_mutable_context()\n",
    "    context.SetContinuousState([0.1, 0])\n",
    "    #simulator.set_target_realtime_rate(1.0 if running_as_notebook else 0.0)\n",
    "    vis.StartRecording(False)\n",
    "    simulator.AdvanceTo(4)\n",
    "    vis.PublishRecording()\n",
    "\n",
    "simulate(value_mlp, value_mlp_context, R_diag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acrobot\n",
    "\n",
    "Note: I haven't quite finished this example yet!  (coming soon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plant = AcrobotPlant()\n",
    "plant_context = plant.CreateDefaultContext()\n",
    "\n",
    "Q = np.diag([10, 10, 1, 1])\n",
    "def quadratic_regulator_state_cost(x):\n",
    "    err = np.copy(x)\n",
    "    err[0] -= np.pi\n",
    "    return (err * (Q @ err)).sum(axis=0)\n",
    "R_diag = np.array([1])\n",
    "R = np.diag(R_diag)\n",
    "\n",
    "value_mlp = MultilayerPerceptron(\n",
    "        [True, True, False, False], [32, 32, 1],\n",
    "        [PerceptronActivationType.kReLU,\n",
    "         PerceptronActivationType.kReLU,\n",
    "         PerceptronActivationType.kIdentity])\n",
    "\n",
    "q1s = np.linspace(0., 2. * np.pi, 21)\n",
    "q2s = np.linspace(0., 2. * np.pi, 21)\n",
    "q1dots = np.linspace(-10., 10., 11)\n",
    "q2dots = np.linspace(-10., 10., 11)\n",
    "Q1s, Q2s, Q1dots, Q2dots = np.meshgrid(q1s, q2s, q1dots, q2dots)\n",
    "state_samples = np.vstack(\n",
    "    (Q1s.flatten(), Q2s.flatten(), Q1dots.flatten(), Q2dots.flatten()))\n",
    "time_step = 0.01\n",
    "discount_factor = 0.95\n",
    "mlp_context = ContinuousFittedValueIteration(plant,\n",
    "                                             plant_context,\n",
    "                                             value_mlp,\n",
    "                                             quadratic_regulator_state_cost,\n",
    "                                             R_diag,\n",
    "                                             state_samples,\n",
    "                                             time_step=time_step,\n",
    "                                             discount_factor=discount_factor,\n",
    "                                             lr=1e-5,\n",
    "                                             minibatch=500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sums-of-squares Dynamic Programming\n",
    "\n",
    "## Cubic polynomial optimal control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scalar dynamics.\n",
    "f = lambda x, u : x - 4 * x ** 3 + u\n",
    "# Quadratic running cost.\n",
    "l = lambda x, u : x ** 2 + u ** 2\n",
    "# Input limits.\n",
    "U = [-1, 1]\n",
    "# State limits (region of state space where we approximate the value function).\n",
    "X = [-1, 1]\n",
    "\n",
    "# Numerically integrate the solution to the HJB to get the \"true\" optimal\n",
    "# cost-to-go; this only works in the scalar case when we can compute the\n",
    "# optimal policy explicitly as a function of dJdx. TODO: Use Drake's\n",
    "# InitialValueProblem after cleanup proposed in\n",
    "# https://github.com/RobotLocomotion/drake/issues/12857 happens.\n",
    "def optimal_cost_to_go():\n",
    "    x = Variable('x')\n",
    "    J = Variable('J')\n",
    "    builder = DiagramBuilder()\n",
    "    sys = builder.AddSystem(\n",
    "        SymbolicVectorSystem(time=x,\n",
    "                             state=[J],\n",
    "                             dynamics=[\n",
    "                                 2 * (x - 4 * x**3)\n",
    "                                 + 2 * x * np.sqrt(2 - 8 * x**2 + 16 * x**4)\n",
    "                             ], output=[J]))\n",
    "    logger = LogVectorOutput(sys.get_output_port(), builder)\n",
    "    diagram = builder.Build()\n",
    "    simulator = Simulator(diagram)\n",
    "    context = simulator.get_mutable_context()\n",
    "    # Set J(0) = 0\n",
    "    context.SetTime(0.0)\n",
    "    context.SetContinuousState([0.0])\n",
    "    simulator.AdvanceTo(1.0)\n",
    "    log = logger.FindLog(context)\n",
    "    return log.sample_times(), log.data()\n",
    "\n",
    "\n",
    "# Plot dynamics with zero input.\n",
    "n_breaks = 101\n",
    "x_breaks = np.linspace(*X, n_breaks)\n",
    "plt.plot(x_breaks, f(x_breaks, np.zeros(n_breaks)))\n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel(r'$f(x, u=0)$')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Given the degree for the approximate value function and the polynomials\n",
    "# in the S procedure, solves the SOS and returns the approximate value function\n",
    "# (together with the objective of the SOS program).\n",
    "def sos_dp(deg):\n",
    "    \n",
    "    # Set up SOS program.\n",
    "    prog = MathematicalProgram()\n",
    "    x = prog.NewIndeterminates(1, 'x')[0]\n",
    "    u = prog.NewIndeterminates(1, 'u')[0]\n",
    "    J = prog.NewFreePolynomial(Variables([x]), deg)\n",
    "\n",
    "    # Maximize volume beneath the value function.\n",
    "    J_int = J.Integrate(x, -1, 1).ToExpression()\n",
    "    prog.AddLinearCost(- J_int)\n",
    "    \n",
    "    # S-procedure for the input limits.\n",
    "    xu = Variables([x, u])\n",
    "    lamx = prog.NewSosPolynomial(xu, deg)[0]\n",
    "    S_procedure = lamx * Polynomial((x - X[0]) * (X[1] - x))\n",
    "    \n",
    "    # S-procedure for the input limits.\n",
    "    lamu = prog.NewSosPolynomial(xu, deg)[0]\n",
    "    S_procedure += lamu * Polynomial((u - U[0]) * (U[1] - u))\n",
    "    \n",
    "    # Enforce Bellman inequality.\n",
    "    J_dot = J.Differentiate(x) * Polynomial(f(x, u))\n",
    "    prog.AddSosConstraint(J_dot + Polynomial(l(x, u)) - S_procedure)\n",
    "\n",
    "    # J(0) = 0.\n",
    "    prog.AddLinearConstraint(J.EvaluatePartial({x: 0}).ToExpression() == 0)\n",
    "\n",
    "    # Solve and retrieve result.\n",
    "    result = Solve(prog)\n",
    "    assert result.is_success()\n",
    "\n",
    "    # retrieve value function\n",
    "    J_opt_expr = result.GetSolution(J.ToExpression())\n",
    "    J_opt = lambda x_eval: J_opt_expr.Evaluate({x: x_eval})\n",
    "    cost = - result.get_optimal_cost()\n",
    "    \n",
    "    return J_opt, cost\n",
    "\n",
    "# Solve for increasing degree.\n",
    "degrees = np.arange(1, 5) * 2\n",
    "J = {deg: sos_dp(deg) for deg in degrees}\n",
    "\n",
    "# Plot solution.\n",
    "x_opt, J_opt = optimal_cost_to_go()\n",
    "plt.figure()\n",
    "plt.plot(x_opt, J_opt.T, 'k', label='J*')\n",
    "plt.plot(-x_opt, J_opt.T, 'k')\n",
    "for deg in degrees:\n",
    "    label = f'Deg. {deg}'\n",
    "    J_plot = [J[deg][0](xi) for xi in x_breaks]\n",
    "    plt.plot(x_breaks, J_plot, label=label)\n",
    "    plt.xlabel(r'$x$')\n",
    "    plt.ylabel(r'$v$')\n",
    "    plt.title('Value-function lower bound')\n",
    "    plt.legend()\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System dimensions. Here:\n",
    "# x = [theta, theta_dot]\n",
    "# z = [sin(theta), cos(theta), theta_dot]\n",
    "nx = 2\n",
    "nz = 3\n",
    "nu = 1\n",
    "\n",
    "# Map from original state to augmented state.\n",
    "# Uses sympy to be able to do symbolic integration later on.\n",
    "x2z = lambda x : np.array([np.sin(x[0]), np.cos(x[0]), x[1]])\n",
    "\n",
    "# System dynamics in augmented state (z).\n",
    "params = PendulumParams()\n",
    "inertia = params.mass() * params.length() ** 2\n",
    "tau_g = params.mass() * params.gravity() * params.length()\n",
    "def f(z, u):\n",
    "    return [\n",
    "        z[1] * z[2],\n",
    "        - z[0] * z[2],\n",
    "        (tau_g * z[0] + u[0] - params.damping() * z[2]) / inertia\n",
    "    ]\n",
    "\n",
    "# State limits (region of state space where we approximate the value function).\n",
    "x_max = np.array([np.pi, 2*np.pi])\n",
    "x_min = - x_max\n",
    "z_max = x2z(x_max)\n",
    "z_min = x2z(x_min)\n",
    "\n",
    "# Equilibrium point in both the system coordinates.\n",
    "x0 = np.array([0, 0])\n",
    "z0 = x2z(x0)\n",
    "    \n",
    "# Quadratic running cost in augmented state.\n",
    "Q = np.diag([1, 1, 1])\n",
    "R = np.diag([5])\n",
    "def l(z, u):\n",
    "    return (z - z0).dot(Q).dot(z - z0) + u.dot(R).dot(u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Use deg=10 for a nice swing-up, but the open-source solver (C-SDP) only seems to be able to handle degree up to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given the degree for the approximate value function and the polynomials\n",
    "# in the S procedure, solves the SOS and returns the approximate value function\n",
    "# (together with the objective of the SOS program).\n",
    "def sos_dp(deg):\n",
    "\n",
    "    # Set up optimization.\n",
    "    prog = MathematicalProgram()\n",
    "    z = prog.NewIndeterminates(nz, 'z')\n",
    "    u = prog.NewIndeterminates(nu, 'u')\n",
    "    J = prog.NewFreePolynomial(Variables(z), deg)\n",
    "    J_expr = J.ToExpression()\n",
    "\n",
    "    # Maximize volume beneath the value function.\n",
    "    obj = J\n",
    "    for i in range(nz):\n",
    "        obj = obj.Integrate(z[i], z_min[i], z_max[i])\n",
    "    prog.AddLinearCost(- obj.ToExpression())\n",
    "\n",
    "    # S procedure for s^2 + c^2 = 1.\n",
    "    lam = prog.NewFreePolynomial(Variables(z), deg).ToExpression()\n",
    "    S_procedure = lam * (z[0]**2 + z[1]**2 - 1)\n",
    "\n",
    "    # Enforce Bellman inequality.\n",
    "    J_dot = J_expr.Jacobian(z).dot(f(z, u))\n",
    "    prog.AddSosConstraint(J_dot + l(z, u) + S_procedure)\n",
    "\n",
    "    # J(z0) = 0.\n",
    "    J0 = J_expr.EvaluatePartial(dict(zip(z, z0)))\n",
    "    prog.AddLinearConstraint(J0 == 0)\n",
    "\n",
    "    # Solve and retrieve result.\n",
    "    result = Solve(prog)\n",
    "    assert result.is_success()\n",
    "    J_star = Polynomial(result.GetSolution(J_expr))\n",
    "\n",
    "    # Solve for the optimal feedback in augmented coordinates.\n",
    "    Rinv = np.linalg.inv(R)\n",
    "    f2 = np.array([[0], [0], [1 / inertia]])\n",
    "    dJdz = J_star.ToExpression().Jacobian(z)\n",
    "    u_star = - .5 * Rinv.dot(f2.T).dot(dJdz.T)\n",
    "\n",
    "    return J_star, u_star, z\n",
    "\n",
    "J_star, u_star, z = sos_dp(deg=4)\n",
    "\n",
    "X1, X2 = np.meshgrid(np.linspace(x_min[0], x_max[0], 51),\n",
    "                     np.linspace(x_min[1], x_max[1], 51))\n",
    "X = np.vstack((X1.flatten(), X2.flatten()))\n",
    "Z = x2z(X)\n",
    "J = np.zeros(Z.shape[1])\n",
    "for i in range(Z.shape[1]):\n",
    "    J[i] = J_star.Evaluate({z[0]: Z[0, i], z[1]: Z[1, i], z[2]: Z[2, i]})\n",
    "\n",
    "fig = plt.figure(figsize=(9, 4))\n",
    "ax = fig.subplots()\n",
    "ax.set_xlabel(\"q\")\n",
    "ax.set_ylabel(\"qdot\")\n",
    "ax.set_title(\"Cost-to-Go\")\n",
    "ax.imshow(J.reshape(X1.shape),\n",
    "        cmap=cm.jet, aspect='auto',\n",
    "        extent=(x_min[0], x_max[0], x_min[1], x_max[1]))\n",
    "ax.invert_yaxis()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Underactuated Robotics - The Simple Pendulum.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
