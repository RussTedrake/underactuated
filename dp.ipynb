{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TKvYiJgnYExi"
   },
   "source": [
    "This notebook provides examples to go along with the [textbook](http://underactuated.csail.mit.edu/dp.html).  I recommend having both windows open, side-by-side!\n",
    "\n",
    "[Click here](http://underactuated.csail.mit.edu/drake.html#notebooks) for instructions on how to run the notebook on Deepnote and/or Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "A4QOaw_zYLfI"
   },
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import HTML, clear_output, display\n",
    "from matplotlib import cm\n",
    "from pydrake.all import (AddMultibodyPlantSceneGraph, ContinuousValueIteration,\n",
    "                         ContinuousValueIterationOptions, DiagramBuilder,\n",
    "                         DiscreteAlgebraicRiccatiEquation,\n",
    "                         DynamicProgrammingOptions, FittedValueIteration,\n",
    "                         LeafSystem, LinearSystem, MeshcatVisualizerCpp,\n",
    "                         MultibodyPlant, MultilayerPerceptron, Parser,\n",
    "                         PerceptronActivationType, PeriodicBoundaryCondition,\n",
    "                         RandomGenerator, Rgba, RigidTransform, RotationMatrix,\n",
    "                         SceneGraph, Simulator, Sphere, StartMeshcat,\n",
    "                         WrapToSystem, ZeroOrderHold)\n",
    "from pydrake.examples.acrobot import AcrobotGeometry, AcrobotPlant\n",
    "from pydrake.examples.pendulum import PendulumGeometry, PendulumPlant\n",
    "\n",
    "from underactuated.double_integrator import DoubleIntegratorVisualizer\n",
    "from underactuated.jupyter import AdvanceToAndVisualize, running_as_notebook\n",
    "from underactuated.meshcat_cpp_utils import plot_surface\n",
    "from underactuated.optimizers import Adam\n",
    "from underactuated.pendulum import PendulumVisualizer\n",
    "from underactuated.utils import FindResource\n",
    "\n",
    "plt.rcParams.update({\"savefig.transparent\": True})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meshcat is now available at http://localhost:7001\n"
     ]
    }
   ],
   "source": [
    "# Start the visualizer (run this cell only once, each instance consumes a port)\n",
    "meshcat = StartMeshcat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Grid World\n",
    "\n",
    "The setup here is *almost* identical as the simplest version described in the notes.  The only difference is that this agent is allowed to move diagonally in a single step; this is slightly easier to code since I can have two actions (one for left/right, and another for up/down), and write the dynamics as the trivial linear system ${\\bf x}[n+1] = {\\bf u}[n].$  Only the value iteration code needs to know that the states and actions are actually restricted to the integers. I also add a very large cost when the action would be diagonal, so that it is never chosen.\n",
    "\n",
    "The obstacle (pit of despair) is provided by the method below.  Play around with it!  The rest of the code is mostly to support visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_world_example():\n",
    "    time_step = 1\n",
    "    # TODO(russt): Support discrete-time systems in the dynamic programming code, and use this properly.\n",
    "    #plant = LinearSystem(A=np.eye(2), B=np.eye(2), C=np.eye(2), D=np.zeros((2,2)), time_period=time_step)\n",
    "    # for now, just cheat because I know how to make the discrete system as a continuous that will be discretized.\n",
    "    plant = LinearSystem(A=np.zeros((2,2)), B=np.eye(2), C=np.eye(2), D=np.zeros((2,2)))\n",
    "    simulator = Simulator(plant)\n",
    "    options = DynamicProgrammingOptions()\n",
    "\n",
    "    xbins = range(0, 21)\n",
    "    ybins = range(0, 16)\n",
    "    state_grid = [set(xbins), set(ybins)]\n",
    "\n",
    "    input_grid = [set([-1, 0, 1]), set([-1, 0, 1])]\n",
    "\n",
    "    goal = [2, 8]\n",
    "\n",
    "    def obstacle(x):\n",
    "        return x[0]>=6 and x[0]<=8 and x[1]>=4 and x[1]<=7\n",
    "\n",
    "    [X, Y] = np.meshgrid(xbins, ybins)\n",
    "\n",
    "    frames=[]\n",
    "    def draw(iteration, mesh, cost_to_go, policy):\n",
    "        J = np.reshape(cost_to_go, X.shape)\n",
    "        artists = [ax.imshow(J, cmap=cm.jet)]\n",
    "        artists += [\n",
    "            ax.quiver(X,\n",
    "                      Y,\n",
    "                      np.reshape(policy[0], X.shape),\n",
    "                      np.reshape(policy[1], Y.shape),\n",
    "                      scale=1.4,\n",
    "                      scale_units='x')\n",
    "        ]\n",
    "        frames.append(artists)\n",
    "\n",
    "    if running_as_notebook:\n",
    "        options.visualization_callback = draw\n",
    "\n",
    "    def min_time_cost(context):\n",
    "        x = context.get_continuous_state_vector().CopyToVector()\n",
    "        x = np.round(x)\n",
    "        state_cost = 1\n",
    "        if obstacle(x):\n",
    "            state_cost = 10\n",
    "        if np.array_equal(x, goal):\n",
    "            state_cost = 0\n",
    "        u = plant.get_input_port(0).Eval(context)\n",
    "        action_cost = np.linalg.norm(u, 1)\n",
    "        if action_cost > 1:\n",
    "            action_cost = 10\n",
    "        return state_cost + action_cost\n",
    "\n",
    "    cost_function = min_time_cost\n",
    "    options.convergence_tol = .1;\n",
    "\n",
    "    (fig,ax) = plt.subplots(figsize=(10,6))\n",
    "    ax.set_xlabel(\"x\")\n",
    "    ax.set_ylabel(\"y\")\n",
    "    ax.set_title(\"Cost-to-Go\")\n",
    "\n",
    "    policy, cost_to_go = FittedValueIteration(simulator, cost_function,\n",
    "                                              state_grid, input_grid, time_step,\n",
    "                                              options)\n",
    "\n",
    "    draw('Final', None, cost_to_go, policy.get_output_values())\n",
    "\n",
    "    ax.invert_yaxis()\n",
    "    plt.colorbar(frames[-1][0])\n",
    "\n",
    "    print(\"generating animation...\")\n",
    "    # create animation using the animate() function\n",
    "    ani = animation.ArtistAnimation(fig, frames, interval=200, blit=True, repeat=False)\n",
    "    plt.close('all')\n",
    "\n",
    "    display(HTML(ani.to_jshtml()))\n",
    "\n",
    "grid_world_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your turn.  Change the cost.  Change the obstacles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Iteration for the Double Integrator\n",
    "\n",
    "Note that I've inserted a sleep command in the draw method to intentionally slow down the algorithm, so that you can watch the convergence in the visualizer.  If you take out the pause, it's quite fast!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DoubleIntegrator():\n",
    "    return LinearSystem(A=np.mat('0 1; 0 0'),\n",
    "                        B=np.mat('0; 1'),\n",
    "                        C=np.eye(2),\n",
    "                        D=np.zeros((2,1)))\n",
    "meshcat.Delete()\n",
    "meshcat.SetProperty('/Background', \"visible\", False)\n",
    "plant = DoubleIntegrator()\n",
    "\n",
    "def double_integrator_example(cost_function,\n",
    "                              convergence_tol,\n",
    "                              animate=True,\n",
    "                              plot=True,\n",
    "                              draw_iterations=True):\n",
    "    simulator = Simulator(plant)\n",
    "    options = DynamicProgrammingOptions()\n",
    "\n",
    "    qbins = np.linspace(-3., 3., 31)\n",
    "    qdotbins = np.linspace(-4., 4., 51)\n",
    "    state_grid = [set(qbins), set(qdotbins)]\n",
    "\n",
    "    input_limit = 1.\n",
    "    input_grid = [set(np.linspace(-input_limit, input_limit, 9))]\n",
    "    timestep = 0.01\n",
    "\n",
    "    [Q, Qdot] = np.meshgrid(qbins, qdotbins)\n",
    "\n",
    "    def draw(iteration, mesh, cost_to_go, policy):\n",
    "        # Don't draw every frame.\n",
    "        if iteration % 20 != 0:\n",
    "            return\n",
    "\n",
    "        # TODO: color by z value (e.g. cm.jet)\n",
    "        plot_surface(meshcat,\n",
    "                     'Cost-to-go',\n",
    "                     Q,\n",
    "                     Qdot,\n",
    "                     np.reshape(cost_to_go, Q.shape),\n",
    "                     wireframe=True)\n",
    "        plot_surface(meshcat,\n",
    "                     'Policy',\n",
    "                     Q,\n",
    "                     Qdot,\n",
    "                     np.reshape(policy, Q.shape),\n",
    "                     rgba=Rgba(.3, .3, .5))\n",
    "\n",
    "        # Slow down the algorithm so we can visualize the convergence.\n",
    "        sleep(0.1)\n",
    "\n",
    "    def simulate(policy):\n",
    "        # Animate the resulting policy.\n",
    "        builder = DiagramBuilder()\n",
    "        plant = builder.AddSystem(DoubleIntegrator())\n",
    "\n",
    "        vi_policy = builder.AddSystem(policy)\n",
    "        builder.Connect(plant.get_output_port(0), vi_policy.get_input_port(0))\n",
    "        builder.Connect(vi_policy.get_output_port(0), plant.get_input_port(0))\n",
    "\n",
    "        visualizer = builder.AddSystem(DoubleIntegratorVisualizer(show=False))\n",
    "        builder.Connect(plant.get_output_port(0), visualizer.get_input_port(0))\n",
    "\n",
    "        diagram = builder.Build()\n",
    "        simulator = Simulator(diagram)\n",
    "\n",
    "        simulator.get_mutable_context().SetContinuousState([-10.0, 0.0])\n",
    "\n",
    "        AdvanceToAndVisualize(simulator, visualizer, 10.)\n",
    "\n",
    "    if running_as_notebook and draw_iterations:\n",
    "        options.visualization_callback = draw\n",
    "    options.convergence_tol = convergence_tol\n",
    "\n",
    "    policy, cost_to_go = FittedValueIteration(simulator, cost_function,\n",
    "                                              state_grid, input_grid, timestep,\n",
    "                                              options)\n",
    "\n",
    "    J = np.reshape(cost_to_go, Q.shape)\n",
    "\n",
    "    plot_surface(meshcat, 'Cost-to-go', Q, Qdot, J, wireframe=True)\n",
    "\n",
    "    if animate:\n",
    "        print('Simulating...')\n",
    "        simulate(policy)\n",
    "\n",
    "    if plot:\n",
    "        fig = plt.figure(1, figsize=(9, 4))\n",
    "        ax1, ax2 = fig.subplots(1, 2)\n",
    "        ax1.set_xlabel(\"q\")\n",
    "        ax1.set_ylabel(\"qdot\")\n",
    "        ax1.set_title(\"Cost-to-Go\")\n",
    "        ax2.set_xlabel(\"q\")\n",
    "        ax2.set_ylabel(\"qdot\")\n",
    "        ax2.set_title(\"Policy\")\n",
    "        ax1.imshow(J,\n",
    "                   cmap=cm.jet,\n",
    "                   extent=(qbins[0], qbins[-1], qdotbins[-1], qdotbins[0]))\n",
    "        ax1.invert_yaxis()\n",
    "        Pi = np.reshape(policy.get_output_values(), Q.shape)\n",
    "        ax2.imshow(Pi,\n",
    "                   cmap=cm.jet,\n",
    "                   extent=(qbins[0], qbins[-1], qdotbins[-1], qdotbins[0]))\n",
    "        ax2.invert_yaxis()\n",
    "        display(plt.show())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_time_cost(context):\n",
    "    x = context.get_continuous_state_vector().CopyToVector()\n",
    "    if x.dot(x) < .05:\n",
    "        return 0.\n",
    "    return 1.\n",
    "\n",
    "\n",
    "double_integrator_example(cost_function=min_time_cost,\n",
    "                          convergence_tol=0.001,\n",
    "                          animate=True);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quadratic_regulator_cost(context):\n",
    "    x = context.get_continuous_state_vector().CopyToVector()\n",
    "    u = plant.EvalVectorInput(context, 0).CopyToVector()\n",
    "    return x.dot(x) + u.dot(u)\n",
    "\n",
    "double_integrator_example(cost_function=quadratic_regulator_cost, convergence_tol=0.1, animate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Iteration for the Simple Pendulum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def pendulum_swingup_example(min_time=True, animate=True):\n",
    "    plant = PendulumPlant()\n",
    "    simulator = Simulator(plant)\n",
    "    options = DynamicProgrammingOptions()\n",
    "\n",
    "    qbins = np.linspace(0., 2. * np.pi, 51)\n",
    "    qdotbins = np.linspace(-10., 10., 51)\n",
    "    state_grid = [set(qbins), set(qdotbins)]\n",
    "    options.periodic_boundary_conditions = [\n",
    "        PeriodicBoundaryCondition(0, 0., 2. * np.pi),\n",
    "    ]\n",
    "    options.discount_factor = .999\n",
    "    input_limit = 3.\n",
    "    input_grid = [set(np.linspace(-input_limit, input_limit, 9))]\n",
    "    timestep = 0.01\n",
    "\n",
    "    [Q, Qdot] = np.meshgrid(qbins, qdotbins)\n",
    "\n",
    "    meshcat.Delete()\n",
    "    meshcat.SetProperty(\"/Background\", \"visible\", False)\n",
    "\n",
    "    def draw(iteration, mesh, cost_to_go, policy):\n",
    "        # Don't draw every frame.\n",
    "        if iteration % 20 != 0:\n",
    "            return\n",
    "\n",
    "        plot_surface(meshcat,\n",
    "                     'Cost-to-go',\n",
    "                     Q,\n",
    "                     Qdot,\n",
    "                     np.reshape(cost_to_go, Q.shape),\n",
    "                     wireframe=True)\n",
    "        plot_surface(meshcat,\n",
    "                     'Policy',\n",
    "                     Q,\n",
    "                     Qdot,\n",
    "                     np.reshape(policy, Q.shape),\n",
    "                     rgba=Rgba(.3, .3, .5))\n",
    "\n",
    "        # Slow down the algorithm so we can visualize the convergence.\n",
    "        sleep(0.1)\n",
    "\n",
    "    def simulate(policy):\n",
    "        # Animate the resulting policy.\n",
    "        builder = DiagramBuilder()\n",
    "        pendulum = builder.AddSystem(PendulumPlant())\n",
    "\n",
    "        wrap = builder.AddSystem(WrapToSystem(2))\n",
    "        wrap.set_interval(0, 0, 2*np.pi)\n",
    "        builder.Connect(pendulum.get_output_port(0), wrap.get_input_port(0))\n",
    "        vi_policy = builder.AddSystem(policy)\n",
    "        builder.Connect(wrap.get_output_port(0), vi_policy.get_input_port(0))\n",
    "        builder.Connect(vi_policy.get_output_port(0),\n",
    "                        pendulum.get_input_port(0))\n",
    "\n",
    "        visualizer = builder.AddSystem(\n",
    "            PendulumVisualizer(show=False))\n",
    "        builder.Connect(pendulum.get_output_port(0),\n",
    "                        visualizer.get_input_port(0))\n",
    "\n",
    "        diagram = builder.Build()\n",
    "        simulator = Simulator(diagram)\n",
    "        simulator.get_mutable_context().SetContinuousState([0.1, 0.0])\n",
    "\n",
    "        AdvanceToAndVisualize(simulator, visualizer, 8.)\n",
    "\n",
    "    if running_as_notebook:\n",
    "        options.visualization_callback = draw\n",
    "\n",
    "    def min_time_cost(context):\n",
    "        x = context.get_continuous_state_vector().CopyToVector()\n",
    "        x[0] = x[0] - np.pi\n",
    "        if x.dot(x) < .05:\n",
    "            return 0.\n",
    "        return 1.\n",
    "\n",
    "    def quadratic_regulator_cost(context):\n",
    "        x = context.get_continuous_state_vector().CopyToVector()\n",
    "        x[0] = x[0] - np.pi\n",
    "        u = plant.EvalVectorInput(context, 0).CopyToVector()\n",
    "        return 2 * x.dot(x) + u.dot(u)\n",
    "\n",
    "    if min_time:\n",
    "        cost_function = min_time_cost\n",
    "        options.convergence_tol = 0.001\n",
    "    else:\n",
    "        cost_function = quadratic_regulator_cost\n",
    "        options.convergence_tol = 0.1\n",
    "\n",
    "    policy, cost_to_go = FittedValueIteration(simulator, cost_function,\n",
    "                                              state_grid, input_grid, timestep,\n",
    "                                              options)\n",
    "\n",
    "    J = np.reshape(cost_to_go, Q.shape)\n",
    "\n",
    "    plot_surface(meshcat, 'Cost-to-go', Q, Qdot, J, wireframe=True)\n",
    "\n",
    "    if animate:\n",
    "        print('Simulating...')\n",
    "        simulate(policy)\n",
    "\n",
    "    fig = plt.figure(figsize=(9, 4))\n",
    "    ax1, ax2 = fig.subplots(1, 2)\n",
    "    ax1.set_xlabel(\"q\")\n",
    "    ax1.set_ylabel(\"qdot\")\n",
    "    ax1.set_title(\"Cost-to-Go\")\n",
    "    ax2.set_xlabel(\"q\")\n",
    "    ax2.set_ylabel(\"qdot\")\n",
    "    ax2.set_title(\"Policy\")\n",
    "    ax1.imshow(J,\n",
    "               cmap=cm.jet, aspect='auto',\n",
    "               extent=(qbins[0], qbins[-1], qdotbins[-1], qdotbins[0]))\n",
    "    ax1.invert_yaxis()\n",
    "    Pi = np.reshape(policy.get_output_values(), Q.shape)\n",
    "    ax2.imshow(Pi,\n",
    "               cmap=cm.jet, aspect='auto',\n",
    "               extent=(qbins[0], qbins[-1], qdotbins[-1], qdotbins[0]))\n",
    "    ax2.invert_yaxis()\n",
    "    display(plt.show())\n",
    "\n",
    "\n",
    "pendulum_swingup_example(min_time=False, animate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Fitted Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the double integrator\n",
    "A = np.array([[0., 1.], [0., 0.]])\n",
    "B = np.array([[0.], [1.]])\n",
    "Q = 0.1*np.eye(2)\n",
    "R = np.eye(1)\n",
    "\n",
    "# vectorized\n",
    "def min_time_cost(x, u):\n",
    "    return 1.0 - np.isclose(x, np.zeros((2,1))).all(axis=0)\n",
    "\n",
    "def quadratic_regulator_cost(x, u):\n",
    "    return (x * (Q @ x)).sum(axis=0) + (u * (R @ u)).sum(axis=0)\n",
    "\n",
    "def min_time_solution(x):\n",
    "    # Caveat: this does not take the time discretization (zero-order hold on u) into account.\n",
    "    q = x[0,:]\n",
    "    qdot = x[1,:]\n",
    "    # mask indicates that we are in the regime where u = +1.\n",
    "    mask = ((qdot < 0) & (2 * q <=\n",
    "                          (qdot**2))) | ((qdot >= 0) & (2 * q < -(qdot**2)))\n",
    "    T = np.empty(q.size)\n",
    "    T[mask] = 2*np.sqrt(.5*qdot[mask]**2 - q[mask]) - qdot[mask]\n",
    "    T[~mask] = qdot[~mask] + 2*np.sqrt(.5*qdot[~mask]**2 + q[~mask])\n",
    "    return T\n",
    "\n",
    "def quadratic_regulator_solution(x, timestep, gamma=1):\n",
    "    S = DiscreteAlgebraicRiccatiEquation(A=np.sqrt(gamma) *\n",
    "                                         (np.eye(2) + timestep * A),\n",
    "                                         B=timestep * B,\n",
    "                                         Q=timestep * Q,\n",
    "                                         R=timestep * R / gamma)\n",
    "    return (x * (S @ x)).sum(axis=0)\n",
    "\n",
    "def plot_and_compare(mlp, context, running_cost, timestep, gamma=1.0):\n",
    "    x1s = np.linspace(-5,5,31)\n",
    "    x2s = np.linspace(-4,4,51)\n",
    "    X1s, X2s = np.meshgrid(x1s, x2s)\n",
    "    N = X1s.size\n",
    "    X = np.vstack((X1s.flatten(), X2s.flatten()))\n",
    "    J = np.zeros((1,N))\n",
    "\n",
    "    mlp.BatchOutput(context, X, J)\n",
    "\n",
    "    plot_surface(meshcat,\n",
    "                 \"Jhat\",\n",
    "                 X1s,\n",
    "                 X2s,\n",
    "                 J.reshape(X1s.shape),\n",
    "                rgba=Rgba(0,0,1),\n",
    "                 wireframe=True)\n",
    "\n",
    "    if running_cost == min_time_cost:\n",
    "        Jd = min_time_solution(X)\n",
    "    elif running_cost == quadratic_regulator_cost:\n",
    "        Jd = quadratic_regulator_solution(X, timestep, gamma)\n",
    "\n",
    "    plot_surface(meshcat,\n",
    "                 \"J_desired\",\n",
    "                 X1s,\n",
    "                 X2s,\n",
    "                 Jd.reshape(X1s.shape),\n",
    "                 rgba=Rgba(1, 0, 0),\n",
    "                 wireframe=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's simply evaluate how well the network can fit the known cost-to-go functions (using supervised learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SupervisedDemo(running_cost, timestep):\n",
    "    x1s = np.linspace(-5,5,51)\n",
    "    x2s = np.linspace(-4,4,51)\n",
    "    X1s, X2s = np.meshgrid(x1s, x2s)\n",
    "    N = X1s.size\n",
    "    X = np.vstack((X1s.flatten(), X2s.flatten()))\n",
    "\n",
    "    if running_cost == min_time_cost:\n",
    "        Jd = min_time_solution(X)\n",
    "    elif running_cost == quadratic_regulator_cost:\n",
    "        Jd = quadratic_regulator_solution(X, timestep)\n",
    "\n",
    "    Jd = Jd.reshape((1,N))\n",
    "\n",
    "    mlp = MultilayerPerceptron(\n",
    "        [2,100,100,1],\n",
    "        [PerceptronActivationType.kReLU, \n",
    "         PerceptronActivationType.kReLU,\n",
    "         PerceptronActivationType.kIdentity])\n",
    "    context = mlp.CreateDefaultContext()\n",
    "    generator = RandomGenerator(152)\n",
    "    mlp.SetRandomContext(context, generator)\n",
    "\n",
    "    optimizer = Adam(mlp.GetMutableParameters(context))\n",
    "\n",
    "    dloss_dparams = np.zeros(mlp.num_parameters())\n",
    "    last_loss = np.inf\n",
    "    for epoch in range(1000 if running_as_notebook else 2):\n",
    "        loss = mlp.BackpropagationMeanSquaredError(context, X, Jd,\n",
    "                                                    dloss_dparams)\n",
    "        clear_output(wait=True)\n",
    "        print(f\"loss = {loss}\")\n",
    "        if np.linalg.norm(last_loss - loss) < 0.0001:\n",
    "            break\n",
    "        last_loss = loss\n",
    "        optimizer.step(loss, dloss_dparams)\n",
    "\n",
    "    plot_and_compare(mlp, context, running_cost, timestep)\n",
    "\n",
    "meshcat.Delete()\n",
    "SupervisedDemo(min_time_cost, 0.1)\n",
    "#SupervisedDemo(quadratic_regulator_cost, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrete time, continuous state, discrete action\n",
    "\n",
    "This is the standard \"fitted value iteration\" algorithm with a multilayer perceptron (MLP) as the function approximator, and a single step of gradient descent performed on each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FittedValueIteration(running_cost, timestep):\n",
    "    x1s = np.linspace(-5,5,31)\n",
    "    x2s = np.linspace(-4,4,31)\n",
    "    us = np.linspace(-1,1,9)\n",
    "    Us, X1s, X2s = np.meshgrid(us, x1s, x2s, indexing='ij')\n",
    "    XwithU = np.vstack((X1s.flatten(), X2s.flatten()))\n",
    "    UwithX = Us.flatten().reshape(1,-1)\n",
    "    Nx = x1s.size * x2s.size\n",
    "    X = XwithU[:,:Nx]\n",
    "    N = X1s.size\n",
    "\n",
    "    Xnext = XwithU + timestep * (A @ XwithU + B @ UwithX)\n",
    "    G = timestep*running_cost(XwithU, UwithX)\n",
    "    Jnext = np.zeros((1,N))\n",
    "    Jd = np.zeros((1,Nx))\n",
    "\n",
    "    mlp = MultilayerPerceptron(\n",
    "        [2,100,100,1] if running_cost == min_time_cost else [2, 16, 16, 1],\n",
    "        [PerceptronActivationType.kReLU,\n",
    "         PerceptronActivationType.kReLU,\n",
    "         PerceptronActivationType.kIdentity])\n",
    "    context = mlp.CreateDefaultContext()\n",
    "    generator = RandomGenerator(123)\n",
    "    mlp.SetRandomContext(context, generator)\n",
    "\n",
    "    optimizer = Adam(mlp.GetMutableParameters(context))\n",
    "\n",
    "    gamma = 0.9\n",
    "    plot_and_compare(mlp, context, running_cost, timestep, gamma)\n",
    "    dloss_dparams = np.zeros(mlp.num_parameters())\n",
    "    last_loss = np.inf\n",
    "    for epoch in range(500 if running_as_notebook else 2):\n",
    "        mlp.BatchOutput(context, Xnext, Jnext)\n",
    "        Jd[:] = np.min((G + gamma*Jnext).reshape(us.size, Nx), axis=0)\n",
    "        for i in range(100 if running_as_notebook else 2):\n",
    "            loss = mlp.BackpropagationMeanSquaredError(\n",
    "                context, X, Jd, dloss_dparams)\n",
    "            optimizer.step(loss, dloss_dparams)\n",
    "        if np.linalg.norm(last_loss - loss) < 1e-8:\n",
    "            break\n",
    "        last_loss = loss\n",
    "        clear_output(wait=True)\n",
    "        print(f\"epoch {epoch}: loss = {loss}\")\n",
    "        if epoch%10 == 0:\n",
    "            plot_and_compare(mlp, context, running_cost, timestep, gamma)\n",
    "\n",
    "    plot_and_compare(mlp, context, running_cost, timestep, gamma)\n",
    "\n",
    "#FittedValueIteration(min_time_cost, 0.1)\n",
    "FittedValueIteration(quadratic_regulator_cost, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous-time, state, and actions\n",
    "\n",
    "I've written this to take an arbitrary system as the input.  It requires that the system has only continuous-time dynamics, and it assumes (currently without checking) that the system is control affine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContinuousFittedValueIterationPolicy(LeafSystem):\n",
    "\n",
    "    def __init__(self,\n",
    "                 plant,\n",
    "                 value_mlp,\n",
    "                 value_mlp_context,\n",
    "                 R_diag,\n",
    "                 input_port_index=0,\n",
    "                 input_limits=None):\n",
    "        LeafSystem.__init__(self)\n",
    "\n",
    "        num_plant_states = value_mlp.get_input_port().size()\n",
    "        self._plant = plant\n",
    "        self._plant_context = plant.CreateDefaultContext()\n",
    "\n",
    "        self.value_mlp = value_mlp\n",
    "        self.value_mlp_context = value_mlp_context\n",
    "        self.J = np.zeros((1,1))\n",
    "        self.dJdX = np.asfortranarray(np.zeros((num_plant_states, 1)))\n",
    "\n",
    "        self.Rinv = 1/R_diag\n",
    "        self.input_limits = input_limits\n",
    "        self.DeclareVectorInputPort(\"plant_state\", num_plant_states)\n",
    "        self._plant_input_port = self._plant.get_input_port(input_port_index)\n",
    "        self.DeclareVectorOutputPort(\"output\", self._plant_input_port.size(),\n",
    "                                     self.CalcOutput)\n",
    "\n",
    "    def CalcOutput(self, context, output):\n",
    "        num_inputs = self._plant_input_port.size()\n",
    "        u = np.zeros(num_inputs)\n",
    "        plant_state = self.get_input_port().Eval(context)\n",
    "\n",
    "        self.value_mlp.BatchOutput(self.value_mlp_context,\n",
    "                                   np.atleast_2d(plant_state).T, self.J,\n",
    "                                   self.dJdX)\n",
    "\n",
    "        self._plant_context.SetContinuousState(plant_state)\n",
    "        self._plant_input_port.FixValue(self._plant_context, u)\n",
    "        state_dynamics_x = self._plant.EvalTimeDerivatives(\n",
    "            self._plant_context).CopyToVector()\n",
    "        for i in range(num_inputs):\n",
    "            u[i] = 1\n",
    "            self._plant_input_port.FixValue(self._plant_context, u)\n",
    "            dstate_dynamics_dui = self._plant.EvalTimeDerivatives(\n",
    "                self._plant_context).CopyToVector() - state_dynamics_x\n",
    "            ui = -0.5 * self.Rinv[i] * dstate_dynamics_dui.dot(self.dJdX)\n",
    "            if self.input_limits != None:\n",
    "                ui = np.minimum(np.maximum(ui, self.input_limits[0]),\n",
    "                                self.input_limits[1])\n",
    "            output.SetAtIndex(i, ui)\n",
    "            u[i] = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double Integrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DoubleIntegratorCVI():\n",
    "    A = np.array([[0., 1.], [0., 0.]])\n",
    "    B = np.array([[0.], [1.]])\n",
    "    plant = LinearSystem(A,B, np.empty((0, 2)), np.empty((0, 1)))\n",
    "    plant_context = plant.CreateDefaultContext()\n",
    "\n",
    "    Q = np.eye(2)\n",
    "    def quadratic_regulator_state_cost(x):\n",
    "        return (x * (Q @ x)).sum(axis=0)\n",
    "    R_diag = np.array([1])\n",
    "    R = np.eye(1)\n",
    "\n",
    "    value = MultilayerPerceptron([2, 16, 16, 1], [\n",
    "        PerceptronActivationType.kReLU, PerceptronActivationType.kReLU,\n",
    "        PerceptronActivationType.kIdentity\n",
    "    ])\n",
    "    value_context = value.CreateDefaultContext()\n",
    "    generator = RandomGenerator(123)\n",
    "    value.SetRandomContext(value_context, generator)\n",
    "\n",
    "    def progress(epoch, loss):\n",
    "        print(f\"epoch {epoch}: loss = {loss}\")\n",
    "\n",
    "    options = ContinuousValueIterationOptions(\n",
    "        time_step=0.01,\n",
    "        discount_factor=0.9,\n",
    "        max_epochs=150,\n",
    "        target_network_smoothing_factor=1.0,\n",
    "        visualization_callback=progress,\n",
    "        epochs_per_visualization_callback=10)\n",
    "\n",
    "    x1s = np.linspace(-5,5,11)\n",
    "    x2s = np.linspace(-4,4,11)\n",
    "    X1s, X2s = np.meshgrid(x1s, x2s, indexing='ij')\n",
    "    state_samples = np.vstack((X1s.flatten(), X2s.flatten()))\n",
    "    state_cost = quadratic_regulator_state_cost(state_samples)\n",
    "\n",
    "    ContinuousValueIteration(plant, plant_context, value, state_samples,\n",
    "                             state_cost, R_diag, value_context, generator,\n",
    "                             options)\n",
    "\n",
    "    meshcat.Delete()\n",
    "    meshcat.ResetRenderMode()\n",
    "\n",
    "    def quadratic_regulator_solution(x, timestep, gamma=1):\n",
    "        S = DiscreteAlgebraicRiccatiEquation(A=np.sqrt(gamma) *\n",
    "                                            (np.eye(2) + timestep * A),\n",
    "                                            B=timestep * B,\n",
    "                                            Q=timestep * Q,\n",
    "                                            R=timestep * R / gamma)\n",
    "        return (x * (S @ x)).sum(axis=0)\n",
    "\n",
    "    Jhat = value.BatchOutput(value_context, state_samples)\n",
    "    plot_surface(meshcat,\n",
    "                 \"Jhat\",\n",
    "                 X1s,\n",
    "                 X2s,\n",
    "                 Jhat.reshape(X1s.shape),\n",
    "                rgba=Rgba(0,0,1),\n",
    "                 wireframe=True)\n",
    "    Jd = quadratic_regulator_solution(state_samples, options.time_step,\n",
    "                                      options.discount_factor)\n",
    "    plot_surface(meshcat,\n",
    "                 \"J_desired\",\n",
    "                 X1s,\n",
    "                 X2s,\n",
    "                 Jd.reshape(X1s.shape),\n",
    "                 rgba=Rgba(1, 0, 0),\n",
    "                 wireframe=True)\n",
    "\n",
    "\n",
    "DoubleIntegratorCVI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pendulum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PendulumCVI():\n",
    "    plant = PendulumPlant()\n",
    "    plant_context = plant.CreateDefaultContext()\n",
    "\n",
    "    Q = np.diag([10, 1])\n",
    "    def quadratic_regulator_state_cost(x):\n",
    "        err = np.copy(x)\n",
    "        err[0] -= np.pi\n",
    "        return (err * (Q @ err)).sum(axis=0)\n",
    "    R_diag = np.array([1])\n",
    "\n",
    "    value = MultilayerPerceptron([True, False], [100, 100, 1], [\n",
    "        PerceptronActivationType.kReLU, PerceptronActivationType.kReLU,\n",
    "        PerceptronActivationType.kIdentity\n",
    "    ])\n",
    "    value_context = value.CreateDefaultContext()\n",
    "    generator = RandomGenerator(123)\n",
    "    value.SetRandomContext(value_context, generator)\n",
    "\n",
    "    qs = np.linspace(0., 2. * np.pi, 51)\n",
    "    qdots = np.linspace(-10., 10., 41)\n",
    "    Qs, Qdots = np.meshgrid(qs, qdots)\n",
    "    state_samples = np.vstack((Qs.flatten(), Qdots.flatten()))\n",
    "    state_cost = quadratic_regulator_state_cost(state_samples)\n",
    "\n",
    "    def progress(epoch, loss):\n",
    "        clear_output(wait=True)\n",
    "        print(f\"epoch {epoch}: loss = {loss}\")\n",
    "\n",
    "    options = ContinuousValueIterationOptions(\n",
    "        time_step=0.1,\n",
    "        discount_factor=0.99,\n",
    "        input_lower_limit=[-3],\n",
    "        input_upper_limit=[3],\n",
    "        target_network_smoothing_factor=0.5,\n",
    "        learning_rate=1e-5,\n",
    "        max_epochs=2500,\n",
    "        optimization_steps_per_epoch=100,\n",
    "        visualization_callback=progress,\n",
    "        epochs_per_visualization_callback=10,\n",
    "        max_threads=None)\n",
    "\n",
    "    ContinuousValueIteration(plant, plant_context, value, state_samples,\n",
    "                             state_cost, R_diag, value_context, generator,\n",
    "                             options)\n",
    "\n",
    "    J = value.BatchOutput(value_context, state_samples)\n",
    "    fig = plt.figure(1, figsize=(9, 4))\n",
    "    ax = fig.subplots()\n",
    "    ax.set_xlabel(\"q\")\n",
    "    ax.set_ylabel(\"qdot\")\n",
    "    ax.set_title(\"Cost-to-Go\")\n",
    "    ax.imshow(J.reshape(qdots.size, qs.size),\n",
    "            cmap=cm.jet,\n",
    "            extent=(qs[0], qs[-1], qdots[-1], qdots[0]))\n",
    "    ax.invert_yaxis()\n",
    "    ax.axis('auto')\n",
    "    display(plt.show());\n",
    "\n",
    "    return value, value_context, R_diag\n",
    "\n",
    "value, value_context, R_diag = PendulumCVI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate(value_mlp, value_mlp_context, R_diag):\n",
    "    builder = DiagramBuilder()\n",
    "\n",
    "    scene_graph = builder.AddSystem(SceneGraph())\n",
    "    plant = builder.AddSystem(PendulumPlant())\n",
    "    PendulumGeometry.AddToBuilder(builder, plant.get_state_output_port(),\n",
    "                                  scene_graph)\n",
    "\n",
    "    policy = builder.AddSystem(\n",
    "        ContinuousFittedValueIterationPolicy(\n",
    "            plant,\n",
    "            value_mlp,\n",
    "            value_mlp_context,\n",
    "            R_diag,\n",
    "            input_limits=[-3, 3]))\n",
    "    builder.Connect(plant.get_state_output_port(), policy.get_input_port())\n",
    "\n",
    "    zoh = builder.AddSystem(ZeroOrderHold(0.01, 1))\n",
    "    builder.Connect(policy.get_output_port(), zoh.get_input_port())\n",
    "    builder.Connect(zoh.get_output_port(), plant.get_input_port())\n",
    "\n",
    "    meshcat.Delete()\n",
    "    meshcat.Set2dRenderMode(\n",
    "        X_WC=RigidTransform(RotationMatrix.MakeZRotation(np.pi), [0, 1, 0]))\n",
    "    vis = MeshcatVisualizerCpp.AddToBuilder(builder, scene_graph, meshcat)\n",
    "\n",
    "    diagram = builder.Build()\n",
    "    simulator = Simulator(diagram)\n",
    "    context = simulator.get_mutable_context()\n",
    "    context.SetContinuousState([0.1, 0])\n",
    "    #simulator.set_target_realtime_rate(1.0 if running_as_notebook else 0.0)\n",
    "    vis.StartRecording(False)\n",
    "    simulator.AdvanceTo(4)\n",
    "    vis.PublishRecording()\n",
    "\n",
    "simulate(value, value_context, R_diag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acrobot\n",
    "\n",
    "Note: I haven't quite finished this example yet!  (coming soon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = RandomGenerator(123)\n",
    "\n",
    "import wandb\n",
    "wandb.init(project=\"continuous_value_iteration_acrobot\")\n",
    "\n",
    "def AcrobotCVI(value=None, value_context=None, max_epochs=500):\n",
    "    plant = AcrobotPlant()\n",
    "    plant_context = plant.CreateDefaultContext()\n",
    "\n",
    "    Q = np.diag([10, 10, 1, 1])\n",
    "    def quadratic_regulator_state_cost(x):\n",
    "        err = np.copy(x)\n",
    "        err[0] -= np.pi\n",
    "        return (err * (Q @ err)).sum(axis=0)\n",
    "    R_diag = np.array([1])\n",
    "\n",
    "    if not value:\n",
    "        value = MultilayerPerceptron([True, True, False, False], [256, 256, 1], [\n",
    "            PerceptronActivationType.kReLU, PerceptronActivationType.kReLU,\n",
    "            PerceptronActivationType.kIdentity\n",
    "        ])\n",
    "    if not value_context:\n",
    "        value_context = value.CreateDefaultContext()\n",
    "        value.SetRandomContext(value_context, generator)\n",
    "\n",
    "    q1s = np.linspace(0., 2. * np.pi, 21)\n",
    "    q2s = np.linspace(0., 2. * np.pi, 21)\n",
    "    q1dots = np.linspace(-5., 5., 21)\n",
    "    q2dots = np.linspace(-5., 5., 21)\n",
    "    Q1s, Q2s, Q1dots, Q2dots = np.meshgrid(q1s, q2s, q1dots, q2dots)\n",
    "    state_samples = np.vstack(\n",
    "        (Q1s.flatten(), Q2s.flatten(), Q1dots.flatten(), Q2dots.flatten()))\n",
    "    state_cost = quadratic_regulator_state_cost(state_samples)\n",
    "\n",
    "    #num_samples = 10000\n",
    "    #rng = np.random.default_rng(generator())\n",
    "    #state_samples = np.vstack((\n",
    "    #    rng.uniform(0, 2*np.pi, (2, num_samples)),\n",
    "    #    rng.normal(0, 5.0, (2, num_samples))\n",
    "    #))\n",
    "    # make sure to include the equilibrium\n",
    "    #state_samples[:, 0] = np.array([np.pi, 0, 0, 0])\n",
    "\n",
    "    def progress(epoch, loss):\n",
    "        clear_output(wait=True)\n",
    "        print(f\"epoch {epoch}: loss = {loss}\")\n",
    "        wandb.log({\"loss\": loss})\n",
    "\n",
    "    options = ContinuousValueIterationOptions(\n",
    "        time_step=0.01,\n",
    "        discount_factor=0.99,\n",
    "        input_lower_limit=[-8],\n",
    "        input_upper_limit=[8],\n",
    "        minibatch_size=32,\n",
    "        learning_rate=1e-4,\n",
    "        target_network_smoothing_factor=0.01,\n",
    "        max_epochs=max_epochs,\n",
    "        optimization_steps_per_epoch=100,\n",
    "        visualization_callback=progress,\n",
    "        epochs_per_visualization_callback=1,\n",
    "        max_threads=None)\n",
    "\n",
    "    wandb.config = {\n",
    "        \"learning_rate\": options.learning_rate,\n",
    "        \"target_network_smoothing_factor\": options.target_network_smoothing_factor,\n",
    "        \"optimization_steps_per_epoch\": options.optimization_steps_per_epoch,\n",
    "        \"minibatch_size\": options.minibatch_size,\n",
    "        \"samples\": \"random\",\n",
    "    }\n",
    "\n",
    "    ContinuousValueIteration(plant, plant_context, value, state_samples,\n",
    "                             state_cost, R_diag, value_context, generator,\n",
    "                             options)\n",
    "\n",
    "    return value, value_context, R_diag\n",
    "\n",
    "value, value_context, R_diag = AcrobotCVI(max_epochs=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell as many times as you like to continue the training...\n",
    "AcrobotCVI(value, value_context, max_epochs=500);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    AcrobotCVI(value, value_context, max_epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate(value_mlp, value_mlp_context, R_diag):\n",
    "    builder = DiagramBuilder()\n",
    "\n",
    "    scene_graph = builder.AddSystem(SceneGraph())\n",
    "    plant = builder.AddSystem(AcrobotPlant())\n",
    "    AcrobotGeometry.AddToBuilder(builder, plant.get_output_port(),\n",
    "                                 scene_graph)\n",
    "\n",
    "    policy = builder.AddSystem(\n",
    "        ContinuousFittedValueIterationPolicy(\n",
    "            plant,\n",
    "            value_mlp,\n",
    "            value_mlp_context,\n",
    "            R_diag,\n",
    "            input_limits=[-8, 8]))\n",
    "    builder.Connect(plant.get_output_port(), policy.get_input_port())\n",
    "\n",
    "    zoh = builder.AddSystem(ZeroOrderHold(0.01, 1))\n",
    "    builder.Connect(policy.get_output_port(), zoh.get_input_port())\n",
    "    builder.Connect(zoh.get_output_port(), plant.get_input_port())\n",
    "\n",
    "    meshcat.Delete()\n",
    "    meshcat.Set2dRenderMode(\n",
    "        X_WC=RigidTransform(RotationMatrix.MakeZRotation(np.pi), [0, 1, 0]))\n",
    "    vis = MeshcatVisualizerCpp.AddToBuilder(builder, scene_graph, meshcat)\n",
    "\n",
    "    diagram = builder.Build()\n",
    "    simulator = Simulator(diagram)\n",
    "    context = simulator.get_mutable_context()\n",
    "    context.SetContinuousState([0.1, 0, 0, 0])\n",
    "    #simulator.set_target_realtime_rate(1.0 if running_as_notebook else 0.0)\n",
    "    vis.StartRecording(False)\n",
    "    simulator.AdvanceTo(5)\n",
    "    vis.PublishRecording()\n",
    "\n",
    "simulate(value, value_context, R_diag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cart-Pole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2kqpex9z) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 723601... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "320d8901445c4f26b572f6cdd0036706",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▃▂▂▃▃▃▃▃█▅▆▃▅▂▄▄█▂▂▂▄▆</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>0.03972</td></tr></table>\n",
       "</div></div>\n",
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">graceful-snowflake-51</strong>: <a href=\"https://wandb.ai/russtedrake/continuous_value_iteration_cartpole/runs/2kqpex9z\" target=\"_blank\">https://wandb.ai/russtedrake/continuous_value_iteration_cartpole/runs/2kqpex9z</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220228_132746-2kqpex9z/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2kqpex9z). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/russtedrake/continuous_value_iteration_cartpole/runs/17w3hgc4\" target=\"_blank\">olive-gorge-52</a></strong> to <a href=\"https://wandb.ai/russtedrake/continuous_value_iteration_cartpole\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generator = RandomGenerator(153)\n",
    "\n",
    "import wandb\n",
    "wandb.init(project=\"continuous_value_iteration_cartpole\")\n",
    "\n",
    "value = MultilayerPerceptron(\n",
    "    [False, True, False, False], [64, 64, 1], [\n",
    "        PerceptronActivationType.kReLU, PerceptronActivationType.kReLU,\n",
    "        PerceptronActivationType.kIdentity\n",
    "    ])\n",
    "value_context = value.CreateDefaultContext()\n",
    "value.SetRandomContext(value_context, generator)\n",
    "# We scale the cost so the cost of swing-up (using trajectory optimization) is\n",
    "# order 10.\n",
    "Q = .01*np.diag([10, 10, 1, 1])\n",
    "R_diag = np.array([.001])\n",
    "\n",
    "def CartPoleCVI(value, value_context, max_epochs=500):\n",
    "    plant = MultibodyPlant(time_step=0.0)\n",
    "    Parser(plant).AddModelFromFile(FindResource('models/cartpole.urdf'))\n",
    "    plant.Finalize()\n",
    "    plant_context = plant.CreateDefaultContext()\n",
    "\n",
    "    # Note: I used trajectory optimization to make sure that this cost function\n",
    "    # actually resulted in swing-up (despite the discount factor).\n",
    "    x_goal = np.atleast_2d([0, np.pi, 0, 0]).T\n",
    "    def quadratic_regulator_state_cost(x):\n",
    "        err = x - x_goal\n",
    "        return (err * (Q @ err)).sum(axis=0)\n",
    "\n",
    "    q1s = np.linspace(-4, 4, 31)\n",
    "    q2s = np.linspace(0., 2. * np.pi, 31)\n",
    "    q1dots = np.linspace(-5., 5., 31)\n",
    "    q2dots = np.linspace(-5., 5., 31)\n",
    "    Q1s, Q2s, Q1dots, Q2dots = np.meshgrid(q1s, q2s, q1dots, q2dots)\n",
    "    state_samples = np.vstack(\n",
    "        (Q1s.flatten(), Q2s.flatten(), Q1dots.flatten(), Q2dots.flatten()))\n",
    "    state_cost = quadratic_regulator_state_cost(state_samples)\n",
    "\n",
    "    #num_samples = 10000\n",
    "    #rng = np.random.default_rng(generator())\n",
    "    #state_samples = np.vstack((\n",
    "    #    rng.uniform(0, 2*np.pi, (2, num_samples)),\n",
    "    #    rng.normal(0, 5.0, (2, num_samples))\n",
    "    #))\n",
    "    # make sure to include the equilibrium\n",
    "    #state_samples[:, 0] = np.array([np.pi, 0, 0, 0])\n",
    "\n",
    "    plot_samples = np.zeros((4, 51))\n",
    "    plot_samples[1] = np.linspace(0., 2. * np.pi, 51)\n",
    "    vertices = np.zeros((3, 51))\n",
    "    vertices[0] = plot_samples[1]\n",
    "    meshcat.SetObject('upright', Sphere(0.02), Rgba(0,0,1))\n",
    "    meshcat.SetTransform('upright', RigidTransform([np.pi, 0, 0]))\n",
    "\n",
    "    def progress(epoch, loss):\n",
    "        clear_output(wait=True)\n",
    "        print(f\"epoch {epoch}: loss = {loss}\")\n",
    "        wandb.log({\"loss\": loss})\n",
    "        if epoch % 10 == 0:\n",
    "            vertices[2] = value.BatchOutput(value_context, plot_samples)\n",
    "            meshcat.SetLine('Jhat_v_theta', vertices, rgba=Rgba(0,0,1))\n",
    "            meshcat.SetTransform('upright',\n",
    "                RigidTransform(\n",
    "                    [np.pi, 0,\n",
    "                     value.BatchOutput(value_context, x_goal)]))\n",
    "\n",
    "    options = ContinuousValueIterationOptions(\n",
    "        time_step=0.02,\n",
    "        discount_factor=0.999,\n",
    "        input_port_index=plant.get_actuation_input_port().get_index(),\n",
    "        minibatch_size=32,\n",
    "        learning_rate=1e-4,\n",
    "        target_network_smoothing_factor=.05,\n",
    "        max_epochs=max_epochs,\n",
    "        optimization_steps_per_epoch=100,\n",
    "        visualization_callback=progress,\n",
    "        epochs_per_visualization_callback=1,\n",
    "        zero_value_states=x_goal,\n",
    "        max_threads=None)\n",
    "\n",
    "    wandb.config = {\n",
    "        \"learning_rate\": options.learning_rate,\n",
    "        \"target_network_smoothing_factor\": options.target_network_smoothing_factor,\n",
    "        \"optimization_steps_per_epoch\": options.optimization_steps_per_epoch,\n",
    "        \"minibatch_size\": options.minibatch_size,\n",
    "        \"samples\": \"random\",\n",
    "    }\n",
    "\n",
    "    ContinuousValueIteration(plant, plant_context, value, state_samples,\n",
    "                             state_cost, R_diag, value_context, generator,\n",
    "                             options)\n",
    "\n",
    "    return value, value_context, R_diag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 999: loss = 0.020605728543615153\n"
     ]
    }
   ],
   "source": [
    "# Run this cell as many times as you like to continue the training...\n",
    "CartPoleCVI(value, value_context, max_epochs=1000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate(value_mlp, value_mlp_context, R_diag):\n",
    "    builder = DiagramBuilder()\n",
    "\n",
    "    plant, scene_graph = AddMultibodyPlantSceneGraph(builder, time_step=0.0)\n",
    "    Parser(plant).AddModelFromFile(FindResource('models/cartpole.urdf'))\n",
    "    plant.Finalize()\n",
    "\n",
    "    policy = builder.AddSystem(\n",
    "        ContinuousFittedValueIterationPolicy(\n",
    "            plant,\n",
    "            value_mlp,\n",
    "            value_mlp_context,\n",
    "            R_diag,\n",
    "            input_port_index=plant.get_actuation_input_port().get_index()))\n",
    "    builder.Connect(plant.get_state_output_port(), policy.get_input_port())\n",
    "\n",
    "    zoh = builder.AddSystem(ZeroOrderHold(0.01, 1))\n",
    "    builder.Connect(policy.get_output_port(), zoh.get_input_port())\n",
    "    builder.Connect(zoh.get_output_port(), plant.get_actuation_input_port())\n",
    "\n",
    "    meshcat.Delete()\n",
    "    meshcat.Set2dRenderMode(\n",
    "        X_WC=RigidTransform(RotationMatrix.MakeZRotation(np.pi), [0, 1, 0]))\n",
    "    vis = MeshcatVisualizerCpp.AddToBuilder(builder, scene_graph, meshcat)\n",
    "\n",
    "    diagram = builder.Build()\n",
    "    simulator = Simulator(diagram)\n",
    "    context = simulator.get_mutable_context()\n",
    "    context.SetContinuousState([0, np.pi+0.1, 0, 0])\n",
    "    #simulator.set_target_realtime_rate(1.0 if running_as_notebook else 0.0)\n",
    "    vis.StartRecording(False)\n",
    "    simulator.AdvanceTo(5)\n",
    "    context.SetContinuousState([0, 0.4, 0, 0])\n",
    "    simulator.AdvanceTo(20)\n",
    "    vis.PublishRecording()\n",
    "\n",
    "simulate(value, value_context, R_diag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Underactuated Robotics - The Simple Pendulum.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
