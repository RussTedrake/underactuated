<!DOCTYPE html>

<html>

  <head>
    <title>Underactuated Robotics</title>
    <meta name="Underactuated Robotics" content="text/html; charset=utf-8;" />

    <script type="text/javascript" src="htmlbook/book.js"></script>

    <script src="htmlbook/mathjax-config.js" defer></script> 
    <script type="text/javascript" id="MathJax-script" defer
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
    </script>
    <script>window.MathJax || document.write('<script type="text/javascript" src="htmlbook/MathJax/es5/tex-svg.js" defer><\/script>')</script>

    <link rel="stylesheet" href="htmlbook/highlight/styles/default.css">
    <script src="htmlbook/highlight/highlight.pack.js"></script> <!-- http://highlightjs.readthedocs.io/en/latest/css-classes-reference.html#language-names-and-aliases -->
    <script>hljs.initHighlightingOnLoad();</script>

    <link rel="stylesheet" type="text/css" href="htmlbook/book.css">
  </head>

<body onload="loadChapter('underactuated');">

<div data-type="titlepage">
  <header>
    <h1><a href="underactuated.html" style="text-decoration:none;">Underactuated Robotics</a></h1>
    <p data-type="subtitle">Algorithms for Walking, Running, Swimming, Flying, and Manipulation</p> 
    <p style="font-size: 18px;"><a href="http://people.csail.mit.edu/russt/">Russ Tedrake</a></p>
    <p style="font-size: 14px; text-align: right;"> 
      &copy; Russ Tedrake, 2020<br/>
      <a href="tocite.html">How to cite these notes</a><br/>
    </p>
  </header>
</div>

<p><b>Note:</b> These are working notes used for <a
href="http://underactuated.csail.mit.edu/Spring2020/">a course being taught
at MIT</a>. They will be updated throughout the Spring 2020 semester.  <a 
href="https://www.youtube.com/channel/UChfUOAhz7ynELF-s_1LPpWg">Lecture  videos are available on YouTube</a>.</p> 

<table style="width:100%;"><tr style="width:100%">
  <td style="width:33%;text-align:left;"><a class="previous_chapter" href=lyapunov.html>Previous Chapter</a></td>
  <td style="width:33%;text-align:center;"><a href=underactuated.html>Table of contents</a></td>
  <td style="width:33%;text-align:right;"><a class="next_chapter" href=planning.html>Next Chapter</a></td>
</tr></table>


<!-- EVERYTHING ABOVE THIS LINE IS OVERWRITTEN BY THE INSTALL SCRIPT -->
<chapter style="counter-reset: chapter 9"><h1>Trajectory
  Optimization</h1>

  <p>I've argued that optimal control is a powerful framework for specifying
  complex behaviors with simple objective functions, letting the dynamics and
  constraints on the system shape the resulting feedback controller (and vice
  versa!). But the computational tools that we've provided so far have been
  limited in some important ways.  The numerical approaches to dynamic
  programming which involve putting a mesh over the state space do not scale
  well to systems with state dimension more than four or five.  Linearization
  around a nominal operating point (or trajectory) allowed us to solve for
  locally optimal control policies (e.g. using LQR) for even very
  high-dimensional systems, but the effectiveness of the resulting controllers
  is limited to the region of state space where the linearization is a good
  approximation of the nonlinear dynamics.  The computational tools for Lyapunov
  analysis from the last chapter can provide, among other things, an effective
  way to compute estimates of those regions.  But we have not yet provided any
  real computational tools for approximate optimal control that work for
  high-dimensional systems beyond the linearization around a goal. That is
  precisely the goal for this chapter.</p>

  <p>The big change that is going to allow us to scale to high-dimensional
  systems is that we are going to give up the goal of solving for the optimal
  feedback controller for the entire state space, and instead attempt to find
  an optimal control solution that is valid from only a single initial
  condition. Instead of representing this as a feedback control function, we
  can represent this solution as a <em>trajectory</em>, $\bx(t), \bu(t)$,
  typically defined over a finite interval.</p>

  <section><h1>Problem Formulation</h1>

    <p>Given an initial condition, $\bx_0$, and an input trajectory $\bu(t)$
    defined over a finite interval, $t\in[t_0,t_f]$,   we can compute the
    long-term (finite-horizon) cost of executing that trajectory   using the
    standard additive-cost optimal control objective, \[ J_{\bu(\cdot)}(\bx_0) =
    \int_{t_0}^{t_f} \ell(\bx(t),\bu(t)) dt. \]  We will write the trajectory
    optimization problem as \begin{align*} \min_{\bu(\cdot)} \quad &
    \int_{t_0}^{t_f} \ell(\bx(t),\bu(t)) dt \\ \subjto \quad & \forall t,
    \dot{\bx}(t) = f(\bx(t),\bu(t)), \\ & \bx(t_0) = \bx_0. \\ \end{align*} Some
    trajectory optimization problems may also include additional constraints,
    such as collision avoidance (e.g., where the constraint is that the signed
    distance between the robot's geometry and the obstacles stays positive) or
    input limits (e.g. $\bu_{min} \le \bu \le \bu_{max}$ ), which can be defined
    for all time or some subset of the trajectory.</p>

    <p> As written, the optimization above is an optimization over continuous
    trajectories.  In order to formulate this as a numerical optimization, we
    must parameterize it with a finite set of numbers.  Perhaps not
    surprisingly, there are many different ways to write down this
    parameterization, with a variety of different properties in terms of speed,
    robustness, and accuracy of the results. We will outline just a few of the
    most popular below; I would recommend <elib>Betts98+Betts01</elib> for
    additional details.  In our graph-search dynamic programming algorithms, we
    discretized the dynamics of the system on a mesh spread across the state
    space; in addition to not scaling well, we also found it difficult to bound
    the errors due to the discretization.  Now we are in the space of
    discretizing/parameterizing over time; conveniently we know relatively much
    more about the numerics of integration along a trajectory.</p>

  </section>

  <section><h1>Computational Tools for Nonlinear
  Optimization</h1>

    <p>Before we dive in, we need to take a moment to understand the
    optimization tools that we will be using.  In the graph-search dynamic
    programming algorithm, we were magically able to provide an iterative
    algorithm that was known to converge to the optimal cost-to-go function.
    With LQR we were able to reduce the problem to a matrix Riccati equation,
    for which we have scalable numerical methods to solve.  In the Lyapunov
    analysis chapter, we were able to formulate a very specific kind of
    optimization problem--a semi-definite program (or SDP)--which is a subset of
    convex optimization, and relied on custom solvers like Gurobi or Mosek to
    solve the problems for us. Convex optimization is a hugely important subset
    of nonlinear optimization, in which we can guarantee that the optimization
    has no "local minima". In this chapter we won't be so lucky; most of the
    optimizations that we formulate may have local minima and the solution
    techniques will at best only guarantee that they give a locally optimal
    solution.</p>

    <p>The generic formulation of a nonlinear optimization problem is     \[
    \min_z c(z) \quad \subjto \quad \bphi(z) \le 0, \]     where $z$ is a
    vector of <em>decision variables</em>, $c$ is a scalar <em>objective
    function</em> and $\phi$ is a vector     of <em>constraints</em>.  Note
    that, although we write $\phi \le 0$, this formulation     captures
    positivity constraints on the decision variables (simply multiply the
    constraint by $-1$) and equality constraints (simply list both $\phi\le0$
    and $-\phi\le0$) as well. </p>

    <p>The picture that you should have in your head is a nonlinear, potentially
    non-convex objective function defined over (multi-dimensional) $z$, with a
    subset of possible $z$ values satisfying the constraints.</p>

    <figure> <img width="80%"
    src="figures/nonlinear_optimization_w_minima.svg"/>
    <figcaption>One-dimensional cartoon of a nonlinear optimization problem. The
    red dots represent local minima.  The blue dot represents the optimal
    solution.</figcaption> </figure>

    <p>Note that minima can be the result of the objective function having zero
    derivative <em>or</em> due to a sloped objective up against a
    constraint.</p>

    <p>Numerical methods for solving these optimization problems require an
    initial guess, $\hat{z}$, and proceed by trying to move down the objective
    function to a minima.  Common approaches include <em>gradient descent</em>,
    in which the gradient of the objective function is computed or estimated, or
    second-order methods such as <em>sequential quadratic programming (SQP)</em>
    which attempts to make a local quadratic approximation of the objective
    function and local linear approximations of the constraints and solves a
    quadratic program on each iteration to jump directly to the minimum of the
    local approximation.</p>

    <p>While not strictly required, these algorithms can often benefit a great
    deal from having the gradients of the objective and constraints computed
    explicitly; the alternative is to obtain them from numerical
    differentiation.  Beyond pure speed considerations, I strongly prefer to
    compute the gradients explicitly because it can help avoid numerical
    accuracy issues that can creep in with finite difference methods.  The
    desire to calculate these gradients will be a major theme in the discussion
    below, and we have gone to great lengths to provide explicit gradients of
    our provided functions and automatic differentiation of user-provided
    functions in <drake></drake>.</p>

    <p>When I started out, I was of the opinion that there is nothing difficult
    about implementing gradient descent or even a second-order method, and I
    wrote all of the solvers myself.  I now realize that I was wrong.  The
    commercial solvers available for nonlinear programming are substantially
    higher performance than anything I wrote myself, with a number of tricks,
    subtleties, and parameter choices that can make a huge difference in
    practice.  Some of these solvers can exploit sparsity in the problem (e.g.,
    if the constraints operate in a sparse way on the decision variables).
    Nowadays, we make heaviest use of SNOPT <elib>Gill06</elib>, which now comes
    bundled with the binary distributions of <drake></drake>, but also <a
    href="http://drake.mit.edu/doxygen_cxx/group__solvers.html">support a large
    suite of numerical solvers</a>.  Note that while I do advocate using these
    tools, you do not need to use them as a black box.  In many cases you can
    improve the optimization performance by understanding and selecting
    non-default configuration parameters.</p>

  </section> <!-- nonlinear optimization -->

  <section><h1>Trajectory optimization as a nonlinear
  program</h1>

    <subsection><h1>Direct Transcription</h1>

      <p> Let us start by representing the finite-time trajectories, $\bx(t)$
      and $\bu(t)$ $\forall t\in[t_0,t_f]$, by their values at a series of
      <em>break points</em>, $t_0,t_1,t_2,t_3$, and denoting the values at those
      points (aka the <em>knot points</em>) $\bx_0,...,\bx_N$ and
      $\bu_0,...,\bu_N$, respectively.</p>

      <p>Then perhaps the simplest mapping of the trajectory optimization
      problem onto a nonlinear program is to fix the break points at even
      intervals, $dt$, and use Euler integration to write \begin{align*}
      \minimize_{\bx_1,...,\bx_N,\bu_0,...,\bu_{N-1}} & \sum_{n=0}^{N-1}
      \ell(\bx_n,\bu_n)dt \\ & \bx_{n+1} = \bx_n + f(\bx_n,\bu_n)dt, \quad \forall
      n\in [0,N-1]. \end{align*} Note that the decision variables here are
      $(\bx_1,...,\bx_N,\bu_0,...,\bu_{N-1})$, because $\bx_0$ is given, and
      $\bu_{N}$ does not appear in the cost nor any of the constraints.  It is
      easy to generalize this approach to add additional costs or constraints on
      $\bx$ and/or $\bu$. (Note that this formulation does not actually benefit
      from the additive cost structure, so more general cost formulations are
      also possible.)  Computing the gradients of the objective and constraints
      is essentially as simple as computing the gradients of $\ell()$ and $f()$.
      </p>

      <!-- <todo>re-implement this.  note that it is nonlinear if dt is a decision variable (which could make sense for the minimum-time problem).</todo>
      https://github.com/RobotLocomotion/drake/blob/last_sha_with_original_matlab/drake/examples/DoubleIntegrator.m#L125
      might actually be better to replace it with the pendulum (and fixed dt).

      <example class="drake"><h1>Direct Transcription for the
      Double Integrator</h1>

        <p>We have implemented an optimization class hierarchy in
        <drake></drake> which makes it easy to try out these algorithms.
        Watching the way that they perform on our simple problems is a very nice
        way to gain intuition.  Here is some simple code to solve the
        (time-discretized) minimum-time problem for the double integrator.</p>

        <pre><code class="matlab" testfile="testDoubleIntegratorDirtran">
        % note: requires Drake ver >= 0.9.7

        cd(fullfile(getDrakePath,'examples'));
        DoubleIntegrator.runDirtran;
        </code></pre>

        Make sure you take a look at the code:
        <pre><code class="matlab">
        edit('DoubleIntegrator.runDirtran')
        </code></pre>

        <p>Nothing compares to running it yourself, and poking around in the
        code.  But you can also click <a target="_blank"
        href="figures/dirtran_mintime_double_integrator.swf">here</a> to watch
        the result. I hope that you recognize the parabolic trajectory from the
        initial condition up to the switching surface, and then the second
        parabolic trajectory down to the origin.  You should also notice that
        the transition between $u=1$ and $u=-1$ is imperfect, due to the
        discretization error.  As an exercise, try increasing the number of knot
        points (the variable <code>N</code> in the code) to see if you can get a
        sharper response, like <a target="_blank"
        href="figures/dirtran_mintime_double_integrator_fine.swf">this.</a></p>

      </example>
      -->

      <p>If you take a moment to think about what the direct transcription
      algorithm is doing,   you will see that by satisfying the dynamic
      constraints, the optimization is effectively   solving the (Euler
      approximation of the) differential equation.  But instead of marching
      forward through time, it is minimizing the inconsistency at each of the
      knot points simultaneously.   While it's easy enough to generalize the
      constraints to use higher-order integration schemes,   paying attention to
      the trade-off between the number of times the constraint must be evaluated
      vs the density of the knot points in time, if the goal is to obtain smooth
      $\bx$ trajectory   solutions then another approach quickly emerges: the
      approach taken by the so-called <em>collocation methods</em>.</p>

    </subsection> <!-- end dirtran -->

    <subsection id="direct_collocation"><h1>Direct
    Collocation</h1>

      <p> In direct collocation (c.f., <elib>Hargraves87</elib>), both the
      input trajectory and the state trajectory are represented explicitly as
      piecewise polynomial functions.  In particular, the sweet spot for this
      algorithm is taking $\bu(t)$ to be a first-order polynomial and $\bx(t)$
      to be a cubic polynomial.</p>

      <p>It turns out that in this sweet spot, the only decision variables we
      need in our optimization are the values $\bu(t)$ and $\bx(t)$ at the
      break points of the spline.  You might think that you would need the
      coefficients of the cubic spline parameters, but you do not.  For the
      first order interpolation on $\bu(t)$, given $\bu(t_k)$ and
      $\bu(t_{k+1})$, we can solve for every value $\bu(t)$ over the internval
      $t \in [k, k+1]$.  But we also have everything that we need for the cubic
      spline: given $\bx(t_k)$ and $\bu(t_k)$, we can compute $\dot\bx(t_k) = f
      (\bx(t_k), \bu(t_k))$; and the four values $\bx(t_k), \bx(t_{k+1}),
      \dot\bx (t_k), \dot\bx(t_{k+1})$ completely define all of the parameters
      of the cubic spline over the interval $t\in[k, k+1]$.  This is very
      convenient, because it is easy for us to add additional constraints to
      $\bu$ and $\bx$ at the knot points (and would have been relatively harder
      to have to convert every constraint into constraints on the spline
      coefficients.</p>

      <figure>
        <img width="80%" src="figures/dircol.svg">
        <figcaption>Cubic spline parameters used in the direct collocation method.</figcaption>
      </figure>

      <p>So far, given $\dot{\bx} = f(\bx,\bu)$, a list of break points $t_0, .
      t_N$, and arbitrary values $\bx(t_k), \bu(t_k)$, we can extract the
      spline representation of $\bu(t)$ and $\bx(t)$.  So far we haven't added
      any actual constraints to our mathematical program.  Now, just like in
      direct transcription, we need a constraint that will tell the optimizer
      how $\bx(t_{k+1})$ is related to $\bx(t_k)$, etc.  In direct collocation,
      the constraint that we add is that the derivative of the spline at the
      <i>collocation points</i> matches the dynamics.  In particular, for our
      sweet spot, if we choose the collocation points to be the midpoints of
      the spline, then we have that \begin{gather*} t_{c} =
      \frac{1}{2}\left(t_k + t_{k+1}\right), \qquad h = t_{k+1} - t_k, \\
      \bu(t_c) = \frac{1}{2}\left(\bu(t_k) + \bu(t_{k+1})\right), \\ \bx(t_c) =
      \frac{1}{2}\left(\bx(t_k) + \bx(t_{k+1})\right) + \frac{h}{8}
      \left(\dot\bx(t_k) - \dot\bx(t_{k+1})\right), \\ \dot\bx(t_c) =
      -\frac{3}{2h}\left(\bx(t_k) - \bx(t_{k+1})\right) - \frac{1}{4}
      \left(\dot\bx(t_k) + \dot\bx(t_{k+1})\right). \end{gather*} These
      equations come directly from the equations that fit the cubic spline to
      the end points/derivatives then interpolate them at the midpoint.  They
      give us precisely what we need to add the constraint to our optimization
      that $\dot{\bx(t_c}) = f(\bx(t_c), \bu(t_c))$, for each segment of the
      spline.</p>

      <p>Once again, direct collocation effectively integrates the equations of
      motion by satisfying the constraints of the optimization -- this time
      producing an integration of the dynamics that is accurate to third-order
      with effectively two evaluations of the plant dynamics per segment (since
      we use $\dot\bx(t_k)$ for two intervals). <elib> Hargraves87</elib>
      claims, without proof, that as the break points are brought closer
      together, the trajectory will converge to a true solution of the
      differential equation. Once again it is very natural to add additional
      terms to the cost function or additional input/state constraints, and
      very easy to calculate the gradients of the objective and constraints.
      I personally find it very nice to explicitly account for the parametric
      encoding of the trajectory in the solution technique.</p>

      <example class="drake"><h1>Direct Collocation for the
      Pendulum, Acrobot, and Cart-Pole</h1>

        <p>Direct collocation also easily solves the minimum-time problem for
        the double integrator.  The performance is similar to direct
        transcription, but the convergence is visibly different.  Try it for
        yourself:</p>

        <pysrc>pendulum/dircol_swingup.py</pysrc>

        <pysrc>acrobot/dircol_swingup.py</pysrc>

        <pysrc>cartpole/dircol_swingup.py</pysrc>

        As always, make sure you take a look at the code!
      </example>

    </subsection> <!-- end dircol -->

    <subsection><h1>Shooting Methods</h1>

      <p>In the methods described above, by asking the optimization package to
      perform the numerical integration of the equations of motion, we are
      effectively over-parameterizing the problem.  In fact, the optimization is
      perfectly well defined if we restrict the decision variables to
      $\bu_0,...,\bu_{N-1}$ only--we can compute $\bx_1,...,\bx_{N}$ ourselves
      by knowing $\bx_0$, and $\bu(\cdot)$.  This is exactly the approach taken
      in the shooting methods.</p>

      <subsubsection><h1>Computing gradients</h1>

        <p>Again, providing gradients of the objectives and constraints to the
        solver is not   strictly required -- most solvers will obtain them from
        finite differences if they   are not provided -- but I feel strongly
        that the solvers are faster and more robust when   exact gradients are
        provided.  Now that we have removed the $\bx$ decision variables   from
        the program, we have to take a little extra care to compute the
        gradients.  This   is still easily accomplished using the chain rule.
        To be concise (and slightly more general),   let us define
        $\bx[n+1]=f_d(\bx[n],\bu[n])$ as the discrete-time approximation of the
        continuous dynamics; for example, the forward Euler integration scheme
        used above would give   $f_d(\bx[n],\bu[n]) =
        \bx[n]+f(\bx[n],\bu[n])dt.$  Then we have   \[ \pd{J}{\bu_k} =
        \sum_{n=0}^{N-1} \left( \pd{\ell(\bx[n],\bu[n])}{\bx[n]} \pd{\bx[n]}{\bu_k}
        + \pd{\ell(\bx[n],\bu[n])}{\bu_k} \right), \]   where the gradient of the
        state with respect to the inputs can be computed   during the "forward
        simulation",   \[ \pd{\bx[n+1]}{\bu_k} = \pd{f_d(\bx[n],\bu[n])}{\bx[n]}
        \pd{\bx[n]}{\bu_k} + \pd{f_d(\bx[n],\bu[n])}{\bu_k}.  \]   These
        simulation gradients can   also be used in the chain rule to provide the
        gradients of any constraints.  Note that there are a lot of terms to
        keep around here, on the order of (state dim) $\times$ (control dim)
        $\times$ (number of timesteps).   Ouch.  Note also that many of these
        terms are zero; for instance with the Euler integration scheme above
        $\pd{\bu[n]}{\bu_k} = 0$ if $k\ne n$.  (If this looks like I'm mixing
        two notations here, recall that I'm using $\bu_k$ to represent the
        decision variable and $\bu[n]$ to represent the input used in the $n$th
        step of the simulation.)</p>

      </subsubsection>  <!-- gradients -->

      <subsubsection><h1>The special case of optimization without
      state constraints</h1>

        <p>By solving for $\bx(\cdot)$ ourselves, we've removed a large number
        of constraints from the optimization.  If no additional state
        constraints are present, and the only gradients we need to compute are
        the gradients of the objective, then a surprisingly efficient algorithm
        emerges.  I'll give the steps here without derivation, but will derive
        it in the Pontryagin section below: <ol> <li>Simulate forward:
        $$\bx[n+1] = f_d(\bx[n],\bu_n),$$ from  $\bx[0] = \bx_0$</li>
        <li>Calculate backwards: $$\lambda[n-1] =
        \pd{\ell(\bx[n],\bu[n])}{\bx[n]}^T + \pd{f(\bx[n],\bu[n])}{\bx[n]}^T
        \lambda[n],$$ from $\lambda[N-1]=0$</li> <li>Extract the gradients:
        $$\pd{J}{\bu[n]} = \pd{\ell(\bx[n],\bu[n])}{\bu[n]} + \lambda[n]^T
        \pd{f(\bx[n],\bu[n])}{\bu[n]},$$ with $\pd{J}{\bu_k} = \sum_n
        \pd{J}{\bu[n]}\pd{\bu[n]}{\bu_k}$.</li> </ol> <p>Here $\lambda[n]$ is a
        vector the same size as $\bx[n]$ which has an interpretation as
        $\lambda[n]=\pd{J}{\bx[n+1]}^T$.  The equation governing $\lambda$ is
        known as the <em>adjoint equation</em>, and it represents a dramatic
        efficient improvement over calculating the huge number of simulation
        gradients described above. In case you are interested, the adjoint
        equation is known as the <em>backpropagation algorithm</em> in the
        neural networks literature and it is one of the primary reasons that
        training neural networks became so popular in the 1980's; super fast GPU
        implementations of this algorithm are also one of the reasons that
        <em>deep learning</em> is incredibly popular right now (the availability
        of massive training databases is perhaps the other main reason). </p>

        <p>To take advantage of this efficiency, advocates of the shooting
        methods often use penalty methods   instead of enforcing hard state
        constraints; instead of telling the solver about   the constraint
        explicitly, you simply add an additional term to the cost function
        which gives a large penalty commensurate with the amount by which the
        constraint   is violated. These are not quite as accurate and can be
        harder to tune (you'd   like the cost to be high compared to other
        costs, but making it too high can   lead to numerical conditioning
        issues), but they can work.</p>

      </subsubsection>

    </subsection> <!-- end shooting -->

    <subsection><h1>Discussion</h1>

      <p>Although the decision about which algorithm is best may depend on the
      situation,   in our work we have come to favor the direct collocation
      method (and occasionally direct   transcription) for most of our work.
      There are a number of arguments for and   against each approach; I will
      try to discuss a few of them here.</p>

      <subsubsection><h1>Solver performance</h1>

        <p>Numerical conditioning.  Tail wagging the dog.</p>

        <p>Sparse constraints.  Potential for parallel evaluation of the
        constraints (computing the dynamics and their derivatives are often the
        most expensive part). </p>

      </subsubsection>

      <subsubsection><h1>Providing an initial guess</h1>

        <p>to avoid local minima.  direct transcription and collocation can take
        an initial guess in $\bx$, too.</p>

      </subsubsection>

      <subsubsection><h1>Implicit dynamics</h1>

        <p>Another potential advantage of the direct transcription and
        collocation methods is that the dynamics constraints can be written in
        implicit form.</p>

      </subsubsection>

      <subsubsection><h1>Variations in the problem formulation</h1>

        <p>There are number of useful variations to the problem formulations
        I've presented above. By far the most common is the addition of a
        terminal cost, e.g.: $$\min \quad h(\bx[N]) + \sum_{n=0}^{N-1}
        \ell(\bx[n],\bu[n]).$$ These terms are easily added to the cost function in
        the any of methods, and the adjoint equations of the shooting method
        simply require the a modified terminal condition $$\lambda[N-1] =
        \pd{h(\bx[N])}{\bx[N]}$$ when computing the gradients.</p>

        <p>Another common modification is including the spacing of the
        breakpoints as additional decision variables.  This is particularly easy
        in the direct transcription and collocation methods, and can also be
        worked into the shooting methods.  Typically we add a lower-bound on the
        time-step so that they don't all vanish to zero.</p>

      </subsubsection>

      <subsubsection><h1>Accuracy of numerical integration</h1>

        <p>One potential complaint about the direct transcription and
        collocation algorithms is that we tend to use simplistic numerical
        integration methods and often fix the integration timestep (e.g. by
        choosing Euler integration and selecting a $dt$); it is difficult to
        bound the resulting integration errors in the solution.  One tantalizing
        possibility in the shooting methods is that the forward integration
        could be accomplished by more sophisticated methods, like variable-step
        integration.  But I must say that I have not had much success with this
        approach, because while the numerical accuracy of any one function
        evaluation might be improved, these integration schemes do not
        necessarily give smooth outputs as you make incremental changes to the
        initial conditions and control (changing $\bu_2$ by $\epsilon$ could
        result in taking a different number of steps in the integration scheme).
        This lack of smoothness can wreak havoc on the convergence of the
        optimization. If numerical accuracy is a premium, then I think you will
        have more success by imposing consistency constraints (e.g. as in the
        Runge-Kutta 4th order simulation with 5th order error checking method)
        as additional constraints on the time-steps; shooting methods do not
        have any particular advantage here.</p>

        <todo>reference DMOC</todo>

      </subsubsection> <!-- end accuracy -->

    </subsection> <!-- end pros/cons -->

  </section> <!-- end three algorithms -->

  <section><h1>Pontryagin's Minimum Principle</h1>

    <p>The tools that we've been developing for numerical trajectory
    optimization are closely tied to theorems from (analytical) optimal control.
     Let us take one section to appreciate those connections.</p>

    <p>What precisely does it mean for a trajectory, $\bx(\cdot),\bu(\cdot)$, to
    be locally optimal?  It means that if I were to perturb that trajectory in
    any way (e.g. change $\bu_3$ by $\epsilon$), then I would either incur
    higher cost in my objective function or violate a constraint.  For an
    unconstrained optimization, a <em>necessary condition</em> for local
    optimality is that the gradient of the objective at the solution be exactly
    zero.  Of course the gradient can also vanish at local maxima or saddle
    points, but it certainly must vanish at local minima. We can generalize this
    argument to constrained optimization using <em>Lagrange multipliers</em>.
    </p>

    <subsection><h1>Constrained optimization with Lagrange
    multipliers</h1>

      <p>Given the equality-constrained optimization problem $$\minimize_\bz
      \ell(\bz) \quad \subjto \quad \bphi(\bz) = 0,$$ where $\bphi$ is a vector.
      Define a vector $\lambda$ of Lagrange multipliers, the same size as
      $\phi$, and the scalar Lagrangian function, $$L(\bz,{\bf \lambda}) =
      \ell(\bz) + \lambda^T\phi(\bz).$$ A necessary condition for $\bz^*$ to be an
      optimal value of the constrained optimization is that the gradients of $L$
      vanish with respect to both $\bz$ and $\lambda$: $$\pd{L}{\bz} = 0, \quad
      \pd{L}{\lambda} = 0.$$  Note that $\pd{L}{\lambda} = \phi(\bz)$, so
      requiring this to be zero is equivalent to requiring the constraints to be
      satisfied.</p>

      <example><h1>Optimization on the unit circle</h1>

        <p>Consider the following optimization: $$\min_{x,y} x+y, \quad \subjto
        \quad x^2 + y^2 = 1.$$ The level sets of $x+y$ are straight lines with
        slope $-1$, and the constraint requires that the solution lives on the
        unit circle.</p> <figure> <img width="80%"
        src="figures/lagrange_multipliers_unit_circle.svg"/> </figure>
        <p>Simply by inspection, we can determine that the optimal solution
        should be $x=y=-\frac{\sqrt{2}}{2}.$ Let's make sure we can obtain the
        same result using Lagrange multipliers. </p>

        <p>Formulating $$L = x+y+\lambda(x^2+y^2-1),$$ we can take the
        derivatives and solve

        \begin{align*}
        \pd{L}{x} = 1 + 2\lambda x = 0 \quad & \Rightarrow & \lambda = -\frac{1}{2x}, \\
        \pd{L}{y} = 1 + 2\lambda y = 1 - \frac{y}{x} = 0 \quad & \Rightarrow & y = x,\\
        \pd{L}{\lambda} = x^2+y^2-1 = 2x^2 -1 = 0 \quad & \Rightarrow & x = \pm \frac{1}{\sqrt{2}}.
        \end{align*}

        Given the two solutions which satisfy the necessary conditions, the
        negative solution is clearly the minimizer of the objective.</p>

      </example>

    </subsection>

    <subsection><h1>Lagrange multiplier derivation of the adjoint equations</h1>

      <p>Let us use Lagrange multipliers to derive the necessary conditions for
      our constrained trajectory optimization problem in discrete time
      \begin{align*} \minimize_{\bx_1,...,\bx_N,\bu_0,...,\bu_{N-1}} &
      \sum_{n=0}^{N-1} \ell(\bx[n],\bu[n]),\\ \subjto \quad & \bx[n+1] =
      f_d(\bx[n],\bu[n]). \end{align*} Formulate the Lagrangian, \[
      L(\bx[\cdot],\bu[\cdot],\lambda[\cdot]) = \sum_{n=0}^{N-1}
      \ell(\bx[n],\bu[n]) + \sum_{n=0}^{N-1} \lambda^T[n] \left(f_d(\bx[n],\bu[n])
      - \bx[n+1]\right), \] and set the derivatives to zero to obtain the
      adjoint equation method described for the shooting algorithm above:
      \begin{gather*} \forall n\in[0,N-1], \pd{L}{\lambda[n]} =
      f_d(\bx[n],\bu[n]) - \bx[n+1] = 0 \Rightarrow \bx[n+1] = f(\bx[n],\bu[n])
      \\ \forall n\in[0,N-1], \pd{L}{\bx[n]} = \pd{\ell(\bx[n],\bu[n])}{\bx} +
      \lambda^T[n] \pd{f_d(\bx[n],\bu[n])}{\bx} - \lambda^T[n-1] = 0 \\ \quad
      \Rightarrow \lambda[n-1] = \pd{\ell(\bx[n],\bu[n])}{\bx}^T +
      \pd{f_d(\bx[n],\bu[n])}{\bx}^T \lambda[n]. \\ \pd{L}{\bx[N]} =
      -\lambda[N-1] = 0 \Rightarrow \lambda[N-1] = 0 \\ \forall n\in[0,N-1],
      \pd{L}{\bu[n]} = \pd{\ell(\bx[n],\bu[n])}{\bu} + \lambda^T[n]
      \pd{f_d(\bx[n],\bu[n])}{\bu} = 0. \end{gather*} Therefore, if we are given
      an initial condition $\bx_0$ and an input trajectory $\bu[\cdot]$, we can
      verify that it satisfies the necessary conditions for optimality by
      simulating the system forward in time to solve for $\bx[\cdot]$, solving
      the adjoint equation backwards in time to solve for $\lambda[\cdot]$, and
      verifying that $\pd{L}{\bu[n]} = 0$ for all $n$. The fact that
      $\pd{J}{\bu} = \pd{L}{\bu}$ when $\pd{L}{\bx} = 0$ and $\pd{L}{\lambda} =
      0$ follows from some basic results in the calculus of variations.</p>

    </subsection>

    <subsection><h1>Necessary conditions for optimality in
    continuous time</h1>

      <p>You won't be surprised to hear that these necessary conditions have an
      analogue in continuous time. I'll state it here without derivation.  Given
      the initial conditions, $\bx_0$, a continuous dynamics, $\dot\bx =
      f(\bx,\bu)$, and the instantaneous cost $\ell(\bx,\bu)$, for a trajectory
      $\bx(\cdot),\bu(\cdot)$ defined over $t\in[t_0,t_f]$  to  be optimal it
      must satisfy the conditions that \begin{align*} \forall
      t\in[t_0,t_f],\quad & \dot{\bx} = f(\bx,\bu), \quad &\bx(0)=\bx_0\\
      \forall t\in[t_0,t_f],\quad & -\dot\lambda = \pd{g}{\bx}^T + \pd{f}{\bx}^T
      \lambda, \quad &\lambda(T) = 0\\ \forall t\in[t_0,t_f],\quad & \pd{g}{\bu}
      + \lambda^T\pd{f}{\bu} = 0.& \end{align*}</p>

      <p>In fact the statement can be generalized even beyond this to the case
      where $\bu$ has constraints. The result is known as Pontryagin's minimum
      principle -- giving <em>necessary conditions</em> for a trajectory to be
      optimal.</p>P

      <theorem><h1>Pontryagin's Minimum Principle</h1>

        <p>Adapted from <elib>Bertsekas00a</elib>. Given the initial conditions,
        $\bx_0$, a continuous dynamics, $\dot\bx = f(\bx,\bu)$, and the
        instantaneous cost $\ell(\bx,\bu)$, for a trajectory
        $\bx^*(\cdot),\bu^*(\cdot)$ defined over $t\in[t_0,t_f]$  to  be optimal
        is must satisfy the conditions that \begin{align*} \forall
        t\in[t_0,t_f],\quad & \dot{\bx}^* = f(\bx^*,\bu^*), \quad
        &\bx^*(0)=\bx_0\\ \forall t\in[t_0,t_f],\quad & -\dot\lambda =
        \pd{g}{\bx}^T + \pd{f}{\bx}^T \lambda, \quad &\lambda(T) = 0\\ \forall
        t\in[t_0,t_f],\quad & u^* = \argmin_{\bu\in{\cal U}} \left[ \ell(\bx^*,\bu)
        + \lambda^T f(\bx^*,\bu) \right].& \end{align*}</p>

      </theorem>

      <p>Note that the terms which are minimized in the final line of the
      theorem are commonly referred to as the Hamiltonian of the optimal control
      problem, $$H(\bx,\bu,\lambda,t) = \ell(\bx,\bu) + \lambda^T f(\bx,\bu).$$ It
      is distinct from, but inspired by, the Hamiltonian of classical mechanics.
      Remembering that $\lambda$ has an interpretation as $\pd{J}{\bx}^T$, you
      should also recognize it from the HJB. </p>

    </subsection>
  </section> <!-- end pontryagin -->

  <section><h1>Trajectory optimization as a convex
  optimization</h1>

    <subsection><h1>Linear systems with convex linear
    constraints</h1>

      <p>An important special case.  Linear/Quadratic objectives results in an
      LP/QP $\Rightarrow$ Convex optimization.</p>

    </subsection>

    <subsection><h1>Differential Flatness</h1>

    </subsection>

    <subsection><h1>Mixed-integer convex optimization for
    non-convex constraints</h1>

    </subsection>

  </section>

  <section><h1>Local Trajectory Feedback Design</h1>

    <p>Once we have obtained a locally optimal trajectory from trajectory
    optimization, there is still work to do...</p>

    <subsection><h1>Model-predictive control</h1>

    </subsection>

    <subsection><h1>Time-varying LQR</h1>

      <p>Take $\bar\bx(t) = \bx(t) - \bx_0(t)$, and $\bar\bu(t) =
      \bu(t)-\bu_0(t)$, then apply finite-horizon LQR (see the LQR chapter).</p>

    </subsection>

  </section> <!-- end feedback design -->

  <section><h1>Iterative LQR and Differential Dynamic
  Programming</h1>

  </section>

  <section><h1>Case Study: A glider that can land on a perch
  like a bird</h1>

  </section>

</chapter>
<!-- EVERYTHING BELOW THIS LINE IS OVERWRITTEN BY THE INSTALL SCRIPT -->

<table style="width:100%;"><tr style="width:100%">
  <td style="width:33%;text-align:left;"><a class="previous_chapter" href=lyapunov.html>Previous Chapter</a></td>
  <td style="width:33%;text-align:center;"><a href=underactuated.html>Table of contents</a></td>
  <td style="width:33%;text-align:right;"><a class="next_chapter" href=planning.html>Next Chapter</a></td>
</tr></table>

<div id="footer">
  <hr>
  <table style="width:100%;">
    <tr><td><em>Underactuated Robotics</em></td><td align="right">&copy; Russ
      Tedrake, 2020</td></tr>
  </table>
</div>


</body>
</html>
